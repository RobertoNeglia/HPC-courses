{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "inputs = np.array([[0, 0, 1, 1], [0, 1, 0, 1], [1, 0, 1, 0], [1, 1, 0, 0]])\n",
    "y = np.array([1, 0, 0, 1])\n",
    "\n",
    "classifier = svm.SVC(kernel=\"linear\")\n",
    "classifier.fit(inputs, y)\n",
    "\n",
    "classifier.predict(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM algorithm is failing to find a separating hyperplane to separate the two clusters of data, so the given data is not linearly separable, and hence it's not possible to solve this problem with a single perceptron. A multi layer perceptron is needed.\n",
    "\n",
    "For a better explanation, please see the file [linearlyseparable.pdf](linearlyseparable.pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the simplified equation:\n",
    "$$ x_1 x_2 + x_3 x_4 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 1, 1) --> 1\n",
      "(0, 1, 0, 1) --> 0\n",
      "(1, 0, 1, 0) --> 0\n",
      "(1, 1, 0, 0) --> 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12874/3255703964.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"(%d, %d, %d, %d) --> %d\" % (i[0], i[1], i[2], i[3], out[0]))\n"
     ]
    }
   ],
   "source": [
    "W1 = np.array([[2, 2, 0, 0], [0, 0, 2, 2]])\n",
    "b1 = np.array([[-3, -3]]).T\n",
    "W2 = np.array([[2, 2]])\n",
    "b2 = np.array([[-1]]).T\n",
    "\n",
    "\n",
    "def step_activation(x):\n",
    "    y = np.zeros(x.shape)\n",
    "    y[x > 0] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def MLNN(x):\n",
    "    x = x.T\n",
    "    H = step_activation((W1 @ x)[:, np.newaxis] + b1)\n",
    "    return step_activation(W2 @ H + b2)\n",
    "\n",
    "\n",
    "def print_results():\n",
    "    for i in inputs:\n",
    "        out = MLNN(i)\n",
    "        print(\"(%d, %d, %d, %d) --> %d\" % (i[0], i[1], i[2], i[3], out[0]))\n",
    "\n",
    "\n",
    "print_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the full equation:\n",
    "$$ \\bar{x_1} \\bar{x_2} x_3 x_4 + x_1 x_2 \\bar{x_3} \\bar{x_4} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 1, 1) --> 1\n",
      "(0, 1, 0, 1) --> 0\n",
      "(1, 0, 1, 0) --> 0\n",
      "(1, 1, 0, 0) --> 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12874/1662451562.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"(%d, %d, %d, %d) --> %d\" % (i[0], i[1], i[2], i[3], out[0]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.array([[-2, -2, 0, 0], [0, 0, 2, 2], [2, 2, 0, 0], [0, 0, -2, -2]])\n",
    "b1 = np.array([[1, -3, -3, 1]]).T\n",
    "W2 = np.array([[2, 2, 2, 2]])\n",
    "b2 = np.array([[-3, -3]]).T\n",
    "W3 = np.array([[2, 2]])\n",
    "b3 = np.array([[-1]]).T\n",
    "\n",
    "\n",
    "def step_activation(x):\n",
    "    y = np.zeros(x.shape)\n",
    "    y[x > 0] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def MLNN(x):\n",
    "    x = x.T\n",
    "    H1 = step_activation((W1 @ x)[:, np.newaxis] + b1)\n",
    "    H2 = step_activation((W2 @ H1)[:, np.newaxis] + b2)\n",
    "    return step_activation(W3 @ H2 + b3)\n",
    "\n",
    "\n",
    "def print_results():\n",
    "    inputs = np.array([[0, 0, 1, 1], [0, 1, 0, 1], [1, 0, 1, 0], [1, 1, 0, 0]])\n",
    "    for i in inputs:\n",
    "        out = MLNN(i)\n",
    "        print(\"(%d, %d, %d, %d) --> %d\" % (i[0], i[1], i[2], i[3], out[0]))\n",
    "\n",
    "\n",
    "print_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a Learning NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 1000\n",
      "Final loss: 0.0004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[0.9886856 ],\n",
       "       [0.00885692],\n",
       "       [0.0145981 ],\n",
       "       [0.9945016 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "inputs = np.array([[0, 0, 1, 1], [0, 1, 0, 1], [1, 0, 1, 0], [1, 1, 0, 0]])\n",
    "outputs = np.array([[1], [0], [0], [1]])\n",
    "\n",
    "n1, n2, n3, n4 = 4, 4, 4, 1\n",
    "\n",
    "\n",
    "def init_params():\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(n2, n1)\n",
    "    b1 = jnp.zeros((n2, 1))\n",
    "    W2 = np.random.randn(n3, n2)\n",
    "    b2 = jnp.zeros((n3, 1))\n",
    "    W3 = np.random.randn(n4, n3)\n",
    "    b3 = jnp.zeros((n4, 1))\n",
    "\n",
    "    params = [W1, b1, W2, b2, W3, b3]\n",
    "    return params\n",
    "\n",
    "\n",
    "def ANN(x, params):\n",
    "    [W1, b1, W2, b2, W3, b3] = params\n",
    "    z1 = x.T\n",
    "    z2 = jnp.tanh(W1 @ z1 - b1)\n",
    "    z3 = jnp.tanh(W2 @ z2 - b2)\n",
    "    z4 = jnp.tanh(W3 @ z3 - b3)\n",
    "    return 0.5 * (z4 + 1).T\n",
    "\n",
    "\n",
    "def loss_quadratic(x, y, params):\n",
    "    return jnp.sum((ANN(x, params) - y) ** 2)\n",
    "\n",
    "\n",
    "loss_jit = jax.jit(loss_quadratic)\n",
    "grad_jit = jax.jit(jax.grad(loss_quadratic, argnums=2))\n",
    "\n",
    "\n",
    "def optimize_params(inputs, outputs, params, epochs=1000, lr=0.1):\n",
    "    for k in range(epochs):\n",
    "        g = grad_jit(inputs, outputs, params)\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= lr * g[i]\n",
    "\n",
    "    print(\"Number of epochs: %d\" % epochs)\n",
    "    print(\"Final loss: %.4f\" % loss_jit(inputs, outputs, params))\n",
    "    return params\n",
    "\n",
    "\n",
    "params = init_params()\n",
    "params = optimize_params(inputs, outputs, params)\n",
    "\n",
    "result = ANN(inputs, params)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
