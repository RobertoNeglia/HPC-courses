{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bZCDNUgw0nI3"},"source":["# Newton method"]},{"cell_type":"code","metadata":{"id":"IJcJCwFhc8cs","executionInfo":{"status":"ok","timestamp":1670935901958,"user_tz":-60,"elapsed":322,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import jax.numpy as jnp\n","import jax\n","\n","# We enable double precision in JAX\n","from jax.config import config\n","config.update(\"jax_enable_x64\", True)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qkdJdGg1AP1"},"source":["We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n","We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$."]},{"cell_type":"code","metadata":{"id":"c0h8ihCddDPf","executionInfo":{"status":"ok","timestamp":1670936546480,"user_tz":-60,"elapsed":2,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}}},"source":["n = 100\n","\n","np.random.seed(0)\n","A = np.random.randn(n,n)\n","x_ex = np.random.randn(n)\n","b = A @ x_ex"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UanVhF4xAVoX"},"source":["Define the loss function\n","\n","$$\n","\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n","$$"]},{"cell_type":"code","source":["def loss(x):\n","  return jnp.sum(jnp.square(A@x - b))"],"metadata":{"id":"esgv2xMxi9Ii","executionInfo":{"status":"ok","timestamp":1670936548907,"user_tz":-60,"elapsed":259,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uAZ9XGaiAs3X"},"source":["By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function (*Hint*: use the `jacrev` or the `jacfwd`) function."]},{"cell_type":"code","metadata":{"id":"KflmuLXld2T4","executionInfo":{"status":"ok","timestamp":1670936552658,"user_tz":-60,"elapsed":282,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}}},"source":["grad = jax.grad(loss)\n","hess = jax.jacfwd(jax.jacrev(loss))\n","\n","loss_jit = jax.jit(loss)\n","grad_jit = jax.jit(grad)\n","hess_jit = jax.jit(hess)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSMg8ocDBndO"},"source":["Check that the results are correct (up to machine precision)."]},{"cell_type":"code","metadata":{"id":"xZulGRQ1efFP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670936554326,"user_tz":-60,"elapsed":348,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"ad603366-0cfe-4a95-a486-9411ebda35b1"},"source":["np.random.seed(0)\n","x_guess = np.random.randn(n)\n","\n","G_ad = grad_jit(x_guess)\n","G_ex = 2 * A.T @ (A @ x_guess - b)\n","print(np.linalg.norm(G_ad - G_ex))\n","\n","H_ad = hess_jit(x_guess)\n","H_ex = 2 * A.T @ A\n","print(np.linalg.norm(H_ad - H_ex))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["2.1758506481219884e-12\n","5.159031549839615e-13\n"]}]},{"cell_type":"markdown","metadata":{"id":"b-gA_kKPB2SV"},"source":["Exploit the formula\n","$$\n","\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n","$$\n","where \n","$$\n","\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n","$$\n","to write an optimized function returning the hessian-vector-product\n","$$\n","(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n","$$\n","Compare the computational performance w.r.t. the full hessian computation."]},{"cell_type":"code","metadata":{"id":"T9969dU4kc6f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6650cee4-0f69-4e66-9030-c34a1c0d0d10","executionInfo":{"status":"ok","timestamp":1670936557687,"user_tz":-60,"elapsed":229,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}}},"source":["np.random.seed(1)\n","v = np.random.randn(n)\n","\n","hvp_basic = lambda x, v: hess(x) @ v\n","hvp = jax.grad(lambda x, v: jnp.dot(grad(x), v), argnums = 0)\n","\n","hvp_basic_jit = jax.jit(hvp_basic)\n","hvp_jit = jax.jit(hvp)\n","\n","Hv_ad = hvp_jit(x_guess, v)\n","Hv_ex = H_ex @ v\n","print(np.linalg.norm(Hv_ad - Hv_ex))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["1.2669597472924609e-12\n"]}]},{"cell_type":"code","metadata":{"id":"jsA4eUnuj3ju","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670936566818,"user_tz":-60,"elapsed":6299,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"be9b35fe-9c42-4969-e274-c2fb1ff9a1a6"},"source":["%timeit hvp_basic_jit(x_guess, v)\n","%timeit hvp_jit(x_guess, v)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["258 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n","5.29 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"]}]},{"cell_type":"markdown","metadata":{"id":"TagmrdjG4Ww4"},"source":["Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$."]},{"cell_type":"code","source":["num_epochs = 100\n","tolerance = 1e-8\n","\n","x = x_guess.copy()\n","for k in range(num_epochs):\n","  G = grad_jit(x)\n","  H = hess_jit(x)\n","  l = loss_jit(x)\n","  incr = np.linalg.solve(H, -G)\n","  x += incr\n","  print('=== epoch %d' % k)\n","  print('loss: %1.3e' % l)\n","  print('incr: %1.3e' % np.linalg.norm(incr))\n","  if np.linalg.norm(incr) < tolerance:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYoU5kS0pc75","executionInfo":{"status":"ok","timestamp":1670937906135,"user_tz":-60,"elapsed":271,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"b594f7f8-6454-4a26-c3af-1b5299dd7a4d"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["=== epoch 0\n","loss: 3.739e+04\n","incr: 1.548e+01\n","=== epoch 1\n","loss: 6.545e-22\n","incr: 2.677e-09\n"]}]},{"cell_type":"markdown","metadata":{"id":"uNL7303C4oTL"},"source":["Repeat the optimization loop for the loss function\n","\n","$$\n","\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n","$$"]},{"cell_type":"code","source":["def loss(x):\n","  return jnp.sum((A@x - b)**4)\n","grad = jax.grad(loss)\n","hess = jax.jacfwd(jax.jacrev(loss))\n","\n","loss_jit = jax.jit(loss)\n","grad_jit = jax.jit(grad)\n","hess_jit = jax.jit(hess)"],"metadata":{"id":"IpjZQD39qVOi","executionInfo":{"status":"ok","timestamp":1670937944479,"user_tz":-60,"elapsed":2,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["num_epochs = 100\n","tolerance = 1e-8\n","\n","x = x_guess.copy()\n","for k in range(num_epochs):\n","  G = grad_jit(x)\n","  H = hess_jit(x)\n","  l = loss_jit(x)\n","  incr = np.linalg.solve(H, -G)\n","  x += incr\n","  print('=== epoch %d' % k)\n","  print('loss: %1.3e' % l)\n","  print('incr: %1.3e' % np.linalg.norm(incr))\n","  if np.linalg.norm(incr) < tolerance:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozjgr0p2qSPQ","executionInfo":{"status":"ok","timestamp":1670937954950,"user_tz":-60,"elapsed":975,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"714ab810-2fa0-4ffc-af93-763f3480cca8"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["=== epoch 0\n","loss: 2.433e+08\n","incr: 5.158e+00\n","=== epoch 1\n","loss: 4.806e+07\n","incr: 3.439e+00\n","=== epoch 2\n","loss: 9.493e+06\n","incr: 2.293e+00\n","=== epoch 3\n","loss: 1.875e+06\n","incr: 1.528e+00\n","=== epoch 4\n","loss: 3.704e+05\n","incr: 1.019e+00\n","=== epoch 5\n","loss: 7.317e+04\n","incr: 6.793e-01\n","=== epoch 6\n","loss: 1.445e+04\n","incr: 4.529e-01\n","=== epoch 7\n","loss: 2.855e+03\n","incr: 3.019e-01\n","=== epoch 8\n","loss: 5.639e+02\n","incr: 2.013e-01\n","=== epoch 9\n","loss: 1.114e+02\n","incr: 1.342e-01\n","=== epoch 10\n","loss: 2.200e+01\n","incr: 8.945e-02\n","=== epoch 11\n","loss: 4.346e+00\n","incr: 5.964e-02\n","=== epoch 12\n","loss: 8.585e-01\n","incr: 3.976e-02\n","=== epoch 13\n","loss: 1.696e-01\n","incr: 2.651e-02\n","=== epoch 14\n","loss: 3.350e-02\n","incr: 1.767e-02\n","=== epoch 15\n","loss: 6.617e-03\n","incr: 1.178e-02\n","=== epoch 16\n","loss: 1.307e-03\n","incr: 7.853e-03\n","=== epoch 17\n","loss: 2.582e-04\n","incr: 5.236e-03\n","=== epoch 18\n","loss: 5.100e-05\n","incr: 3.490e-03\n","=== epoch 19\n","loss: 1.007e-05\n","incr: 2.327e-03\n","=== epoch 20\n","loss: 1.990e-06\n","incr: 1.551e-03\n","=== epoch 21\n","loss: 3.931e-07\n","incr: 1.034e-03\n","=== epoch 22\n","loss: 7.764e-08\n","incr: 6.895e-04\n","=== epoch 23\n","loss: 1.534e-08\n","incr: 4.596e-04\n","=== epoch 24\n","loss: 3.029e-09\n","incr: 3.064e-04\n","=== epoch 25\n","loss: 5.984e-10\n","incr: 2.043e-04\n","=== epoch 26\n","loss: 1.182e-10\n","incr: 1.362e-04\n","=== epoch 27\n","loss: 2.335e-11\n","incr: 9.079e-05\n","=== epoch 28\n","loss: 4.612e-12\n","incr: 6.053e-05\n","=== epoch 29\n","loss: 9.111e-13\n","incr: 4.035e-05\n","=== epoch 30\n","loss: 1.800e-13\n","incr: 2.690e-05\n","=== epoch 31\n","loss: 3.555e-14\n","incr: 1.793e-05\n","=== epoch 32\n","loss: 7.022e-15\n","incr: 1.196e-05\n","=== epoch 33\n","loss: 1.387e-15\n","incr: 7.971e-06\n","=== epoch 34\n","loss: 2.740e-16\n","incr: 5.314e-06\n","=== epoch 35\n","loss: 5.412e-17\n","incr: 3.543e-06\n","=== epoch 36\n","loss: 1.069e-17\n","incr: 2.362e-06\n","=== epoch 37\n","loss: 2.112e-18\n","incr: 1.574e-06\n","=== epoch 38\n","loss: 4.171e-19\n","incr: 1.050e-06\n","=== epoch 39\n","loss: 8.239e-20\n","incr: 6.998e-07\n","=== epoch 40\n","loss: 1.628e-20\n","incr: 4.665e-07\n","=== epoch 41\n","loss: 3.215e-21\n","incr: 3.110e-07\n","=== epoch 42\n","loss: 6.350e-22\n","incr: 2.073e-07\n","=== epoch 43\n","loss: 1.254e-22\n","incr: 1.382e-07\n","=== epoch 44\n","loss: 2.478e-23\n","incr: 9.215e-08\n","=== epoch 45\n","loss: 4.894e-24\n","incr: 6.143e-08\n","=== epoch 46\n","loss: 9.668e-25\n","incr: 4.096e-08\n","=== epoch 47\n","loss: 1.910e-25\n","incr: 2.730e-08\n","=== epoch 48\n","loss: 3.772e-26\n","incr: 1.820e-08\n","=== epoch 49\n","loss: 7.451e-27\n","incr: 1.214e-08\n","=== epoch 50\n","loss: 1.472e-27\n","incr: 8.090e-09\n"]}]}]}