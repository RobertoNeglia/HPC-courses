{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKvkeUvRrBtr"
      },
      "source": [
        "# California Housing data from 1990 US Census\n",
        "\n",
        "In this lab we will rely on [Pandas](https://pandas.pydata.org/) and [Seaborn](http://seaborn.pydata.org/) to inspect and visualize data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si_DpGyVWZ5I"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "import jax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tORMQT4qrH9C"
      },
      "source": [
        "Load the dataset (if you are using Google Colab, it is already in the `sample_data` folder!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlhHDjBZmRWl"
      },
      "source": [
        "data = pd.read_csv('sample_data/california_housing_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8exyJVturUOy"
      },
      "source": [
        "## Data inspection\n",
        "\n",
        "Display some basic information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apJCc1IPrTyp"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66g2s9v9nFLh"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXDJ4ARqnPfY"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GN6DWxerbXy"
      },
      "source": [
        "We are interested in predicting the field `median_house_value`. Plot its distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPG0IsEWnWlN"
      },
      "source": [
        "sns.distplot(data['median_house_value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrVPuS5drjVo"
      },
      "source": [
        "It looks like the distribution tail has been truncated. Get rid of it to ease the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55_JIe0HTOjU"
      },
      "source": [
        "data = data[data['median_house_value'] < 500001]\n",
        "sns.distplot(data['median_house_value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ek1Wpq5r38f"
      },
      "source": [
        "Use a scatterplot to visualize the geograhical distribution of the houses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dRiIxafneRv"
      },
      "source": [
        "sns.scatterplot(data = data, x='longitude' ,y='latitude', hue='median_house_value')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHwjE_v4r-CA"
      },
      "source": [
        "Look for linear correlations among features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBNLx91znnpu"
      },
      "source": [
        "data.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYkJHwf_np_j"
      },
      "source": [
        "sns.heatmap(data.corr(), annot = True, cmap = 'vlag_r', vmin = -1, vmax = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LNH8bJITYXy"
      },
      "source": [
        "sns.scatterplot(data = data, x='latitude' ,y='median_house_value')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJc_xoA3vLiv"
      },
      "source": [
        "## Data normalization\n",
        "\n",
        "Apply an affine transformation to the data, so that each feature has zero mean and unitary standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eVFn6wkpDk7"
      },
      "source": [
        "data_mean = data.mean()\n",
        "data_std = data.std()\n",
        "data_normalized = ...\n",
        "\n",
        "data_normalized.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyO7bropWw7"
      },
      "source": [
        "_, ax = plt.subplots(figsize=(16,6))\n",
        "sns.violinplot(data = data_normalized, ax = ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Hzcd6RvrHK"
      },
      "source": [
        "## Train-validation split\n",
        "\n",
        "Shuffle the data (**hint:** use the [np.random.shuffle](https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html) function) and split the data as follows:\n",
        "- put 80% in the train dataset\n",
        "- put 20% in the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT4j1Jb1AWWi"
      },
      "source": [
        "np.random.seed(0) # for reproducibility\n",
        "data_normalized_np = data_normalized.to_numpy()\n",
        "...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz8v0XDnwLqw"
      },
      "source": [
        "## ANN setup\n",
        "\n",
        "Write a function `initialize_params` that, given the input `layers_size = [n1, n2, ..., nL]`, generates the parameters associated with an ANN, having as many layers as the number of elements of `layers_size`, with as many neurons as `n1`, `n2`, etc.\n",
        "\n",
        "To initialize the parameters, employ the following strategy:\n",
        "- Inizialize the biases with zero value.\n",
        "- Inizialize the weights sampling from a Gaussian distribution with zero mean and with standard deviation \n",
        "$$\n",
        "\\sqrt{\\frac{2}{n + m}}\n",
        "$$\n",
        "where $n$ and $m$ are the number of input and output neurons of the corresponding weights matrix (this is known as \"Glorot Normal\" or \"Xavier Normal\", see [Glorot, Bengio 2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)).\n",
        "\n",
        "Other popular initializations strategies are:\n",
        "- Gaussian distribution with zero mean and with standard deviation (for some constant $K$)\n",
        "$$\n",
        "\\frac{K}{\\sqrt{n}}\n",
        "$$\n",
        "- Uniform distribution\n",
        "$$\n",
        "\\left[-\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{1}{n}}\\right]\n",
        "$$\n",
        "- Uniform distribution (this is known as \"Glorot Uniform\" or \"Xavier Uniform\")\n",
        "$$\n",
        "\\left[-\\sqrt{\\frac{6}{n + m}}, \\sqrt{\\frac{6}{n + m}}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz4fSpfmzxnp"
      },
      "source": [
        "def initialize_params(layers_size):\n",
        "  np.random.seed(0) # for reproducibility\n",
        "  params = list()\n",
        "  ...\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qzsJOMTz0_8"
      },
      "source": [
        "Implement a generic feedforward ANN with a function `y = ANN(x, params)`, using $\\tanh$ as activation function.\n",
        "\n",
        "By convention, both the input and the output have:\n",
        "- 1 sample per row\n",
        "- 1 feature per column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtwLtoDzqaM2"
      },
      "source": [
        "def ANN(x, params):\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmvIfrrQ0Fl_"
      },
      "source": [
        "Implement the quadratic (MSE) loss function `L = loss(x, y, params)`, defined as:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\mathrm{ANN}(x_i, \\boldsymbol{\\theta}))^2\n",
        "$$\n",
        "\n",
        "where $m$ is the number of samples in $\\mathbf{x}$, $\\mathbf{y}$ and $\\boldsymbol{\\theta}$ are the ANN parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svtW7wfW0Da_"
      },
      "source": [
        "def loss(x, y, params):\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q-49G4mm2Bl"
      },
      "source": [
        "Test your code, by generating the parameters associated with an ANN with two hidden layers with 5 neurons each and by computing the associated loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVlrCC00eKd"
      },
      "source": [
        "## Training\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "Implement the GD method:\n",
        "$$\n",
        "\\begin{split}\n",
        "& \\boldsymbol{\\theta}^{(0)} \\text{given} \\\\\n",
        "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
        "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{N} \\sum_{i=1}^N \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
        "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} - \\lambda \\mathbf{g}^{(k)}\n",
        "\\end{split}\n",
        "$$\n",
        "where $N$ is the number of training samples. At each iteration, append the current cost to the list `history`.\n",
        "\n",
        "Train an ANN with two hidden layers with 20 neurons each. \n",
        "Try to (manually) optimize the training hyperparameters.\n",
        "\n",
        "During training, store the MSE error obtained on the train and validation sets in two lists, respectively called `history_train` and `history_valid`. Finally, plot the erros trend and diplay the final values of the errors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlf_xHATnHjd"
      },
      "source": [
        "\n",
        "Hints: \n",
        "- Use `jax.jit` to speedup the evaluation of the loss and of the gradients.\n",
        "- Use `tqdm` to show a nice progress bar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH_cSddkEN7X"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "for i in tqdm(range(100)):\n",
        "  # do something ...\n",
        "  time.sleep(0.02) # only for testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4fiOqFqnmUf"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "\n",
        "Implement the SGD method:\n",
        "$$\n",
        "\\begin{split}\n",
        "& \\boldsymbol{\\theta}^{(0)} \\text{given} \\\\\n",
        "& \\text{for } k = 0, 1, \\dots , n_{\\text{epochs}} - 1\\\\\n",
        "& \\qquad \\mathbf{g}^{(k)} = \\frac{1}{|I_k|} \\sum_{i \\in I_k} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(x_i, y_i, \\boldsymbol{\\theta}^{(k)}) \\\\\n",
        "& \\qquad \\boldsymbol{\\theta}^{(k+1)} = \\boldsymbol{\\theta}^{(k)} - \\lambda_k \\mathbf{g}^{(k)}\n",
        "\\end{split}\n",
        "$$\n",
        "where $I_k$ is the current minibatch. To select it, use the function [np.random.choice](https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html) with replacement.\n",
        "\n",
        "Consider a linear decay of the learning rate:\n",
        "$$\n",
        "\\lambda_k = \\max\\left(\\lambda_\\min, \\lambda_\\max \\left(1 - \\frac{k}{K}\\right)\\right)\n",
        "$$\n",
        "\n",
        "Test different choices of batch size and try to optimize the learning rate decay strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K75tZrUw0oSH"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Load the test dataset `sample_data/california_housing_test.csv` and use the trained model to predict the house prices of the dataset.\n",
        "\n",
        "Compare predicted prices with actual prices by means of a scatterplot.\n",
        "\n",
        "Finally, compute the RMSE (root mean square error)."
      ]
    }
  ]
}