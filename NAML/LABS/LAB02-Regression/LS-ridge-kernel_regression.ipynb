{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LS-ridge-kernel_regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNPOx3nNv0PWc1qmBciX5Kl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4C5ncMKPCGS_"},"source":["# Moore-Penrose pseudo inverse"]},{"cell_type":"code","metadata":{"id":"mf3YmOwwWx3P"},"source":["import numpy as np\n","import scipy.linalg as la\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTAIp1RRBzfn"},"source":["Write a function computing the Moore-Penrose pseudo inverse, exploiting the full SVD."]},{"cell_type":"code","metadata":{"id":"l53EKZ8m9v6X"},"source":["def my_pinv1(A):\n","  ...\n","  return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pji4usPEFaPS"},"source":["Write now a function computing the Moore-Penrose pseudo inverse, exploiting the reduced SVD."]},{"cell_type":"code","metadata":{"id":"n-cfuhorCG0f"},"source":["def my_pinv2(A):\n","  ...\n","  return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AiVbuB94CItY"},"source":["Generate a random matrix $A$ (with elements sampled from a standard Gaussian distribution) with 5 rows and 4 columns. Compute its Moore-Penrose pseudo inverse thorugh the two functions above defined, and compare the result with the function `numpy.linalg.pinv` (see [Documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html))."]},{"cell_type":"code","metadata":{"id":"56sxO58a87LU"},"source":["A = np.random.randn(5,4)\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oLqUjmIvCnCN"},"source":["Compare the three implementations performances through the Google Colab magic command `%timeit`."]},{"cell_type":"code","metadata":{"id":"nhvfVtIo-NC1"},"source":["%timeit np.linalg.pinv(A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y21T3Nnj-aL-"},"source":["%timeit my_pinv1(A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zzvnNMJ-dD7"},"source":["%timeit my_pinv2(A)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2rYM4FpDI9M"},"source":["# Least-square regression"]},{"cell_type":"markdown","metadata":{"id":"hquNEJ1sDN9I"},"source":["Consider the linear model\n","$$\n","y = mx + q.\n","$$\n","where $m = 2$ and $q = 3$.\n","\n","Generate $N = 100$ points $x_i$, sampling from a standard Gaussian distribution, and the associated $y_i$. Then, add a synthetic noise ($\\epsilon_i$) by sampling from a Gaussian distribution with zero mean and standard deviation $\\sigma = 2$. Plot the noisy data $(x_i, \\tilde{y}_i)$, where $\\tilde{y}_i = y_i + \\epsilon_i$, in the $(x,y)$ plane, together with the line $y = mx + q$."]},{"cell_type":"code","metadata":{"id":"SRF2lfmNAH1M"},"source":["m = 2.0\n","q = 3.0\n","N = 100\n","noise = 2.0\n","\n","X = np.random.randn(N)\n","Y = m*X + q + noise * np.random.randn(N)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8w-4ou8D20u"},"source":["Using the previously implemented functions to compute the Moore-Penrose pseudo inverse, solve the least-squares problem\n","$$\n","\\min_{m,q} \\sum_{i=1}^N (\\tilde{y}_i - (m x_i + q))^2\n","$$\n","and display the regression line superimposed to the noisy data and the exact model."]},{"cell_type":"markdown","metadata":{"id":"-wssES11FISl"},"source":["Repeat the excercise by solving the normal equations. Compare the results"]},{"cell_type":"markdown","metadata":{"id":"V5E5i7tk62e4"},"source":["# Ridge regression and Kernel regression"]},{"cell_type":"markdown","metadata":{"id":"793QCqaoIAGB"},"source":["Consider the function\n","$$\n","y = f(x) = \\tanh(2x - 1).\n","$$\n","\n","Generate $N = 100$ points $x_i$, sampling from a standard Gaussian distribution, and the associated $y_i$. Then, add a synthetic noise ($\\epsilon_i$) by sampling from a Gaussian distribution with zero mean and standard deviation $\\sigma = 0.1$. Plot the noisy data $(x_i, \\tilde{y}_i)$, where $\\tilde{y}_i = y_i + \\epsilon_i$, in the $(x,y)$ plane.\n","\n","Then, generate 1000 testing points, uniformly distributed in the interval $[-3,3]$, and display the function $y = f(x)$ in correspondence of the testing points."]},{"cell_type":"code","metadata":{"id":"cQ1mVije617U"},"source":["N = 100\n","noise = 0.1\n","y_ex = lambda x: np.tanh(2*(x - 1))\n","\n","X = np.random.randn(N)\n","Y = y_ex(X) + noise * np.random.randn(N)\n","\n","N_test = 1000\n","X_test = np.linspace(-3,3,N_test)\n","Y_test_ex = y_ex(X_test)\n","\n","plt.scatter(X,Y, marker = '+', color = 'black', label = 'data')\n","plt.plot(X_test, Y_test_ex, color = 'black', label = '$y_{ex}(x)$')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6SY2zU3I406"},"source":["Proceeding as in the previous exercise, compute the regression line resulting from the **least squares regression** of data $(x_i, \\tilde{y}_i)$. Plot the resulting regression line."]},{"cell_type":"markdown","metadata":{"id":"ERGPDoiwJv-O"},"source":["Let us now consider **ridge regression**, corresponding to a regularizaton parameter $\\lambda = 1.0$. Compare the obtained regression line with the one obtained through least squares regression."]},{"cell_type":"markdown","metadata":{"id":"_c7X6mmPKXiC"},"source":["Consider now **kernel regression**. \n","\n","1. Consider first the scalar product kernel \n","$$K(x_i,x_j) = x_i x_j + 1.$$\n","Compute the regression function and compare the result with the ones obtained at the previous point. How do the results compare?\n","\n","2. Consider then the higher-order scalar product kernel, for $q > 1$.\n","$$K(x_i,x_j) = (x_i x_j + 1)^q.$$\n","\n","3. Consider finally a Gaussian kernel, for $\\sigma > 0$.\n","$$K(x_i,x_j) = \\exp\\left(-\\frac{(x_i - x_j)^2}{2 \\sigma^2}\\right).$$"]}]}