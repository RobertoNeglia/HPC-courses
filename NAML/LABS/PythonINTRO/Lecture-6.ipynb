{"cells":[{"cell_type":"markdown","metadata":{"id":"ZEVpEp9tHCAn"},"source":["# SciPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wJ59tStHCAp"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"mQ4aILNAHCAr"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"x3ZBFWqaHCAr"},"source":["The SciPy framework builds on top of the low-level NumPy framework for multidimensional arrays, and provides a large number of higher-level scientific algorithms. Some of the topics that SciPy covers are:\n","\n","* Special functions ([scipy.special](http://docs.scipy.org/doc/scipy/reference/special.html))\n","* Integration ([scipy.integrate](http://docs.scipy.org/doc/scipy/reference/integrate.html))\n","* Optimization ([scipy.optimize](http://docs.scipy.org/doc/scipy/reference/optimize.html))\n","* Interpolation ([scipy.interpolate](http://docs.scipy.org/doc/scipy/reference/interpolate.html))\n","* Fourier Transforms ([scipy.fftpack](http://docs.scipy.org/doc/scipy/reference/fftpack.html))\n","* Signal Processing ([scipy.signal](http://docs.scipy.org/doc/scipy/reference/signal.html))\n","* Linear Algebra ([scipy.linalg](http://docs.scipy.org/doc/scipy/reference/linalg.html))\n","* Sparse Eigenvalue Problems ([scipy.sparse](http://docs.scipy.org/doc/scipy/reference/sparse.html))\n","* Statistics ([scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html))\n","* Multi-dimensional image processing ([scipy.ndimage](http://docs.scipy.org/doc/scipy/reference/ndimage.html))\n","* File IO ([scipy.io](http://docs.scipy.org/doc/scipy/reference/io.html))\n","\n","Each of these submodules provides a number of functions and classes that can be used to solve problems in their respective topics.\n","\n","In this lecture we will look at how to use some of these subpackages.\n","\n","To access the SciPy package in a Python program, we start by importing everything from the `scipy` module."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ALB_o2kHCAs"},"outputs":[],"source":["from scipy import *"]},{"cell_type":"markdown","metadata":{"id":"fUflndv5HCAs"},"source":["If we only need to use part of the SciPy framework we can selectively include only those modules we are interested in. For example, to include the linear algebra package under the name `la`, we can do:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDvZ4RcdHCAt"},"outputs":[],"source":["import scipy.linalg as la"]},{"cell_type":"markdown","metadata":{"id":"I1JBeHSqHCAt"},"source":["## Linear algebra"]},{"cell_type":"markdown","metadata":{"id":"TY1gWq72HCAt"},"source":["The linear algebra module contains a lot of matrix related functions, including linear equation solving, eigenvalue solvers, matrix functions (for example matrix-exponentiation), a number of different decompositions (SVD, LU, cholesky), etc. \n","\n","Detailed documetation is available at: http://docs.scipy.org/doc/scipy/reference/linalg.html\n","\n","Here we will look at how to use some of these functions:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xmc6GJ6LHCAu"},"source":["### Linear equation systems"]},{"cell_type":"markdown","metadata":{"id":"JrlSWIKgHCAu"},"source":["Linear equation systems on the matrix form\n","\n","$A x = b$\n","\n","where $A$ is a matrix and $x,b$ are vectors can be solved like:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qCCrlXsBHCAv","executionInfo":{"status":"ok","timestamp":1663601203769,"user_tz":-120,"elapsed":22,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}}},"outputs":[],"source":["import scipy.linalg as la\n","import numpy as np\n","from numpy import random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VnbqiHrMHCAv","executionInfo":{"status":"ok","timestamp":1663601204301,"user_tz":-120,"elapsed":550,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}}},"outputs":[],"source":["A = np.array([[1,2,3], [4,5,6], [6,8,9]])\n","b = np.array([6,15,23])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YUmS-mSHCAv","outputId":"033d9e66-d8a6-4b33-d643-a4454565a7fe"},"outputs":[{"data":{"text/plain":["array([[1, 4, 6],\n","       [2, 5, 8],\n","       [3, 6, 9]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["A.transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgNmMNgbHCAw","outputId":"6c2887de-676b-4361-8a67-825c9c1e1bf0"},"outputs":[{"data":{"text/plain":["array([[1, 4, 6],\n","       [2, 5, 8],\n","       [3, 6, 9]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["A.T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEd2e33ZHCAx","outputId":"81acc8f3-3f7d-4a1b-b50a-c3b874ffd0e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on function solve in module scipy.linalg.basic:\n","\n","solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False, debug=None, check_finite=True, assume_a='gen', transposed=False)\n","    Solves the linear equation set ``a * x = b`` for the unknown ``x``\n","    for square ``a`` matrix.\n","    \n","    If the data matrix is known to be a particular type then supplying the\n","    corresponding string to ``assume_a`` key chooses the dedicated solver.\n","    The available options are\n","    \n","    ===================  ========\n","     generic matrix       'gen'\n","     symmetric            'sym'\n","     hermitian            'her'\n","     positive definite    'pos'\n","    ===================  ========\n","    \n","    If omitted, ``'gen'`` is the default structure.\n","    \n","    The datatype of the arrays define which solver is called regardless\n","    of the values. In other words, even when the complex array entries have\n","    precisely zero imaginary parts, the complex solver will be called based\n","    on the data type of the array.\n","    \n","    Parameters\n","    ----------\n","    a : (N, N) array_like\n","        Square input data\n","    b : (N, NRHS) array_like\n","        Input data for the right hand side.\n","    sym_pos : bool, optional\n","        Assume `a` is symmetric and positive definite. This key is deprecated\n","        and assume_a = 'pos' keyword is recommended instead. The functionality\n","        is the same. It will be removed in the future.\n","    lower : bool, optional\n","        If True, only the data contained in the lower triangle of `a`. Default\n","        is to use upper triangle. (ignored for ``'gen'``)\n","    overwrite_a : bool, optional\n","        Allow overwriting data in `a` (may enhance performance).\n","        Default is False.\n","    overwrite_b : bool, optional\n","        Allow overwriting data in `b` (may enhance performance).\n","        Default is False.\n","    check_finite : bool, optional\n","        Whether to check that the input matrices contain only finite numbers.\n","        Disabling may give a performance gain, but may result in problems\n","        (crashes, non-termination) if the inputs do contain infinities or NaNs.\n","    assume_a : str, optional\n","        Valid entries are explained above.\n","    transposed: bool, optional\n","        If True, ``a^T x = b`` for real matrices, raises `NotImplementedError`\n","        for complex matrices (only for True).\n","    \n","    Returns\n","    -------\n","    x : (N, NRHS) ndarray\n","        The solution array.\n","    \n","    Raises\n","    ------\n","    ValueError\n","        If size mismatches detected or input a is not square.\n","    LinAlgError\n","        If the matrix is singular.\n","    LinAlgWarning\n","        If an ill-conditioned input a is detected.\n","    NotImplementedError\n","        If transposed is True and input a is a complex matrix.\n","    \n","    Examples\n","    --------\n","    Given `a` and `b`, solve for `x`:\n","    \n","    >>> a = np.array([[3, 2, 0], [1, -1, 0], [0, 5, 1]])\n","    >>> b = np.array([2, 4, -1])\n","    >>> from scipy import linalg\n","    >>> x = linalg.solve(a, b)\n","    >>> x\n","    array([ 2., -2.,  9.])\n","    >>> np.dot(a, x) == b\n","    array([ True,  True,  True], dtype=bool)\n","    \n","    Notes\n","    -----\n","    If the input b matrix is a 1D array with N elements, when supplied\n","    together with an NxN input a, it is assumed as a valid column vector\n","    despite the apparent size mismatch. This is compatible with the\n","    numpy.dot() behavior and the returned result is still 1D array.\n","    \n","    The generic, symmetric, hermitian and positive definite solutions are\n","    obtained via calling ?GESV, ?SYSV, ?HESV, and ?POSV routines of\n","    LAPACK respectively.\n","\n"]}],"source":["help(la.solve)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofSplqHUHCAx","outputId":"7c3f86f5-490d-4f3f-e1d3-c42df6aec18f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1. 1. 1.]\n"]}],"source":["x = la.solve(A, b)\n","print(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xccym_kXHCAx","outputId":"e3fad977-4115-4f30-8066-164605c5bd8c"},"outputs":[{"data":{"text/plain":["array([0., 0., 0.])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# check\n","np.dot(A, x) - b"]},{"cell_type":"markdown","metadata":{"id":"fjOUSOu8HCAx"},"source":["We can also do the same with\n","\n","$A X = B$\n","\n","where $A, B, X$ are matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtyZG7k4HCAx"},"outputs":[],"source":["A = random.rand(3,3)\n","B = random.rand(3,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdrLq7U-HCAy"},"outputs":[],"source":["X = la.solve(A, B)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dz-7bzowHCAy","outputId":"831af0fa-62f8-4249-d6da-4cf5e6b0b8d6"},"outputs":[{"data":{"text/plain":["array([[-0.04795782,  1.03937412, -0.25098415],\n","       [ 0.92752652,  0.47858424,  0.69493357],\n","       [ 0.17229905, -0.78208753,  0.67692863]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91sE9c36HCAy","outputId":"b7eadfea-057a-4fe2-8afc-f0ada314a1cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on function norm in module scipy.linalg.misc:\n","\n","norm(a, ord=None, axis=None, keepdims=False, check_finite=True)\n","    Matrix or vector norm.\n","    \n","    This function is able to return one of seven different matrix norms,\n","    or one of an infinite number of vector norms (described below), depending\n","    on the value of the ``ord`` parameter.\n","    \n","    Parameters\n","    ----------\n","    a : (M,) or (M, N) array_like\n","        Input array.  If `axis` is None, `a` must be 1-D or 2-D.\n","    ord : {non-zero int, inf, -inf, 'fro'}, optional\n","        Order of the norm (see table under ``Notes``). inf means numpy's\n","        `inf` object\n","    axis : {int, 2-tuple of ints, None}, optional\n","        If `axis` is an integer, it specifies the axis of `a` along which to\n","        compute the vector norms.  If `axis` is a 2-tuple, it specifies the\n","        axes that hold 2-D matrices, and the matrix norms of these matrices\n","        are computed.  If `axis` is None then either a vector norm (when `a`\n","        is 1-D) or a matrix norm (when `a` is 2-D) is returned.\n","    keepdims : bool, optional\n","        If this is set to True, the axes which are normed over are left in the\n","        result as dimensions with size one.  With this option the result will\n","        broadcast correctly against the original `a`.\n","    check_finite : bool, optional\n","        Whether to check that the input matrix contains only finite numbers.\n","        Disabling may give a performance gain, but may result in problems\n","        (crashes, non-termination) if the inputs do contain infinities or NaNs.\n","    \n","    Returns\n","    -------\n","    n : float or ndarray\n","        Norm of the matrix or vector(s).\n","    \n","    Notes\n","    -----\n","    For values of ``ord <= 0``, the result is, strictly speaking, not a\n","    mathematical 'norm', but it may still be useful for various numerical\n","    purposes.\n","    \n","    The following norms can be calculated:\n","    \n","    =====  ============================  ==========================\n","    ord    norm for matrices             norm for vectors\n","    =====  ============================  ==========================\n","    None   Frobenius norm                2-norm\n","    'fro'  Frobenius norm                --\n","    inf    max(sum(abs(x), axis=1))      max(abs(x))\n","    -inf   min(sum(abs(x), axis=1))      min(abs(x))\n","    0      --                            sum(x != 0)\n","    1      max(sum(abs(x), axis=0))      as below\n","    -1     min(sum(abs(x), axis=0))      as below\n","    2      2-norm (largest sing. value)  as below\n","    -2     smallest singular value       as below\n","    other  --                            sum(abs(x)**ord)**(1./ord)\n","    =====  ============================  ==========================\n","    \n","    The Frobenius norm is given by [1]_:\n","    \n","        :math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`\n","    \n","    The ``axis`` and ``keepdims`` arguments are passed directly to\n","    ``numpy.linalg.norm`` and are only usable if they are supported\n","    by the version of numpy in use.\n","    \n","    References\n","    ----------\n","    .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,\n","           Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n","    \n","    Examples\n","    --------\n","    >>> from scipy.linalg import norm\n","    >>> a = np.arange(9) - 4.0\n","    >>> a\n","    array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n","    >>> b = a.reshape((3, 3))\n","    >>> b\n","    array([[-4., -3., -2.],\n","           [-1.,  0.,  1.],\n","           [ 2.,  3.,  4.]])\n","    \n","    >>> norm(a)\n","    7.745966692414834\n","    >>> norm(b)\n","    7.745966692414834\n","    >>> norm(b, 'fro')\n","    7.745966692414834\n","    >>> norm(a, np.inf)\n","    4\n","    >>> norm(b, np.inf)\n","    9\n","    >>> norm(a, -np.inf)\n","    0\n","    >>> norm(b, -np.inf)\n","    2\n","    \n","    >>> norm(a, 1)\n","    20\n","    >>> norm(b, 1)\n","    7\n","    >>> norm(a, -1)\n","    -4.6566128774142013e-010\n","    >>> norm(b, -1)\n","    6\n","    >>> norm(a, 2)\n","    7.745966692414834\n","    >>> norm(b, 2)\n","    7.3484692283495345\n","    \n","    >>> norm(a, -2)\n","    0\n","    >>> norm(b, -2)\n","    1.8570331885190563e-016\n","    >>> norm(a, 3)\n","    5.8480354764257312\n","    >>> norm(a, -3)\n","    0\n","\n"]}],"source":["help(la.norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLI9R7ssHCAy","outputId":"0c0474c6-f638-4c63-d7c8-6b79049fbfc2"},"outputs":[{"data":{"text/plain":["2.1364582767275725e-16"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# check\n","la.norm(np.dot(A, X) - B)"]},{"cell_type":"markdown","metadata":{"id":"BHJWeb1zHCAy"},"source":["### Eigenvalues and eigenvectors"]},{"cell_type":"markdown","metadata":{"id":"S0nF8na3HCAy"},"source":["The eigenvalue problem for a matrix $A$:\n","\n","$\\displaystyle A v_n = \\lambda_n v_n$\n","\n","where $v_n$ is the $n$th eigenvector and $\\lambda_n$ is the $n$th eigenvalue.\n","\n","To calculate eigenvalues of a matrix, use the `eigvals` and for calculating both eigenvalues and eigenvectors, use the function `eig`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrhYQFgTHCAz","outputId":"bd17a43b-b1cd-44c0-bed8-3135f02418a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on function eigvals in module scipy.linalg.decomp:\n","\n","eigvals(a, b=None, overwrite_a=False, check_finite=True, homogeneous_eigvals=False)\n","    Compute eigenvalues from an ordinary or generalized eigenvalue problem.\n","    \n","    Find eigenvalues of a general matrix::\n","    \n","        a   vr[:,i] = w[i]        b   vr[:,i]\n","    \n","    Parameters\n","    ----------\n","    a : (M, M) array_like\n","        A complex or real matrix whose eigenvalues and eigenvectors\n","        will be computed.\n","    b : (M, M) array_like, optional\n","        Right-hand side matrix in a generalized eigenvalue problem.\n","        If omitted, identity matrix is assumed.\n","    overwrite_a : bool, optional\n","        Whether to overwrite data in a (may improve performance)\n","    check_finite : bool, optional\n","        Whether to check that the input matrices contain only finite numbers.\n","        Disabling may give a performance gain, but may result in problems\n","        (crashes, non-termination) if the inputs do contain infinities\n","        or NaNs.\n","    homogeneous_eigvals : bool, optional\n","        If True, return the eigenvalues in homogeneous coordinates.\n","        In this case ``w`` is a (2, M) array so that::\n","    \n","            w[1,i] a vr[:,i] = w[0,i] b vr[:,i]\n","    \n","        Default is False.\n","    \n","    Returns\n","    -------\n","    w : (M,) or (2, M) double or complex ndarray\n","        The eigenvalues, each repeated according to its multiplicity\n","        but not in any specific order. The shape is (M,) unless\n","        ``homogeneous_eigvals=True``.\n","    \n","    Raises\n","    ------\n","    LinAlgError\n","        If eigenvalue computation does not converge\n","    \n","    See Also\n","    --------\n","    eig : eigenvalues and right eigenvectors of general arrays.\n","    eigvalsh : eigenvalues of symmetric or Hermitian arrays\n","    eigvals_banded : eigenvalues for symmetric/Hermitian band matrices\n","    eigvalsh_tridiagonal : eigenvalues of symmetric/Hermitian tridiagonal\n","        matrices\n","    \n","    Examples\n","    --------\n","    >>> from scipy import linalg\n","    >>> a = np.array([[0., -1.], [1., 0.]])\n","    >>> linalg.eigvals(a)\n","    array([0.+1.j, 0.-1.j])\n","    \n","    >>> b = np.array([[0., 1.], [1., 1.]])\n","    >>> linalg.eigvals(a, b)\n","    array([ 1.+0.j, -1.+0.j])\n","    \n","    >>> a = np.array([[3., 0., 0.], [0., 8., 0.], [0., 0., 7.]])\n","    >>> linalg.eigvals(a, homogeneous_eigvals=True)\n","    array([[3.+0.j, 8.+0.j, 7.+0.j],\n","           [1.+0.j, 1.+0.j, 1.+0.j]])\n","\n"]}],"source":["help(la.eigvals)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHvJbOG6HCAz"},"outputs":[],"source":["evals = la.eigvals(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJv9cJY_HCAz","outputId":"bdc86b0d-522d-4a04-d29c-e5b0f96e08ee"},"outputs":[{"data":{"text/plain":["array([1.7307039 +0.j       , 0.04604059+0.4423735j,\n","       0.04604059-0.4423735j])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XtX_ufOHCAz"},"outputs":[],"source":["eigvv = la.eig(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_cHrIY2HCAz","outputId":"3d11e44d-1614-427f-a0a2-7147e71bad06"},"outputs":[{"data":{"text/plain":["(1.7307038963632602+0j)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["eigvv[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETVeFPauHCA0","outputId":"df5273ca-c938-49d3-e657-de8fe3721010"},"outputs":[{"data":{"text/plain":["array([-0.63750819+0.j, -0.32479824+0.j, -0.69863396+0.j])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["eigvv[1][:,0]"]},{"cell_type":"markdown","metadata":{"id":"dgL3duNOHCA0"},"source":["The eigenvector corresponding to the $n$th eigenvalue (stored in the $n$th position of  `eigvv[0]`) is the $n$th *column* in `eigvv[1]`. To verify this, let's try mutiplying eigenvectors with the matrix and compare to the product of the eigenvector and the eigenvalue:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxiLDqooHCA0"},"outputs":[],"source":["n = 1\n","\n","la.norm(np.dot(A, eigvv[1][:,n]) - eigvv[0][n] * eigvv[1][:,n])"]},{"cell_type":"markdown","metadata":{"id":"Ipcp8_4oHCA0"},"source":["There are also more specialized eigensolvers, like the `eigh` for Hermitian matrices. "]},{"cell_type":"markdown","metadata":{"id":"mixx2HDfHCA0"},"source":["### Matrix operations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHoHRacgHCA0"},"outputs":[],"source":["# the matrix inverse\n","la.inv(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FM_GymSHCA0"},"outputs":[],"source":["# determinant.\n","la.det(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZvlFVF0HCA1"},"outputs":[],"source":["# norms of various orders\n","la.norm(A, ord=2), la.norm(A, ord=np.inf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKDJvI0BHCA1"},"outputs":[],"source":["# condition number\n","np.linalg.cond(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx9qYnotHCA1","outputId":"a08ed47b-156d-496e-83b7-de4544922cfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.5        0.33333333 0.25       0.2        0.16666667\n","  0.14285714 0.125      0.11111111 0.1       ]\n"," [0.5        0.33333333 0.25       0.2        0.16666667 0.14285714\n","  0.125      0.11111111 0.1        0.09090909]\n"," [0.33333333 0.25       0.2        0.16666667 0.14285714 0.125\n","  0.11111111 0.1        0.09090909 0.08333333]\n"," [0.25       0.2        0.16666667 0.14285714 0.125      0.11111111\n","  0.1        0.09090909 0.08333333 0.07692308]\n"," [0.2        0.16666667 0.14285714 0.125      0.11111111 0.1\n","  0.09090909 0.08333333 0.07692308 0.07142857]\n"," [0.16666667 0.14285714 0.125      0.11111111 0.1        0.09090909\n","  0.08333333 0.07692308 0.07142857 0.06666667]\n"," [0.14285714 0.125      0.11111111 0.1        0.09090909 0.08333333\n","  0.07692308 0.07142857 0.06666667 0.0625    ]\n"," [0.125      0.11111111 0.1        0.09090909 0.08333333 0.07692308\n","  0.07142857 0.06666667 0.0625     0.05882353]\n"," [0.11111111 0.1        0.09090909 0.08333333 0.07692308 0.07142857\n","  0.06666667 0.0625     0.05882353 0.05555556]\n"," [0.1        0.09090909 0.08333333 0.07692308 0.07142857 0.06666667\n","  0.0625     0.05882353 0.05555556 0.05263158]]\n"]},{"data":{"text/plain":["16024416992541.715"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["H=la.hilbert(10)\n","print(H)\n","np.linalg.cond(H)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8I_vmZHHCA1","outputId":"26ba0195-3cfd-4d26-f15b-0086f498bfcc"},"outputs":[{"data":{"text/plain":["array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["xtrue=np.array([1,1,1,1,1,1,1,1,1,1])\n","xtrue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ll9wVyuZHCA1","outputId":"08dd8717-1422-454d-ab11-e2b5e6c2fe1a"},"outputs":[{"data":{"text/plain":["array([2.92896825, 2.01987734, 1.60321068, 1.34680042, 1.16822899,\n","       1.03489566, 0.93072899, 0.84669538, 0.77725094, 0.7187714 ])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["bh=np.dot(H,xtrue.T)\n","bh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yL39kr1MHCA1"},"outputs":[],"source":["bh2=np.array([ 3.,  2.01987734,  1.60321068,  1.34680042,  1.16822899,\n","        1., 1.,  0.84669538,  0.77725094,  0.7187714 ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gQEtXexHCA2"},"outputs":[],"source":["x1=la.solve(H,bh)\n","x2=la.solve(H,bh2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YI_ZTlkNHCA2","outputId":"e66e7e14-3961-4abd-cc9b-ed08c4408ffb"},"outputs":[{"data":{"text/plain":["array([1.        , 0.99999996, 1.00000087, 0.99999211, 1.00003756,\n","       0.99989682, 1.00016919, 0.99983655, 1.00008579, 0.99998114])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["x1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"940tR5z0HCA2","outputId":"d86b7f60-4141-47a2-b243-29401069f3c2"},"outputs":[{"data":{"text/plain":["array([ 8.85723060e+05, -7.63363576e+07,  1.62228229e+09, -1.47187023e+10,\n","        7.00801059e+10, -1.92341747e+11,  3.15120079e+11, -3.04131498e+11,\n","        1.59478539e+11, -3.50342060e+10])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["x2"]},{"cell_type":"markdown","metadata":{"id":"frCJZ_oEHCA2"},"source":["## Sparse matrices"]},{"cell_type":"markdown","metadata":{"id":"cDlDYxViHCA2"},"source":["Sparse matrices are often useful in numerical simulations dealing with large systems, if the problem can be described in matrix form where the matrices or vectors mostly contains zeros. Scipy has a good support for sparse matrices, with basic linear algebra operations (such as equation solving, eigenvalue calculations, etc).\n","\n","There are many possible strategies for storing sparse matrices in an efficient way. Some of the most common are the so-called coordinate form (COO), list of list (LIL) form,  and compressed-sparse column CSC (and row, CSR). Each format has some advantanges and disadvantages. Most computational algorithms (equation solving, matrix-matrix multiplication, etc) can be efficiently implemented using CSR or CSC formats, but they are not so intuitive and not so easy to initialize. So often a sparse matrix is initially created in COO or LIL format (where we can efficiently add elements to the sparse matrix data), and then converted to CSC or CSR before used in real calcalations.\n","\n","For more information about these sparse formats, see e.g. http://en.wikipedia.org/wiki/Sparse_matrix\n","\n","When we create a sparse matrix we have to choose which format it should be stored in. For example, "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gP1yXc1uHCA3","executionInfo":{"status":"ok","timestamp":1663601344940,"user_tz":-120,"elapsed":285,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}}},"outputs":[],"source":["import scipy.sparse as sp"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7Mtr80DHCA3","executionInfo":{"status":"ok","timestamp":1663601342020,"user_tz":-120,"elapsed":327,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}},"outputId":"901b7a8e-055d-4afd-b639-89b36082e12a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2, -1,  0,  0,  0],\n","       [-1,  2, -1,  0,  0],\n","       [ 0, -1,  2, -1,  0],\n","       [ 0,  0, -1,  2, -1],\n","       [ 0,  0,  0, -1,  2]])"]},"metadata":{},"execution_count":3}],"source":["# dense matrix\n","M = np.array([[2,-1,0,0,0], [-1,2,-1,0,0], [0,-1,2,-1,0],[0,0,-1,2,-1], [0,0,0,-1,2]]); \n","M"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDrXlBWLHCA3","executionInfo":{"status":"ok","timestamp":1663601357983,"user_tz":-120,"elapsed":424,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}},"outputId":"ea15fb56-04dc-4471-ef46-cbe7a1ac0ca6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0  2  5  8 11 13]\n","[0 1 0 1 2 1 2 3 2 3 4 3 4]\n","[ 2 -1 -1  2 -1 -1  2 -1 -1  2 -1 -1  2]\n"]}],"source":["# convert from dense to sparse\n","A = sp.csr_matrix(M); \n","#print(A)\n","print(A.indptr)\n","print(A.indices)\n","print(A.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27XhNRVeHCA4","outputId":"26138c0c-63be-46e5-eebd-f1524f0b55b0"},"outputs":[{"data":{"text/plain":["matrix([[ 2, -1,  0,  0,  0],\n","        [-1,  2, -1,  0,  0],\n","        [ 0, -1,  2, -1,  0],\n","        [ 0,  0, -1,  2, -1],\n","        [ 0,  0,  0, -1,  2]], dtype=int64)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# convert from sparse to dense\n","A.todense()"]},{"cell_type":"markdown","metadata":{"id":"O11XDfyNHCA4"},"source":["More efficient way to create sparse matrices: create an empty matrix and populate it using matrix indexing (avoids creating a potentially large dense matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptTH_8psHCA4","outputId":"189fcb77-f496-437f-ccbd-80228360e3f1"},"outputs":[{"data":{"text/plain":["<4x4 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 6 stored elements in List of Lists format>"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["A = sp.lil_matrix((4,4)) # empty 4x4 sparse matrix\n","A[0,0] = 1\n","A[1,1] = 3\n","A[2,2] = A[2,1] = 1\n","A[3,3] = A[3,0] = 1\n","A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FII_OUODHCA4","outputId":"b73dcdb6-d3e0-429b-bfb9-c648f1eae015"},"outputs":[{"data":{"text/plain":["array([list([0]), list([1]), list([1, 2]), list([0, 3])], dtype=object)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["A.rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qc8OLzkWHCA4"},"outputs":[],"source":["A.todense()"]},{"cell_type":"markdown","metadata":{"id":"bGaCMHNyHCA4"},"source":["Converting between different sparse matrix formats:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbnWEsjCHCA5"},"outputs":[],"source":["A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZC26RYrzHCA5","outputId":"a94d4df3-55c2-4726-8b11-241b9ec95abc"},"outputs":[{"data":{"text/plain":["<4x4 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 6 stored elements in Compressed Sparse Row format>"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["A = sp.csr_matrix(A); A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7HWCUafHCA5"},"outputs":[],"source":["A = sp.csc_matrix(A); A"]},{"cell_type":"markdown","metadata":{"id":"reRc6XzLHCA5"},"source":["We can compute with sparse matrices like with dense matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HDAjI8HHCA5"},"outputs":[],"source":["A.todense()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5W8Dv4THCA5","outputId":"17802517-c322-409c-d08e-a8ba7f62b4b8"},"outputs":[{"data":{"text/plain":["<4x4 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 6 stored elements in Compressed Sparse Row format>"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["(A * A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nhb6g8IzHCA5"},"outputs":[],"source":["A.todense()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZxyLrb2HCA5","outputId":"9d8cad27-63e8-40f0-f883-db9c44e9dfaa"},"outputs":[{"data":{"text/plain":["<4x4 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 6 stored elements in Compressed Sparse Row format>"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["A.dot(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsgIPvIWHCA6","outputId":"d4c3b8d0-81ec-4a59-aaf3-4d9522827cea"},"outputs":[{"data":{"text/plain":["array([[1],\n","       [2],\n","       [3],\n","       [4]])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["v = np.array([1,2,3,4])[:,np.newaxis]; v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meK8Q35lHCA6","outputId":"b43e16a1-bce6-4991-862c-e900673fcee9"},"outputs":[{"data":{"text/plain":["array([[1.],\n","       [6.],\n","       [5.],\n","       [5.]])"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# sparse matrix - dense vector multiplication\n","A * v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FYGuohpHCA6"},"outputs":[],"source":["# same result with dense matrix - dense vector multiplcation\n","A.todense() * v"]},{"cell_type":"markdown","metadata":{"id":"LPRfAbCrHCA6"},"source":["### Tridiagonal matrix solver\n","\n","See a separate [notebook ](Laplacian1D.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"rMXxT_0mHCA6"},"source":["### Sparse solver"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3XElSiIHCA6","executionInfo":{"status":"ok","timestamp":1663601505326,"user_tz":-120,"elapsed":1122,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}},"outputId":"ad05a7d0-4d72-4485-e549-b70effbb7d91"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1  5  0  0  0]\n"," [ 0  2  8  0  0]\n"," [ 0  0  3  9  0]\n"," [ 0  0  0  4 10]\n"," [ 0  0  0  0  5]]\n"]}],"source":["import numpy as np\n","from scipy import sparse\n","from scipy.sparse.linalg import spsolve\n","mtx = sparse.spdiags([[1, 2, 3, 4, 5], [6, 5, 8, 9, 10]], [0, 1], 5, 5)\n","print(mtx.todense())\n","mtx=mtx.tocsr()\n","#mtx=sparse.csr_matrix(mtx)\n","rhs = np.array([1, 2, 3, 4, 5], dtype=np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLtOzCO5HCA6"},"outputs":[],"source":["help(spsolve)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8Quyc6fHCA7"},"outputs":[],"source":["# solve as single precision real\n","mtx1 = mtx.astype(np.float32)\n","x = spsolve(mtx1, rhs, use_umfpack=True)\n","print(x)  \n","\n","print(\"Error: %s\" % (mtx1 * x - rhs))  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXZAG4FaHCA7"},"outputs":[],"source":["# solve as double precision real\n","mtx2 = mtx.astype(np.float64)\n","x = spsolve(mtx2, rhs, use_umfpack=True)\n","print(x)  \n","\n","print(\"Error: %s\" % (mtx2 * x - rhs))  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrPCzeSQHCA7"},"outputs":[],"source":["#solve as double precision complex\n","mtx2 = mtx.astype(np.complex128)\n","x = spsolve(mtx2, rhs, use_umfpack=True)\n","print(x)\n","\n","print(\"Error: %s\" % (mtx2 * x - rhs))   "]},{"cell_type":"markdown","metadata":{"id":"qgfoOlmIHCA7"},"source":["### Iterative solvers"]},{"cell_type":"code","source":["help(sparse.linalg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMAKmmUZYs55","executionInfo":{"status":"ok","timestamp":1663601670929,"user_tz":-120,"elapsed":429,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}},"outputId":"b586f856-e495-4121-d488-d79824eebd4f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on package scipy.sparse.linalg in scipy.sparse:\n","\n","NAME\n","    scipy.sparse.linalg\n","\n","DESCRIPTION\n","    Sparse linear algebra (:mod:`scipy.sparse.linalg`)\n","    ==================================================\n","    \n","    .. currentmodule:: scipy.sparse.linalg\n","    \n","    Abstract linear operators\n","    -------------------------\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       LinearOperator -- abstract representation of a linear operator\n","       aslinearoperator -- convert an object to an abstract linear operator\n","    \n","    Matrix Operations\n","    -----------------\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       inv -- compute the sparse matrix inverse\n","       expm -- compute the sparse matrix exponential\n","       expm_multiply -- compute the product of a matrix exponential and a matrix\n","    \n","    Matrix norms\n","    ------------\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       norm -- Norm of a sparse matrix\n","       onenormest -- Estimate the 1-norm of a sparse matrix\n","    \n","    Solving linear problems\n","    -----------------------\n","    \n","    Direct methods for linear equation systems:\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       spsolve -- Solve the sparse linear system Ax=b\n","       spsolve_triangular -- Solve the sparse linear system Ax=b for a triangular matrix\n","       factorized -- Pre-factorize matrix to a function solving a linear system\n","       MatrixRankWarning -- Warning on exactly singular matrices\n","       use_solver -- Select direct solver to use\n","    \n","    Iterative methods for linear equation systems:\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       bicg -- Use BIConjugate Gradient iteration to solve A x = b\n","       bicgstab -- Use BIConjugate Gradient STABilized iteration to solve A x = b\n","       cg -- Use Conjugate Gradient iteration to solve A x = b\n","       cgs -- Use Conjugate Gradient Squared iteration to solve A x = b\n","       gmres -- Use Generalized Minimal RESidual iteration to solve A x = b\n","       lgmres -- Solve a matrix equation using the LGMRES algorithm\n","       minres -- Use MINimum RESidual iteration to solve Ax = b\n","       qmr -- Use Quasi-Minimal Residual iteration to solve A x = b\n","       gcrotmk -- Solve a matrix equation using the GCROT(m,k) algorithm\n","    \n","    Iterative methods for least-squares problems:\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       lsqr -- Find the least-squares solution to a sparse linear equation system\n","       lsmr -- Find the least-squares solution to a sparse linear equation system\n","    \n","    Matrix factorizations\n","    ---------------------\n","    \n","    Eigenvalue problems:\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       eigs -- Find k eigenvalues and eigenvectors of the square matrix A\n","       eigsh -- Find k eigenvalues and eigenvectors of a symmetric matrix\n","       lobpcg -- Solve symmetric partial eigenproblems with optional preconditioning\n","    \n","    Singular values problems:\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       svds -- Compute k singular values/vectors for a sparse matrix\n","    \n","    Complete or incomplete LU factorizations\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       splu -- Compute a LU decomposition for a sparse matrix\n","       spilu -- Compute an incomplete LU decomposition for a sparse matrix\n","       SuperLU -- Object representing an LU factorization\n","    \n","    Exceptions\n","    ----------\n","    \n","    .. autosummary::\n","       :toctree: generated/\n","    \n","       ArpackNoConvergence\n","       ArpackError\n","\n","PACKAGE CONTENTS\n","    _expm_multiply\n","    _norm\n","    _onenormest\n","    dsolve (package)\n","    eigen (package)\n","    interface\n","    isolve (package)\n","    matfuncs\n","    setup\n","    tests (package)\n","\n","SUBMODULES\n","    arpack\n","    iterative\n","    linsolve\n","    utils\n","\n","CLASSES\n","    builtins.RuntimeError(builtins.Exception)\n","        scipy.sparse.linalg.eigen.arpack.arpack.ArpackError\n","            scipy.sparse.linalg.eigen.arpack.arpack.ArpackNoConvergence\n","    builtins.UserWarning(builtins.Warning)\n","        scipy.sparse.linalg.dsolve.linsolve.MatrixRankWarning\n","    builtins.object\n","        builtins.SuperLU\n","        scipy.sparse.linalg.interface.LinearOperator\n","    \n","    class ArpackError(builtins.RuntimeError)\n","     |  ArpackError(info, infodict={'d': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3,4.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}, 's': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3,4.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}, 'z': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}, 'c': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}})\n","     |  \n","     |  ARPACK error\n","     |  \n","     |  Method resolution order:\n","     |      ArpackError\n","     |      builtins.RuntimeError\n","     |      builtins.Exception\n","     |      builtins.BaseException\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, info, infodict={'d': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3,4.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}, 's': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3,4.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}, 'z': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}, 'c': {0: 'Normal exit.', 1: 'Maximum number of iterations taken. All possible eigenvalues of OP has been found. IPARAM(5) returns the number of wanted converged Ritz values.', 2: 'No longer an informational error. Deprecated starting with release 2 of ARPACK.', 3: 'No shifts could be applied during a cycle of the Implicitly restarted Arnoldi iteration. One possibility is to increase the size of NCV relative to NEV. ', -1: 'N must be positive.', -2: 'NEV must be positive.', -3: 'NCV-NEV >= 2 and less than or equal to N.', -4: 'The maximum number of Arnoldi update iterations allowed must be greater than zero.', -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\", -6: \"BMAT must be one of 'I' or 'G'.\", -7: 'Length of private work array WORKL is not sufficient.', -8: 'Error return from LAPACK eigenvalue calculation;', -9: 'Starting vector is zero.', -10: 'IPARAM(7) must be 1,2,3.', -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\", -12: 'IPARAM(1) must be equal to 0 or 1.', -13: \"NEV and WHICH = 'BE' are incompatible.\", -9999: 'Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.'}})\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from builtins.RuntimeError:\n","     |  \n","     |  __new__(*args, **kwargs) from builtins.type\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from builtins.BaseException:\n","     |  \n","     |  __delattr__(self, name, /)\n","     |      Implement delattr(self, name).\n","     |  \n","     |  __getattribute__(self, name, /)\n","     |      Return getattr(self, name).\n","     |  \n","     |  __reduce__(...)\n","     |      Helper for pickle.\n","     |  \n","     |  __repr__(self, /)\n","     |      Return repr(self).\n","     |  \n","     |  __setattr__(self, name, value, /)\n","     |      Implement setattr(self, name, value).\n","     |  \n","     |  __setstate__(...)\n","     |  \n","     |  __str__(self, /)\n","     |      Return str(self).\n","     |  \n","     |  with_traceback(...)\n","     |      Exception.with_traceback(tb) --\n","     |      set self.__traceback__ to tb and return self.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from builtins.BaseException:\n","     |  \n","     |  __cause__\n","     |      exception cause\n","     |  \n","     |  __context__\n","     |      exception context\n","     |  \n","     |  __dict__\n","     |  \n","     |  __suppress_context__\n","     |  \n","     |  __traceback__\n","     |  \n","     |  args\n","    \n","    class ArpackNoConvergence(ArpackError)\n","     |  ArpackNoConvergence(msg, eigenvalues, eigenvectors)\n","     |  \n","     |  ARPACK iteration did not converge\n","     |  \n","     |  Attributes\n","     |  ----------\n","     |  eigenvalues : ndarray\n","     |      Partial result. Converged eigenvalues.\n","     |  eigenvectors : ndarray\n","     |      Partial result. Converged eigenvectors.\n","     |  \n","     |  Method resolution order:\n","     |      ArpackNoConvergence\n","     |      ArpackError\n","     |      builtins.RuntimeError\n","     |      builtins.Exception\n","     |      builtins.BaseException\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, msg, eigenvalues, eigenvectors)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from ArpackError:\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from builtins.RuntimeError:\n","     |  \n","     |  __new__(*args, **kwargs) from builtins.type\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from builtins.BaseException:\n","     |  \n","     |  __delattr__(self, name, /)\n","     |      Implement delattr(self, name).\n","     |  \n","     |  __getattribute__(self, name, /)\n","     |      Return getattr(self, name).\n","     |  \n","     |  __reduce__(...)\n","     |      Helper for pickle.\n","     |  \n","     |  __repr__(self, /)\n","     |      Return repr(self).\n","     |  \n","     |  __setattr__(self, name, value, /)\n","     |      Implement setattr(self, name, value).\n","     |  \n","     |  __setstate__(...)\n","     |  \n","     |  __str__(self, /)\n","     |      Return str(self).\n","     |  \n","     |  with_traceback(...)\n","     |      Exception.with_traceback(tb) --\n","     |      set self.__traceback__ to tb and return self.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from builtins.BaseException:\n","     |  \n","     |  __cause__\n","     |      exception cause\n","     |  \n","     |  __context__\n","     |      exception context\n","     |  \n","     |  __dict__\n","     |  \n","     |  __suppress_context__\n","     |  \n","     |  __traceback__\n","     |  \n","     |  args\n","    \n","    class LinearOperator(builtins.object)\n","     |  LinearOperator(*args, **kwargs)\n","     |  \n","     |  Common interface for performing matrix vector products\n","     |  \n","     |  Many iterative methods (e.g. cg, gmres) do not need to know the\n","     |  individual entries of a matrix to solve a linear system A*x=b.\n","     |  Such solvers only require the computation of matrix vector\n","     |  products, A*v where v is a dense vector.  This class serves as\n","     |  an abstract interface between iterative solvers and matrix-like\n","     |  objects.\n","     |  \n","     |  To construct a concrete LinearOperator, either pass appropriate\n","     |  callables to the constructor of this class, or subclass it.\n","     |  \n","     |  A subclass must implement either one of the methods ``_matvec``\n","     |  and ``_matmat``, and the attributes/properties ``shape`` (pair of\n","     |  integers) and ``dtype`` (may be None). It may call the ``__init__``\n","     |  on this class to have these attributes validated. Implementing\n","     |  ``_matvec`` automatically implements ``_matmat`` (using a naive\n","     |  algorithm) and vice-versa.\n","     |  \n","     |  Optionally, a subclass may implement ``_rmatvec`` or ``_adjoint``\n","     |  to implement the Hermitian adjoint (conjugate transpose). As with\n","     |  ``_matvec`` and ``_matmat``, implementing either ``_rmatvec`` or\n","     |  ``_adjoint`` implements the other automatically. Implementing\n","     |  ``_adjoint`` is preferable; ``_rmatvec`` is mostly there for\n","     |  backwards compatibility.\n","     |  \n","     |  Parameters\n","     |  ----------\n","     |  shape : tuple\n","     |      Matrix dimensions (M, N).\n","     |  matvec : callable f(v)\n","     |      Returns returns A * v.\n","     |  rmatvec : callable f(v)\n","     |      Returns A^H * v, where A^H is the conjugate transpose of A.\n","     |  matmat : callable f(V)\n","     |      Returns A * V, where V is a dense matrix with dimensions (N, K).\n","     |  dtype : dtype\n","     |      Data type of the matrix.\n","     |  rmatmat : callable f(V)\n","     |      Returns A^H * V, where V is a dense matrix with dimensions (M, K).\n","     |  \n","     |  Attributes\n","     |  ----------\n","     |  args : tuple\n","     |      For linear operators describing products etc. of other linear\n","     |      operators, the operands of the binary operation.\n","     |  ndim : int\n","     |      Number of dimensions (this is always 2)\n","     |  \n","     |  See Also\n","     |  --------\n","     |  aslinearoperator : Construct LinearOperators\n","     |  \n","     |  Notes\n","     |  -----\n","     |  The user-defined matvec() function must properly handle the case\n","     |  where v has shape (N,) as well as the (N,1) case.  The shape of\n","     |  the return type is handled internally by LinearOperator.\n","     |  \n","     |  LinearOperator instances can also be multiplied, added with each\n","     |  other and exponentiated, all lazily: the result of these operations\n","     |  is always a new, composite LinearOperator, that defers linear\n","     |  operations to the original operators and combines the results.\n","     |  \n","     |  More details regarding how to subclass a LinearOperator and several\n","     |  examples of concrete LinearOperator instances can be found in the\n","     |  external project `PyLops <https://pylops.readthedocs.io>`_.\n","     |  \n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> import numpy as np\n","     |  >>> from scipy.sparse.linalg import LinearOperator\n","     |  >>> def mv(v):\n","     |  ...     return np.array([2*v[0], 3*v[1]])\n","     |  ...\n","     |  >>> A = LinearOperator((2,2), matvec=mv)\n","     |  >>> A\n","     |  <2x2 _CustomLinearOperator with dtype=float64>\n","     |  >>> A.matvec(np.ones(2))\n","     |  array([ 2.,  3.])\n","     |  >>> A * np.ones(2)\n","     |  array([ 2.,  3.])\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __add__(self, x)\n","     |  \n","     |  __call__(self, x)\n","     |      Call self as a function.\n","     |  \n","     |  __init__(self, dtype, shape)\n","     |      Initialize this LinearOperator.\n","     |      \n","     |      To be called by subclasses. ``dtype`` may be None; ``shape`` should\n","     |      be convertible to a length-2 tuple.\n","     |  \n","     |  __matmul__(self, other)\n","     |  \n","     |  __mul__(self, x)\n","     |  \n","     |  __neg__(self)\n","     |  \n","     |  __pow__(self, p)\n","     |  \n","     |  __repr__(self)\n","     |      Return repr(self).\n","     |  \n","     |  __rmatmul__(self, other)\n","     |  \n","     |  __rmul__(self, x)\n","     |  \n","     |  __sub__(self, x)\n","     |  \n","     |  adjoint(self)\n","     |      Hermitian adjoint.\n","     |      \n","     |      Returns the Hermitian adjoint of self, aka the Hermitian\n","     |      conjugate or Hermitian transpose. For a complex matrix, the\n","     |      Hermitian adjoint is equal to the conjugate transpose.\n","     |      \n","     |      Can be abbreviated self.H instead of self.adjoint().\n","     |      \n","     |      Returns\n","     |      -------\n","     |      A_H : LinearOperator\n","     |          Hermitian adjoint of self.\n","     |  \n","     |  dot(self, x)\n","     |      Matrix-matrix or matrix-vector multiplication.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      x : array_like\n","     |          1-d or 2-d array, representing a vector or matrix.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      Ax : array\n","     |          1-d or 2-d array (depending on the shape of x) that represents\n","     |          the result of applying this linear operator on x.\n","     |  \n","     |  matmat(self, X)\n","     |      Matrix-matrix multiplication.\n","     |      \n","     |      Performs the operation y=A*X where A is an MxN linear\n","     |      operator and X dense N*K matrix or ndarray.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      X : {matrix, ndarray}\n","     |          An array with shape (N,K).\n","     |      \n","     |      Returns\n","     |      -------\n","     |      Y : {matrix, ndarray}\n","     |          A matrix or ndarray with shape (M,K) depending on\n","     |          the type of the X argument.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This matmat wraps any user-specified matmat routine or overridden\n","     |      _matmat method to ensure that y has the correct type.\n","     |  \n","     |  matvec(self, x)\n","     |      Matrix-vector multiplication.\n","     |      \n","     |      Performs the operation y=A*x where A is an MxN linear\n","     |      operator and x is a column vector or 1-d array.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      x : {matrix, ndarray}\n","     |          An array with shape (N,) or (N,1).\n","     |      \n","     |      Returns\n","     |      -------\n","     |      y : {matrix, ndarray}\n","     |          A matrix or ndarray with shape (M,) or (M,1) depending\n","     |          on the type and shape of the x argument.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This matvec wraps the user-specified matvec routine or overridden\n","     |      _matvec method to ensure that y has the correct shape and type.\n","     |  \n","     |  rmatmat(self, X)\n","     |      Adjoint matrix-matrix multiplication.\n","     |      \n","     |      Performs the operation y = A^H * x where A is an MxN linear\n","     |      operator and x is a column vector or 1-d array, or 2-d array.\n","     |      The default implementation defers to the adjoint.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      X : {matrix, ndarray}\n","     |          A matrix or 2D array.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      Y : {matrix, ndarray}\n","     |          A matrix or 2D array depending on the type of the input.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This rmatmat wraps the user-specified rmatmat routine.\n","     |  \n","     |  rmatvec(self, x)\n","     |      Adjoint matrix-vector multiplication.\n","     |      \n","     |      Performs the operation y = A^H * x where A is an MxN linear\n","     |      operator and x is a column vector or 1-d array.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      x : {matrix, ndarray}\n","     |          An array with shape (M,) or (M,1).\n","     |      \n","     |      Returns\n","     |      -------\n","     |      y : {matrix, ndarray}\n","     |          A matrix or ndarray with shape (N,) or (N,1) depending\n","     |          on the type and shape of the x argument.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n","     |      _rmatvec method to ensure that y has the correct shape and type.\n","     |  \n","     |  transpose(self)\n","     |      Transpose this linear operator.\n","     |      \n","     |      Returns a LinearOperator that represents the transpose of this one.\n","     |      Can be abbreviated self.T instead of self.transpose().\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods defined here:\n","     |  \n","     |  __new__(cls, *args, **kwargs)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  H\n","     |      Hermitian adjoint.\n","     |      \n","     |      Returns the Hermitian adjoint of self, aka the Hermitian\n","     |      conjugate or Hermitian transpose. For a complex matrix, the\n","     |      Hermitian adjoint is equal to the conjugate transpose.\n","     |      \n","     |      Can be abbreviated self.H instead of self.adjoint().\n","     |      \n","     |      Returns\n","     |      -------\n","     |      A_H : LinearOperator\n","     |          Hermitian adjoint of self.\n","     |  \n","     |  T\n","     |      Transpose this linear operator.\n","     |      \n","     |      Returns a LinearOperator that represents the transpose of this one.\n","     |      Can be abbreviated self.T instead of self.transpose().\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  ndim = 2\n","    \n","    class MatrixRankWarning(builtins.UserWarning)\n","     |  Base class for warnings generated by user code.\n","     |  \n","     |  Method resolution order:\n","     |      MatrixRankWarning\n","     |      builtins.UserWarning\n","     |      builtins.Warning\n","     |      builtins.Exception\n","     |      builtins.BaseException\n","     |      builtins.object\n","     |  \n","     |  Data descriptors defined here:\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from builtins.UserWarning:\n","     |  \n","     |  __init__(self, /, *args, **kwargs)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from builtins.UserWarning:\n","     |  \n","     |  __new__(*args, **kwargs) from builtins.type\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from builtins.BaseException:\n","     |  \n","     |  __delattr__(self, name, /)\n","     |      Implement delattr(self, name).\n","     |  \n","     |  __getattribute__(self, name, /)\n","     |      Return getattr(self, name).\n","     |  \n","     |  __reduce__(...)\n","     |      Helper for pickle.\n","     |  \n","     |  __repr__(self, /)\n","     |      Return repr(self).\n","     |  \n","     |  __setattr__(self, name, value, /)\n","     |      Implement setattr(self, name, value).\n","     |  \n","     |  __setstate__(...)\n","     |  \n","     |  __str__(self, /)\n","     |      Return str(self).\n","     |  \n","     |  with_traceback(...)\n","     |      Exception.with_traceback(tb) --\n","     |      set self.__traceback__ to tb and return self.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from builtins.BaseException:\n","     |  \n","     |  __cause__\n","     |      exception cause\n","     |  \n","     |  __context__\n","     |      exception context\n","     |  \n","     |  __dict__\n","     |  \n","     |  __suppress_context__\n","     |  \n","     |  __traceback__\n","     |  \n","     |  args\n","    \n","    class SuperLU(object)\n","     |  LU factorization of a sparse matrix.\n","     |  \n","     |  Factorization is represented as::\n","     |  \n","     |      Pr * A * Pc = L * U\n","     |  \n","     |  To construct these `SuperLU` objects, call the `splu` and `spilu`\n","     |  functions.\n","     |  \n","     |  Attributes\n","     |  ----------\n","     |  shape\n","     |  nnz\n","     |  perm_c\n","     |  perm_r\n","     |  L\n","     |  U\n","     |  \n","     |  Methods\n","     |  -------\n","     |  solve\n","     |  \n","     |  Notes\n","     |  -----\n","     |  \n","     |  .. versionadded:: 0.14.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  The LU decomposition can be used to solve matrix equations. Consider:\n","     |  \n","     |  >>> import numpy as np\n","     |  >>> from scipy.sparse import csc_matrix, linalg as sla\n","     |  >>> A = csc_matrix([[1,2,0,4],[1,0,0,1],[1,0,2,1],[2,2,1,0.]])\n","     |  \n","     |  This can be solved for a given right-hand side:\n","     |  \n","     |  >>> lu = sla.splu(A)\n","     |  >>> b = np.array([1, 2, 3, 4])\n","     |  >>> x = lu.solve(b)\n","     |  >>> A.dot(x)\n","     |  array([ 1.,  2.,  3.,  4.])\n","     |  \n","     |  The ``lu`` object also contains an explicit representation of the\n","     |  decomposition. The permutations are represented as mappings of\n","     |  indices:\n","     |  \n","     |  >>> lu.perm_r\n","     |  array([0, 2, 1, 3], dtype=int32)\n","     |  >>> lu.perm_c\n","     |  array([2, 0, 1, 3], dtype=int32)\n","     |  \n","     |  The L and U factors are sparse matrices in CSC format:\n","     |  \n","     |  >>> lu.L.A\n","     |  array([[ 1. ,  0. ,  0. ,  0. ],\n","     |         [ 0. ,  1. ,  0. ,  0. ],\n","     |         [ 0. ,  0. ,  1. ,  0. ],\n","     |         [ 1. ,  0.5,  0.5,  1. ]])\n","     |  >>> lu.U.A\n","     |  array([[ 2.,  0.,  1.,  4.],\n","     |         [ 0.,  2.,  1.,  1.],\n","     |         [ 0.,  0.,  1.,  1.],\n","     |         [ 0.,  0.,  0., -5.]])\n","     |  \n","     |  The permutation matrices can be constructed:\n","     |  \n","     |  >>> Pr = csc_matrix((np.ones(4), (lu.perm_r, np.arange(4))))\n","     |  >>> Pc = csc_matrix((np.ones(4), (np.arange(4), lu.perm_c)))\n","     |  \n","     |  We can reassemble the original matrix:\n","     |  \n","     |  >>> (Pr.T * (lu.L * lu.U) * Pc.T).A\n","     |  array([[ 1.,  2.,  0.,  4.],\n","     |         [ 1.,  0.,  0.,  1.],\n","     |         [ 1.,  0.,  2.,  1.],\n","     |         [ 2.,  2.,  1.,  0.]])\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  solve(...)\n","     |      solve(rhs[, trans])\n","     |      \n","     |      Solves linear system of equations with one or several right-hand sides.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      rhs : ndarray, shape (n,) or (n, k)\n","     |          Right hand side(s) of equation\n","     |      trans : {'N', 'T', 'H'}, optional\n","     |          Type of system to solve::\n","     |      \n","     |              'N':   A   * x == rhs  (default)\n","     |              'T':   A^T * x == rhs\n","     |              'H':   A^H * x == rhs\n","     |      \n","     |          i.e., normal, transposed, and hermitian conjugate.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      x : ndarray, shape ``rhs.shape``\n","     |          Solution vector(s)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  L\n","     |      Lower triangular factor with unit diagonal as a\n","     |      `scipy.sparse.csc_matrix`.\n","     |      \n","     |      .. versionadded:: 0.14.0\n","     |  \n","     |  U\n","     |      Upper triangular factor as a `scipy.sparse.csc_matrix`.\n","     |      \n","     |      .. versionadded:: 0.14.0\n","     |  \n","     |  nnz\n","     |      Number of nonzero elements in the matrix.\n","     |  \n","     |  perm_c\n","     |      Permutation Pc represented as an array of indices.\n","     |      \n","     |      The column permutation matrix can be reconstructed via:\n","     |      \n","     |      >>> Pc = np.zeros((n, n))\n","     |      >>> Pc[np.arange(n), perm_c] = 1\n","     |  \n","     |  perm_r\n","     |      Permutation Pr represented as an array of indices.\n","     |      \n","     |      The row permutation matrix can be reconstructed via:\n","     |      \n","     |      >>> Pr = np.zeros((n, n))\n","     |      >>> Pr[perm_r, np.arange(n)] = 1\n","     |  \n","     |  shape\n","     |      Shape of the original matrix as a tuple of ints.\n","\n","FUNCTIONS\n","    aslinearoperator(A)\n","        Return A as a LinearOperator.\n","        \n","        'A' may be any of the following types:\n","         - ndarray\n","         - matrix\n","         - sparse matrix (e.g. csr_matrix, lil_matrix, etc.)\n","         - LinearOperator\n","         - An object with .shape and .matvec attributes\n","        \n","        See the LinearOperator documentation for additional information.\n","        \n","        Notes\n","        -----\n","        If 'A' has no .dtype attribute, the data type is determined by calling\n","        :func:`LinearOperator.matvec()` - set the .dtype attribute to prevent this\n","        call upon the linear operator creation.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse.linalg import aslinearoperator\n","        >>> M = np.array([[1,2,3],[4,5,6]], dtype=np.int32)\n","        >>> aslinearoperator(M)\n","        <2x3 MatrixLinearOperator with dtype=int32>\n","    \n","    bicg(A, b, x0=None, tol=1e-05, maxiter=None, M=None, callback=None, atol=None)\n","        Use BIConjugate Gradient iteration to solve ``Ax = b``.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real or complex N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` and ``A^T x`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : integer\n","            Provides convergence information:\n","                0  : successful exit\n","                >0 : convergence to tolerance not achieved, number of iterations\n","                <0 : illegal input or breakdown\n","        \n","        Other Parameters\n","        ----------------\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is ``'legacy'``, which emulates\n","            a different legacy behavior.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : integer\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A.  Effective preconditioning dramatically improves the\n","            rate of convergence, which implies that fewer iterations are needed\n","            to reach a given error tolerance.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","        \n","        \n","        \n","                       \n","                       Examples\n","                       --------\n","                       >>> from scipy.sparse import csc_matrix\n","                       >>> from scipy.sparse.linalg import bicg\n","                       >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\n","                       >>> b = np.array([2, 4, -1], dtype=float)\n","                       >>> x, exitCode = bicg(A, b)\n","                       >>> print(exitCode)            # 0 indicates successful convergence\n","                       0\n","                       >>> np.allclose(A.dot(x), b)\n","                       True\n","    \n","    bicgstab(A, b, x0=None, tol=1e-05, maxiter=None, M=None, callback=None, atol=None)\n","        Use BIConjugate Gradient STABilized iteration to solve ``Ax = b``.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real or complex N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : integer\n","            Provides convergence information:\n","                0  : successful exit\n","                >0 : convergence to tolerance not achieved, number of iterations\n","                <0 : illegal input or breakdown\n","        \n","        Other Parameters\n","        ----------------\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is ``'legacy'``, which emulates\n","            a different legacy behavior.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : integer\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A.  Effective preconditioning dramatically improves the\n","            rate of convergence, which implies that fewer iterations are needed\n","            to reach a given error tolerance.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","    \n","    cg(A, b, x0=None, tol=1e-05, maxiter=None, M=None, callback=None, atol=None)\n","        Use Conjugate Gradient iteration to solve ``Ax = b``.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real or complex N-by-N matrix of the linear system.\n","            ``A`` must represent a hermitian, positive definite matrix.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : integer\n","            Provides convergence information:\n","                0  : successful exit\n","                >0 : convergence to tolerance not achieved, number of iterations\n","                <0 : illegal input or breakdown\n","        \n","        Other Parameters\n","        ----------------\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is ``'legacy'``, which emulates\n","            a different legacy behavior.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : integer\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A.  Effective preconditioning dramatically improves the\n","            rate of convergence, which implies that fewer iterations are needed\n","            to reach a given error tolerance.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","    \n","    cgs(A, b, x0=None, tol=1e-05, maxiter=None, M=None, callback=None, atol=None)\n","        Use Conjugate Gradient Squared iteration to solve ``Ax = b``.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real-valued N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : integer\n","            Provides convergence information:\n","                0  : successful exit\n","                >0 : convergence to tolerance not achieved, number of iterations\n","                <0 : illegal input or breakdown\n","        \n","        Other Parameters\n","        ----------------\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is ``'legacy'``, which emulates\n","            a different legacy behavior.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : integer\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A.  Effective preconditioning dramatically improves the\n","            rate of convergence, which implies that fewer iterations are needed\n","            to reach a given error tolerance.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","    \n","    eigs(A, k=6, M=None, sigma=None, which='LM', v0=None, ncv=None, maxiter=None, tol=0, return_eigenvectors=True, Minv=None, OPinv=None, OPpart=None)\n","        Find k eigenvalues and eigenvectors of the square matrix A.\n","        \n","        Solves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem\n","        for w[i] eigenvalues with corresponding eigenvectors x[i].\n","        \n","        If M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the\n","        generalized eigenvalue problem for w[i] eigenvalues\n","        with corresponding eigenvectors x[i]\n","        \n","        Parameters\n","        ----------\n","        A : ndarray, sparse matrix or LinearOperator\n","            An array, sparse matrix, or LinearOperator representing\n","            the operation ``A * x``, where A is a real or complex square matrix.\n","        k : int, optional\n","            The number of eigenvalues and eigenvectors desired.\n","            `k` must be smaller than N-1. It is not possible to compute all\n","            eigenvectors of a matrix.\n","        M : ndarray, sparse matrix or LinearOperator, optional\n","            An array, sparse matrix, or LinearOperator representing\n","            the operation M*x for the generalized eigenvalue problem\n","        \n","                A * x = w * M * x.\n","        \n","            M must represent a real symmetric matrix if A is real, and must\n","            represent a complex Hermitian matrix if A is complex. For best\n","            results, the data type of M should be the same as that of A.\n","            Additionally:\n","        \n","                If `sigma` is None, M is positive definite\n","        \n","                If sigma is specified, M is positive semi-definite\n","        \n","            If sigma is None, eigs requires an operator to compute the solution\n","            of the linear equation ``M * x = b``.  This is done internally via a\n","            (sparse) LU decomposition for an explicit matrix M, or via an\n","            iterative solver for a general linear operator.  Alternatively,\n","            the user can supply the matrix or operator Minv, which gives\n","            ``x = Minv * b = M^-1 * b``.\n","        sigma : real or complex, optional\n","            Find eigenvalues near sigma using shift-invert mode.  This requires\n","            an operator to compute the solution of the linear system\n","            ``[A - sigma * M] * x = b``, where M is the identity matrix if\n","            unspecified. This is computed internally via a (sparse) LU\n","            decomposition for explicit matrices A & M, or via an iterative\n","            solver if either A or M is a general linear operator.\n","            Alternatively, the user can supply the matrix or operator OPinv,\n","            which gives ``x = OPinv * b = [A - sigma * M]^-1 * b``.\n","            For a real matrix A, shift-invert can either be done in imaginary\n","            mode or real mode, specified by the parameter OPpart ('r' or 'i').\n","            Note that when sigma is specified, the keyword 'which' (below)\n","            refers to the shifted eigenvalues ``w'[i]`` where:\n","        \n","                If A is real and OPpart == 'r' (default),\n","                  ``w'[i] = 1/2 * [1/(w[i]-sigma) + 1/(w[i]-conj(sigma))]``.\n","        \n","                If A is real and OPpart == 'i',\n","                  ``w'[i] = 1/2i * [1/(w[i]-sigma) - 1/(w[i]-conj(sigma))]``.\n","        \n","                If A is complex, ``w'[i] = 1/(w[i]-sigma)``.\n","        \n","        v0 : ndarray, optional\n","            Starting vector for iteration.\n","            Default: random\n","        ncv : int, optional\n","            The number of Lanczos vectors generated\n","            `ncv` must be greater than `k`; it is recommended that ``ncv > 2*k``.\n","            Default: ``min(n, max(2*k + 1, 20))``\n","        which : str, ['LM' | 'SM' | 'LR' | 'SR' | 'LI' | 'SI'], optional\n","            Which `k` eigenvectors and eigenvalues to find:\n","        \n","                'LM' : largest magnitude\n","        \n","                'SM' : smallest magnitude\n","        \n","                'LR' : largest real part\n","        \n","                'SR' : smallest real part\n","        \n","                'LI' : largest imaginary part\n","        \n","                'SI' : smallest imaginary part\n","        \n","            When sigma != None, 'which' refers to the shifted eigenvalues w'[i]\n","            (see discussion in 'sigma', above).  ARPACK is generally better\n","            at finding large values than small values.  If small eigenvalues are\n","            desired, consider using shift-invert mode for better performance.\n","        maxiter : int, optional\n","            Maximum number of Arnoldi update iterations allowed\n","            Default: ``n*10``\n","        tol : float, optional\n","            Relative accuracy for eigenvalues (stopping criterion)\n","            The default value of 0 implies machine precision.\n","        return_eigenvectors : bool, optional\n","            Return eigenvectors (True) in addition to eigenvalues\n","        Minv : ndarray, sparse matrix or LinearOperator, optional\n","            See notes in M, above.\n","        OPinv : ndarray, sparse matrix or LinearOperator, optional\n","            See notes in sigma, above.\n","        OPpart : {'r' or 'i'}, optional\n","            See notes in sigma, above\n","        \n","        Returns\n","        -------\n","        w : ndarray\n","            Array of k eigenvalues.\n","        v : ndarray\n","            An array of `k` eigenvectors.\n","            ``v[:, i]`` is the eigenvector corresponding to the eigenvalue w[i].\n","        \n","        Raises\n","        ------\n","        ArpackNoConvergence\n","            When the requested convergence is not obtained.\n","            The currently converged eigenvalues and eigenvectors can be found\n","            as ``eigenvalues`` and ``eigenvectors`` attributes of the exception\n","            object.\n","        \n","        See Also\n","        --------\n","        eigsh : eigenvalues and eigenvectors for symmetric matrix A\n","        svds : singular value decomposition for a matrix A\n","        \n","        Notes\n","        -----\n","        This function is a wrapper to the ARPACK [1]_ SNEUPD, DNEUPD, CNEUPD,\n","        ZNEUPD, functions which use the Implicitly Restarted Arnoldi Method to\n","        find the eigenvalues and eigenvectors [2]_.\n","        \n","        References\n","        ----------\n","        .. [1] ARPACK Software, http://www.caam.rice.edu/software/ARPACK/\n","        .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE:\n","           Solution of Large Scale Eigenvalue Problems by Implicitly Restarted\n","           Arnoldi Methods. SIAM, Philadelphia, PA, 1998.\n","        \n","        Examples\n","        --------\n","        Find 6 eigenvectors of the identity matrix:\n","        \n","        >>> from scipy.sparse.linalg import eigs\n","        >>> id = np.eye(13)\n","        >>> vals, vecs = eigs(id, k=6)\n","        >>> vals\n","        array([ 1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j])\n","        >>> vecs.shape\n","        (13, 6)\n","    \n","    eigsh(A, k=6, M=None, sigma=None, which='LM', v0=None, ncv=None, maxiter=None, tol=0, return_eigenvectors=True, Minv=None, OPinv=None, mode='normal')\n","        Find k eigenvalues and eigenvectors of the real symmetric square matrix\n","        or complex Hermitian matrix A.\n","        \n","        Solves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem for\n","        w[i] eigenvalues with corresponding eigenvectors x[i].\n","        \n","        If M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the\n","        generalized eigenvalue problem for w[i] eigenvalues\n","        with corresponding eigenvectors x[i].\n","        \n","        Note that there is no specialized routine for the case when A is a complex\n","        Hermitian matrix. In this case, ``eigsh()`` will call ``eigs()`` and return the\n","        real parts of the eigenvalues thus obtained.\n","        \n","        Parameters\n","        ----------\n","        A : ndarray, sparse matrix or LinearOperator\n","            A square operator representing the operation ``A * x``, where ``A`` is\n","            real symmetric or complex Hermitian. For buckling mode (see below)\n","            ``A`` must additionally be positive-definite.\n","        k : int, optional\n","            The number of eigenvalues and eigenvectors desired.\n","            `k` must be smaller than N. It is not possible to compute all\n","            eigenvectors of a matrix.\n","        \n","        Returns\n","        -------\n","        w : array\n","            Array of k eigenvalues.\n","        v : array\n","            An array representing the `k` eigenvectors.  The column ``v[:, i]`` is\n","            the eigenvector corresponding to the eigenvalue ``w[i]``.\n","        \n","        Other Parameters\n","        ----------------\n","        M : An N x N matrix, array, sparse matrix, or linear operator representing\n","            the operation ``M @ x`` for the generalized eigenvalue problem\n","        \n","                A @ x = w * M @ x.\n","        \n","            M must represent a real symmetric matrix if A is real, and must\n","            represent a complex Hermitian matrix if A is complex. For best\n","            results, the data type of M should be the same as that of A.\n","            Additionally:\n","        \n","                If sigma is None, M is symmetric positive definite.\n","        \n","                If sigma is specified, M is symmetric positive semi-definite.\n","        \n","                In buckling mode, M is symmetric indefinite.\n","        \n","            If sigma is None, eigsh requires an operator to compute the solution\n","            of the linear equation ``M @ x = b``. This is done internally via a\n","            (sparse) LU decomposition for an explicit matrix M, or via an\n","            iterative solver for a general linear operator.  Alternatively,\n","            the user can supply the matrix or operator Minv, which gives\n","            ``x = Minv @ b = M^-1 @ b``.\n","        sigma : real\n","            Find eigenvalues near sigma using shift-invert mode.  This requires\n","            an operator to compute the solution of the linear system\n","            ``[A - sigma * M] x = b``, where M is the identity matrix if\n","            unspecified.  This is computed internally via a (sparse) LU\n","            decomposition for explicit matrices A & M, or via an iterative\n","            solver if either A or M is a general linear operator.\n","            Alternatively, the user can supply the matrix or operator OPinv,\n","            which gives ``x = OPinv @ b = [A - sigma * M]^-1 @ b``.\n","            Note that when sigma is specified, the keyword 'which' refers to\n","            the shifted eigenvalues ``w'[i]`` where:\n","        \n","                if mode == 'normal', ``w'[i] = 1 / (w[i] - sigma)``.\n","        \n","                if mode == 'cayley', ``w'[i] = (w[i] + sigma) / (w[i] - sigma)``.\n","        \n","                if mode == 'buckling', ``w'[i] = w[i] / (w[i] - sigma)``.\n","        \n","            (see further discussion in 'mode' below)\n","        v0 : ndarray, optional\n","            Starting vector for iteration.\n","            Default: random\n","        ncv : int, optional\n","            The number of Lanczos vectors generated ncv must be greater than k and\n","            smaller than n; it is recommended that ``ncv > 2*k``.\n","            Default: ``min(n, max(2*k + 1, 20))``\n","        which : str ['LM' | 'SM' | 'LA' | 'SA' | 'BE']\n","            If A is a complex Hermitian matrix, 'BE' is invalid.\n","            Which `k` eigenvectors and eigenvalues to find:\n","        \n","                'LM' : Largest (in magnitude) eigenvalues.\n","        \n","                'SM' : Smallest (in magnitude) eigenvalues.\n","        \n","                'LA' : Largest (algebraic) eigenvalues.\n","        \n","                'SA' : Smallest (algebraic) eigenvalues.\n","        \n","                'BE' : Half (k/2) from each end of the spectrum.\n","        \n","            When k is odd, return one more (k/2+1) from the high end.\n","            When sigma != None, 'which' refers to the shifted eigenvalues ``w'[i]``\n","            (see discussion in 'sigma', above).  ARPACK is generally better\n","            at finding large values than small values.  If small eigenvalues are\n","            desired, consider using shift-invert mode for better performance.\n","        maxiter : int, optional\n","            Maximum number of Arnoldi update iterations allowed.\n","            Default: ``n*10``\n","        tol : float\n","            Relative accuracy for eigenvalues (stopping criterion).\n","            The default value of 0 implies machine precision.\n","        Minv : N x N matrix, array, sparse matrix, or LinearOperator\n","            See notes in M, above.\n","        OPinv : N x N matrix, array, sparse matrix, or LinearOperator\n","            See notes in sigma, above.\n","        return_eigenvectors : bool\n","            Return eigenvectors (True) in addition to eigenvalues.\n","            This value determines the order in which eigenvalues are sorted.\n","            The sort order is also dependent on the `which` variable.\n","        \n","                For which = 'LM' or 'SA':\n","                    If `return_eigenvectors` is True, eigenvalues are sorted by\n","                    algebraic value.\n","        \n","                    If `return_eigenvectors` is False, eigenvalues are sorted by\n","                    absolute value.\n","        \n","                For which = 'BE' or 'LA':\n","                    eigenvalues are always sorted by algebraic value.\n","        \n","                For which = 'SM':\n","                    If `return_eigenvectors` is True, eigenvalues are sorted by\n","                    algebraic value.\n","        \n","                    If `return_eigenvectors` is False, eigenvalues are sorted by\n","                    decreasing absolute value.\n","        \n","        mode : string ['normal' | 'buckling' | 'cayley']\n","            Specify strategy to use for shift-invert mode.  This argument applies\n","            only for real-valued A and sigma != None.  For shift-invert mode,\n","            ARPACK internally solves the eigenvalue problem\n","            ``OP * x'[i] = w'[i] * B * x'[i]``\n","            and transforms the resulting Ritz vectors x'[i] and Ritz values w'[i]\n","            into the desired eigenvectors and eigenvalues of the problem\n","            ``A * x[i] = w[i] * M * x[i]``.\n","            The modes are as follows:\n","        \n","                'normal' :\n","                    OP = [A - sigma * M]^-1 @ M,\n","                    B = M,\n","                    w'[i] = 1 / (w[i] - sigma)\n","        \n","                'buckling' :\n","                    OP = [A - sigma * M]^-1 @ A,\n","                    B = A,\n","                    w'[i] = w[i] / (w[i] - sigma)\n","        \n","                'cayley' :\n","                    OP = [A - sigma * M]^-1 @ [A + sigma * M],\n","                    B = M,\n","                    w'[i] = (w[i] + sigma) / (w[i] - sigma)\n","        \n","            The choice of mode will affect which eigenvalues are selected by\n","            the keyword 'which', and can also impact the stability of\n","            convergence (see [2] for a discussion).\n","        \n","        Raises\n","        ------\n","        ArpackNoConvergence\n","            When the requested convergence is not obtained.\n","        \n","            The currently converged eigenvalues and eigenvectors can be found\n","            as ``eigenvalues`` and ``eigenvectors`` attributes of the exception\n","            object.\n","        \n","        See Also\n","        --------\n","        eigs : eigenvalues and eigenvectors for a general (nonsymmetric) matrix A\n","        svds : singular value decomposition for a matrix A\n","        \n","        Notes\n","        -----\n","        This function is a wrapper to the ARPACK [1]_ SSEUPD and DSEUPD\n","        functions which use the Implicitly Restarted Lanczos Method to\n","        find the eigenvalues and eigenvectors [2]_.\n","        \n","        References\n","        ----------\n","        .. [1] ARPACK Software, http://www.caam.rice.edu/software/ARPACK/\n","        .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE:\n","           Solution of Large Scale Eigenvalue Problems by Implicitly Restarted\n","           Arnoldi Methods. SIAM, Philadelphia, PA, 1998.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse.linalg import eigsh\n","        >>> identity = np.eye(13)\n","        >>> eigenvalues, eigenvectors = eigsh(identity, k=6)\n","        >>> eigenvalues\n","        array([1., 1., 1., 1., 1., 1.])\n","        >>> eigenvectors.shape\n","        (13, 6)\n","    \n","    expm(A)\n","        Compute the matrix exponential using Pade approximation.\n","        \n","        Parameters\n","        ----------\n","        A : (M,M) array_like or sparse matrix\n","            2D Array or Matrix (sparse or dense) to be exponentiated\n","        \n","        Returns\n","        -------\n","        expA : (M,M) ndarray\n","            Matrix exponential of `A`\n","        \n","        Notes\n","        -----\n","        This is algorithm (6.1) which is a simplification of algorithm (5.1).\n","        \n","        .. versionadded:: 0.12.0\n","        \n","        References\n","        ----------\n","        .. [1] Awad H. Al-Mohy and Nicholas J. Higham (2009)\n","               \"A New Scaling and Squaring Algorithm for the Matrix Exponential.\"\n","               SIAM Journal on Matrix Analysis and Applications.\n","               31 (3). pp. 970-989. ISSN 1095-7162\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import expm\n","        >>> A = csc_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n","        >>> A.todense()\n","        matrix([[1, 0, 0],\n","                [0, 2, 0],\n","                [0, 0, 3]], dtype=int64)\n","        >>> Aexp = expm(A)\n","        >>> Aexp\n","        <3x3 sparse matrix of type '<class 'numpy.float64'>'\n","            with 3 stored elements in Compressed Sparse Column format>\n","        >>> Aexp.todense()\n","        matrix([[  2.71828183,   0.        ,   0.        ],\n","                [  0.        ,   7.3890561 ,   0.        ],\n","                [  0.        ,   0.        ,  20.08553692]])\n","    \n","    expm_multiply(A, B, start=None, stop=None, num=None, endpoint=None)\n","        Compute the action of the matrix exponential of A on B.\n","        \n","        Parameters\n","        ----------\n","        A : transposable linear operator\n","            The operator whose exponential is of interest.\n","        B : ndarray\n","            The matrix or vector to be multiplied by the matrix exponential of A.\n","        start : scalar, optional\n","            The starting time point of the sequence.\n","        stop : scalar, optional\n","            The end time point of the sequence, unless `endpoint` is set to False.\n","            In that case, the sequence consists of all but the last of ``num + 1``\n","            evenly spaced time points, so that `stop` is excluded.\n","            Note that the step size changes when `endpoint` is False.\n","        num : int, optional\n","            Number of time points to use.\n","        endpoint : bool, optional\n","            If True, `stop` is the last time point.  Otherwise, it is not included.\n","        \n","        Returns\n","        -------\n","        expm_A_B : ndarray\n","             The result of the action :math:`e^{t_k A} B`.\n","        \n","        Notes\n","        -----\n","        The optional arguments defining the sequence of evenly spaced time points\n","        are compatible with the arguments of `numpy.linspace`.\n","        \n","        The output ndarray shape is somewhat complicated so I explain it here.\n","        The ndim of the output could be either 1, 2, or 3.\n","        It would be 1 if you are computing the expm action on a single vector\n","        at a single time point.\n","        It would be 2 if you are computing the expm action on a vector\n","        at multiple time points, or if you are computing the expm action\n","        on a matrix at a single time point.\n","        It would be 3 if you want the action on a matrix with multiple\n","        columns at multiple time points.\n","        If multiple time points are requested, expm_A_B[0] will always\n","        be the action of the expm at the first time point,\n","        regardless of whether the action is on a vector or a matrix.\n","        \n","        References\n","        ----------\n","        .. [1] Awad H. Al-Mohy and Nicholas J. Higham (2011)\n","               \"Computing the Action of the Matrix Exponential,\n","               with an Application to Exponential Integrators.\"\n","               SIAM Journal on Scientific Computing,\n","               33 (2). pp. 488-511. ISSN 1064-8275\n","               http://eprints.ma.man.ac.uk/1591/\n","        \n","        .. [2] Nicholas J. Higham and Awad H. Al-Mohy (2010)\n","               \"Computing Matrix Functions.\"\n","               Acta Numerica,\n","               19. 159-208. ISSN 0962-4929\n","               http://eprints.ma.man.ac.uk/1451/\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import expm, expm_multiply\n","        >>> A = csc_matrix([[1, 0], [0, 1]])\n","        >>> A.todense()\n","        matrix([[1, 0],\n","                [0, 1]], dtype=int64)\n","        >>> B = np.array([np.exp(-1.), np.exp(-2.)])\n","        >>> B\n","        array([ 0.36787944,  0.13533528])\n","        >>> expm_multiply(A, B, start=1, stop=2, num=3, endpoint=True)\n","        array([[ 1.        ,  0.36787944],\n","               [ 1.64872127,  0.60653066],\n","               [ 2.71828183,  1.        ]])\n","        >>> expm(A).dot(B)                  # Verify 1st timestep\n","        array([ 1.        ,  0.36787944])\n","        >>> expm(1.5*A).dot(B)              # Verify 2nd timestep\n","        array([ 1.64872127,  0.60653066])\n","        >>> expm(2*A).dot(B)                # Verify 3rd timestep\n","        array([ 2.71828183,  1.        ])\n","    \n","    factorized(A)\n","        Return a function for solving a sparse linear system, with A pre-factorized.\n","        \n","        Parameters\n","        ----------\n","        A : (N, N) array_like\n","            Input.\n","        \n","        Returns\n","        -------\n","        solve : callable\n","            To solve the linear system of equations given in `A`, the `solve`\n","            callable should be passed an ndarray of shape (N,).\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse.linalg import factorized\n","        >>> A = np.array([[ 3. ,  2. , -1. ],\n","        ...               [ 2. , -2. ,  4. ],\n","        ...               [-1. ,  0.5, -1. ]])\n","        >>> solve = factorized(A) # Makes LU decomposition.\n","        >>> rhs1 = np.array([1, -2, 0])\n","        >>> solve(rhs1) # Uses the LU factors.\n","        array([ 1., -2., -2.])\n","    \n","    gcrotmk(A, b, x0=None, tol=1e-05, maxiter=1000, M=None, callback=None, m=20, k=None, CU=None, discard_C=False, truncate='oldest', atol=None)\n","        Solve a matrix equation using flexible GCROT(m,k) algorithm.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real or complex N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is `tol`.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : int, optional\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}, optional\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A. gcrotmk is a 'flexible' algorithm and the preconditioner\n","            can vary from iteration to iteration. Effective preconditioning\n","            dramatically improves the rate of convergence, which implies that\n","            fewer iterations are needed to reach a given error tolerance.\n","        callback : function, optional\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","        m : int, optional\n","            Number of inner FGMRES iterations per each outer iteration.\n","            Default: 20\n","        k : int, optional\n","            Number of vectors to carry between inner FGMRES iterations.\n","            According to [2]_, good values are around m.\n","            Default: m\n","        CU : list of tuples, optional\n","            List of tuples ``(c, u)`` which contain the columns of the matrices\n","            C and U in the GCROT(m,k) algorithm. For details, see [2]_.\n","            The list given and vectors contained in it are modified in-place.\n","            If not given, start from empty matrices. The ``c`` elements in the\n","            tuples can be ``None``, in which case the vectors are recomputed\n","            via ``c = A u`` on start and orthogonalized as described in [3]_.\n","        discard_C : bool, optional\n","            Discard the C-vectors at the end. Useful if recycling Krylov subspaces\n","            for different linear systems.\n","        truncate : {'oldest', 'smallest'}, optional\n","            Truncation scheme to use. Drop: oldest vectors, or vectors with\n","            smallest singular values using the scheme discussed in [1,2].\n","            See [2]_ for detailed comparison.\n","            Default: 'oldest'\n","        \n","        Returns\n","        -------\n","        x : array or matrix\n","            The solution found.\n","        info : int\n","            Provides convergence information:\n","        \n","            * 0  : successful exit\n","            * >0 : convergence to tolerance not achieved, number of iterations\n","        \n","        References\n","        ----------\n","        .. [1] E. de Sturler, ''Truncation strategies for optimal Krylov subspace\n","               methods'', SIAM J. Numer. Anal. 36, 864 (1999).\n","        .. [2] J.E. Hicken and D.W. Zingg, ''A simplified and flexible variant\n","               of GCROT for solving nonsymmetric linear systems'',\n","               SIAM J. Sci. Comput. 32, 172 (2010).\n","        .. [3] M.L. Parks, E. de Sturler, G. Mackey, D.D. Johnson, S. Maiti,\n","               ''Recycling Krylov subspaces for sequences of linear systems'',\n","               SIAM J. Sci. Comput. 28, 1651 (2006).\n","    \n","    gmres(A, b, x0=None, tol=1e-05, restart=None, maxiter=None, M=None, callback=None, restrt=None, atol=None, callback_type=None)\n","        Use Generalized Minimal RESidual iteration to solve ``Ax = b``.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real or complex N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : int\n","            Provides convergence information:\n","              * 0  : successful exit\n","              * >0 : convergence to tolerance not achieved, number of iterations\n","              * <0 : illegal input or breakdown\n","        \n","        Other parameters\n","        ----------------\n","        x0 : {array, matrix}\n","            Starting guess for the solution (a vector of zeros by default).\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is ``'legacy'``, which emulates\n","            a different legacy behavior.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        restart : int, optional\n","            Number of iterations between restarts. Larger values increase\n","            iteration cost, but may be necessary for convergence.\n","            Default is 20.\n","        maxiter : int, optional\n","            Maximum number of iterations (restart cycles).  Iteration will stop\n","            after maxiter steps even if the specified tolerance has not been\n","            achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}\n","            Inverse of the preconditioner of A.  M should approximate the\n","            inverse of A and be easy to solve for (see Notes).  Effective\n","            preconditioning dramatically improves the rate of convergence,\n","            which implies that fewer iterations are needed to reach a given\n","            error tolerance.  By default, no preconditioner is used.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as `callback(args)`, where `args` are selected by `callback_type`.\n","        callback_type : {'x', 'pr_norm', 'legacy'}, optional\n","            Callback function argument requested:\n","              - ``x``: current iterate (ndarray), called on every restart\n","              - ``pr_norm``: relative (preconditioned) residual norm (float),\n","                called on every inner iteration\n","              - ``legacy`` (default): same as ``pr_norm``, but also changes the\n","                meaning of 'maxiter' to count inner iterations instead of restart\n","                cycles.\n","        restrt : int, optional\n","            DEPRECATED - use `restart` instead.\n","        \n","        See Also\n","        --------\n","        LinearOperator\n","        \n","        Notes\n","        -----\n","        A preconditioner, P, is chosen such that P is close to A but easy to solve\n","        for. The preconditioner parameter required by this routine is\n","        ``M = P^-1``. The inverse should preferably not be calculated\n","        explicitly.  Rather, use the following template to produce M::\n","        \n","          # Construct a linear operator that computes P^-1 * x.\n","          import scipy.sparse.linalg as spla\n","          M_x = lambda x: spla.spsolve(P, x)\n","          M = spla.LinearOperator((n, n), M_x)\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import gmres\n","        >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\n","        >>> b = np.array([2, 4, -1], dtype=float)\n","        >>> x, exitCode = gmres(A, b)\n","        >>> print(exitCode)            # 0 indicates successful convergence\n","        0\n","        >>> np.allclose(A.dot(x), b)\n","        True\n","    \n","    inv(A)\n","        Compute the inverse of a sparse matrix\n","        \n","        Parameters\n","        ----------\n","        A : (M,M) ndarray or sparse matrix\n","            square matrix to be inverted\n","        \n","        Returns\n","        -------\n","        Ainv : (M,M) ndarray or sparse matrix\n","            inverse of `A`\n","        \n","        Notes\n","        -----\n","        This computes the sparse inverse of `A`. If the inverse of `A` is expected\n","        to be non-sparse, it will likely be faster to convert `A` to dense and use\n","        scipy.linalg.inv.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import inv\n","        >>> A = csc_matrix([[1., 0.], [1., 2.]])\n","        >>> Ainv = inv(A)\n","        >>> Ainv\n","        <2x2 sparse matrix of type '<class 'numpy.float64'>'\n","            with 3 stored elements in Compressed Sparse Column format>\n","        >>> A.dot(Ainv)\n","        <2x2 sparse matrix of type '<class 'numpy.float64'>'\n","            with 2 stored elements in Compressed Sparse Column format>\n","        >>> A.dot(Ainv).todense()\n","        matrix([[ 1.,  0.],\n","                [ 0.,  1.]])\n","        \n","        .. versionadded:: 0.12.0\n","    \n","    lgmres(A, b, x0=None, tol=1e-05, maxiter=1000, M=None, callback=None, inner_m=30, outer_k=3, outer_v=None, store_outer_Av=True, prepend_outer_v=False, atol=None)\n","        Solve a matrix equation using the LGMRES algorithm.\n","        \n","        The LGMRES algorithm [1]_ [2]_ is designed to avoid some problems\n","        in the convergence in restarted GMRES, and often converges in fewer\n","        iterations.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real or complex N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is `tol`.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : int, optional\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}, optional\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A.  Effective preconditioning dramatically improves the\n","            rate of convergence, which implies that fewer iterations are needed\n","            to reach a given error tolerance.\n","        callback : function, optional\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","        inner_m : int, optional\n","            Number of inner GMRES iterations per each outer iteration.\n","        outer_k : int, optional\n","            Number of vectors to carry between inner GMRES iterations.\n","            According to [1]_, good values are in the range of 1...3.\n","            However, note that if you want to use the additional vectors to\n","            accelerate solving multiple similar problems, larger values may\n","            be beneficial.\n","        outer_v : list of tuples, optional\n","            List containing tuples ``(v, Av)`` of vectors and corresponding\n","            matrix-vector products, used to augment the Krylov subspace, and\n","            carried between inner GMRES iterations. The element ``Av`` can\n","            be `None` if the matrix-vector product should be re-evaluated.\n","            This parameter is modified in-place by `lgmres`, and can be used\n","            to pass \"guess\" vectors in and out of the algorithm when solving\n","            similar problems.\n","        store_outer_Av : bool, optional\n","            Whether LGMRES should store also A*v in addition to vectors `v`\n","            in the `outer_v` list. Default is True.\n","        prepend_outer_v : bool, optional \n","            Whether to put outer_v augmentation vectors before Krylov iterates.\n","            In standard LGMRES, prepend_outer_v=False.\n","        \n","        Returns\n","        -------\n","        x : array or matrix\n","            The converged solution.\n","        info : int\n","            Provides convergence information:\n","        \n","                - 0  : successful exit\n","                - >0 : convergence to tolerance not achieved, number of iterations\n","                - <0 : illegal input or breakdown\n","        \n","        Notes\n","        -----\n","        The LGMRES algorithm [1]_ [2]_ is designed to avoid the\n","        slowing of convergence in restarted GMRES, due to alternating\n","        residual vectors. Typically, it often outperforms GMRES(m) of\n","        comparable memory requirements by some measure, or at least is not\n","        much worse.\n","        \n","        Another advantage in this algorithm is that you can supply it with\n","        'guess' vectors in the `outer_v` argument that augment the Krylov\n","        subspace. If the solution lies close to the span of these vectors,\n","        the algorithm converges faster. This can be useful if several very\n","        similar matrices need to be inverted one after another, such as in\n","        Newton-Krylov iteration where the Jacobian matrix often changes\n","        little in the nonlinear steps.\n","        \n","        References\n","        ----------\n","        .. [1] A.H. Baker and E.R. Jessup and T. Manteuffel, \"A Technique for\n","                 Accelerating the Convergence of Restarted GMRES\", SIAM J. Matrix\n","                 Anal. Appl. 26, 962 (2005).\n","        .. [2] A.H. Baker, \"On Improving the Performance of the Linear Solver\n","                 restarted GMRES\", PhD thesis, University of Colorado (2003).\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import lgmres\n","        >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\n","        >>> b = np.array([2, 4, -1], dtype=float)\n","        >>> x, exitCode = lgmres(A, b)\n","        >>> print(exitCode)            # 0 indicates successful convergence\n","        0\n","        >>> np.allclose(A.dot(x), b)\n","        True\n","    \n","    lobpcg(A, X, B=None, M=None, Y=None, tol=None, maxiter=None, largest=True, verbosityLevel=0, retLambdaHistory=False, retResidualNormsHistory=False)\n","        Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG)\n","        \n","        LOBPCG is a preconditioned eigensolver for large symmetric positive\n","        definite (SPD) generalized eigenproblems.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The symmetric linear operator of the problem, usually a\n","            sparse matrix.  Often called the \"stiffness matrix\".\n","        X : ndarray, float32 or float64\n","            Initial approximation to the ``k`` eigenvectors (non-sparse). If `A`\n","            has ``shape=(n,n)`` then `X` should have shape ``shape=(n,k)``.\n","        B : {dense matrix, sparse matrix, LinearOperator}, optional\n","            The right hand side operator in a generalized eigenproblem.\n","            By default, ``B = Identity``.  Often called the \"mass matrix\".\n","        M : {dense matrix, sparse matrix, LinearOperator}, optional\n","            Preconditioner to `A`; by default ``M = Identity``.\n","            `M` should approximate the inverse of `A`.\n","        Y : ndarray, float32 or float64, optional\n","            n-by-sizeY matrix of constraints (non-sparse), sizeY < n\n","            The iterations will be performed in the B-orthogonal complement\n","            of the column-space of Y. Y must be full rank.\n","        tol : scalar, optional\n","            Solver tolerance (stopping criterion).\n","            The default is ``tol=n*sqrt(eps)``.\n","        maxiter : int, optional\n","            Maximum number of iterations.  The default is ``maxiter = 20``.\n","        largest : bool, optional\n","            When True, solve for the largest eigenvalues, otherwise the smallest.\n","        verbosityLevel : int, optional\n","            Controls solver output.  The default is ``verbosityLevel=0``.\n","        retLambdaHistory : bool, optional\n","            Whether to return eigenvalue history.  Default is False.\n","        retResidualNormsHistory : bool, optional\n","            Whether to return history of residual norms.  Default is False.\n","        \n","        Returns\n","        -------\n","        w : ndarray\n","            Array of ``k`` eigenvalues\n","        v : ndarray\n","            An array of ``k`` eigenvectors.  `v` has the same shape as `X`.\n","        lambdas : list of ndarray, optional\n","            The eigenvalue history, if `retLambdaHistory` is True.\n","        rnorms : list of ndarray, optional\n","            The history of residual norms, if `retResidualNormsHistory` is True.\n","        \n","        Notes\n","        -----\n","        If both ``retLambdaHistory`` and ``retResidualNormsHistory`` are True,\n","        the return tuple has the following format\n","        ``(lambda, V, lambda history, residual norms history)``.\n","        \n","        In the following ``n`` denotes the matrix size and ``m`` the number\n","        of required eigenvalues (smallest or largest).\n","        \n","        The LOBPCG code internally solves eigenproblems of the size ``3m`` on every\n","        iteration by calling the \"standard\" dense eigensolver, so if ``m`` is not\n","        small enough compared to ``n``, it does not make sense to call the LOBPCG\n","        code, but rather one should use the \"standard\" eigensolver, e.g. numpy or\n","        scipy function in this case.\n","        If one calls the LOBPCG algorithm for ``5m > n``, it will most likely break\n","        internally, so the code tries to call the standard function instead.\n","        \n","        It is not that ``n`` should be large for the LOBPCG to work, but rather the\n","        ratio ``n / m`` should be large. It you call LOBPCG with ``m=1``\n","        and ``n=10``, it works though ``n`` is small. The method is intended\n","        for extremely large ``n / m`` [4]_.\n","        \n","        The convergence speed depends basically on two factors:\n","        \n","        1. How well relatively separated the seeking eigenvalues are from the rest\n","           of the eigenvalues. One can try to vary ``m`` to make this better.\n","        \n","        2. How well conditioned the problem is. This can be changed by using proper\n","           preconditioning. For example, a rod vibration test problem (under tests\n","           directory) is ill-conditioned for large ``n``, so convergence will be\n","           slow, unless efficient preconditioning is used. For this specific\n","           problem, a good simple preconditioner function would be a linear solve\n","           for `A`, which is easy to code since A is tridiagonal.\n","        \n","        References\n","        ----------\n","        .. [1] A. V. Knyazev (2001),\n","               Toward the Optimal Preconditioned Eigensolver: Locally Optimal\n","               Block Preconditioned Conjugate Gradient Method.\n","               SIAM Journal on Scientific Computing 23, no. 2,\n","               pp. 517-541. :doi:`10.1137/S1064827500366124`\n","        \n","        .. [2] A. V. Knyazev, I. Lashuk, M. E. Argentati, and E. Ovchinnikov\n","               (2007), Block Locally Optimal Preconditioned Eigenvalue Xolvers\n","               (BLOPEX) in hypre and PETSc. :arxiv:`0705.2626`\n","        \n","        .. [3] A. V. Knyazev's C and MATLAB implementations:\n","               https://bitbucket.org/joseroman/blopex\n","        \n","        .. [4] S. Yamada, T. Imamura, T. Kano, and M. Machida (2006),\n","               High-performance computing for exact numerical approaches to\n","               quantum many-body problems on the earth simulator. In Proceedings\n","               of the 2006 ACM/IEEE Conference on Supercomputing.\n","               :doi:`10.1145/1188455.1188504`\n","        \n","        Examples\n","        --------\n","        \n","        Solve ``A x = lambda x`` with constraints and preconditioning.\n","        \n","        >>> import numpy as np\n","        >>> from scipy.sparse import spdiags, issparse\n","        >>> from scipy.sparse.linalg import lobpcg, LinearOperator\n","        >>> n = 100\n","        >>> vals = np.arange(1, n + 1)\n","        >>> A = spdiags(vals, 0, n, n)\n","        >>> A.toarray()\n","        array([[  1.,   0.,   0., ...,   0.,   0.,   0.],\n","               [  0.,   2.,   0., ...,   0.,   0.,   0.],\n","               [  0.,   0.,   3., ...,   0.,   0.,   0.],\n","               ...,\n","               [  0.,   0.,   0., ...,  98.,   0.,   0.],\n","               [  0.,   0.,   0., ...,   0.,  99.,   0.],\n","               [  0.,   0.,   0., ...,   0.,   0., 100.]])\n","        \n","        Constraints:\n","        \n","        >>> Y = np.eye(n, 3)\n","        \n","        Initial guess for eigenvectors, should have linearly independent\n","        columns. Column dimension = number of requested eigenvalues.\n","        \n","        >>> rng = np.random.default_rng()\n","        >>> X = rng.random((n, 3))\n","        \n","        Preconditioner in the inverse of A in this example:\n","        \n","        >>> invA = spdiags([1./vals], 0, n, n)\n","        \n","        The preconditiner must be defined by a function:\n","        \n","        >>> def precond( x ):\n","        ...     return invA @ x\n","        \n","        The argument x of the preconditioner function is a matrix inside `lobpcg`,\n","        thus the use of matrix-matrix product ``@``.\n","        \n","        The preconditioner function is passed to lobpcg as a `LinearOperator`:\n","        \n","        >>> M = LinearOperator(matvec=precond, matmat=precond,\n","        ...                    shape=(n, n), dtype=float)\n","        \n","        Let us now solve the eigenvalue problem for the matrix A:\n","        \n","        >>> eigenvalues, _ = lobpcg(A, X, Y=Y, M=M, largest=False)\n","        >>> eigenvalues\n","        array([4., 5., 6.])\n","        \n","        Note that the vectors passed in Y are the eigenvectors of the 3 smallest\n","        eigenvalues. The results returned are orthogonal to those.\n","    \n","    lsmr(A, b, damp=0.0, atol=1e-06, btol=1e-06, conlim=100000000.0, maxiter=None, show=False, x0=None)\n","        Iterative solver for least-squares problems.\n","        \n","        lsmr solves the system of linear equations ``Ax = b``. If the system\n","        is inconsistent, it solves the least-squares problem ``min ||b - Ax||_2``.\n","        ``A`` is a rectangular matrix of dimension m-by-n, where all cases are\n","        allowed: m = n, m > n, or m < n. ``b`` is a vector of length m.\n","        The matrix A may be dense or sparse (usually sparse).\n","        \n","        Parameters\n","        ----------\n","        A : {matrix, sparse matrix, ndarray, LinearOperator}\n","            Matrix A in the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` and ``A^H x`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : array_like, shape (m,)\n","            Vector ``b`` in the linear system.\n","        damp : float\n","            Damping factor for regularized least-squares. `lsmr` solves\n","            the regularized least-squares problem::\n","        \n","             min ||(b) - (  A   )x||\n","                 ||(0)   (damp*I) ||_2\n","        \n","            where damp is a scalar.  If damp is None or 0, the system\n","            is solved without regularization.\n","        atol, btol : float, optional\n","            Stopping tolerances. `lsmr` continues iterations until a\n","            certain backward error estimate is smaller than some quantity\n","            depending on atol and btol.  Let ``r = b - Ax`` be the\n","            residual vector for the current approximate solution ``x``.\n","            If ``Ax = b`` seems to be consistent, ``lsmr`` terminates\n","            when ``norm(r) <= atol * norm(A) * norm(x) + btol * norm(b)``.\n","            Otherwise, lsmr terminates when ``norm(A^H r) <=\n","            atol * norm(A) * norm(r)``.  If both tolerances are 1.0e-6 (say),\n","            the final ``norm(r)`` should be accurate to about 6\n","            digits. (The final ``x`` will usually have fewer correct digits,\n","            depending on ``cond(A)`` and the size of LAMBDA.)  If `atol`\n","            or `btol` is None, a default value of 1.0e-6 will be used.\n","            Ideally, they should be estimates of the relative error in the\n","            entries of ``A`` and ``b`` respectively.  For example, if the entries\n","            of ``A`` have 7 correct digits, set ``atol = 1e-7``. This prevents\n","            the algorithm from doing unnecessary work beyond the\n","            uncertainty of the input data.\n","        conlim : float, optional\n","            `lsmr` terminates if an estimate of ``cond(A)`` exceeds\n","            `conlim`.  For compatible systems ``Ax = b``, conlim could be\n","            as large as 1.0e+12 (say).  For least-squares problems,\n","            `conlim` should be less than 1.0e+8. If `conlim` is None, the\n","            default value is 1e+8.  Maximum precision can be obtained by\n","            setting ``atol = btol = conlim = 0``, but the number of\n","            iterations may then be excessive.\n","        maxiter : int, optional\n","            `lsmr` terminates if the number of iterations reaches\n","            `maxiter`.  The default is ``maxiter = min(m, n)``.  For\n","            ill-conditioned systems, a larger value of `maxiter` may be\n","            needed.\n","        show : bool, optional\n","            Print iterations logs if ``show=True``.\n","        x0 : array_like, shape (n,), optional\n","            Initial guess of ``x``, if None zeros are used.\n","        \n","            .. versionadded:: 1.0.0\n","            \n","        Returns\n","        -------\n","        x : ndarray of float\n","            Least-square solution returned.\n","        istop : int\n","            istop gives the reason for stopping::\n","        \n","              istop   = 0 means x=0 is a solution.  If x0 was given, then x=x0 is a\n","                          solution.\n","                      = 1 means x is an approximate solution to A*x = B,\n","                          according to atol and btol.\n","                      = 2 means x approximately solves the least-squares problem\n","                          according to atol.\n","                      = 3 means COND(A) seems to be greater than CONLIM.\n","                      = 4 is the same as 1 with atol = btol = eps (machine\n","                          precision)\n","                      = 5 is the same as 2 with atol = eps.\n","                      = 6 is the same as 3 with CONLIM = 1/eps.\n","                      = 7 means ITN reached maxiter before the other stopping\n","                          conditions were satisfied.\n","        \n","        itn : int\n","            Number of iterations used.\n","        normr : float\n","            ``norm(b-Ax)``\n","        normar : float\n","            ``norm(A^H (b - Ax))``\n","        norma : float\n","            ``norm(A)``\n","        conda : float\n","            Condition number of A.\n","        normx : float\n","            ``norm(x)``\n","        \n","        Notes\n","        -----\n","        \n","        .. versionadded:: 0.11.0\n","        \n","        References\n","        ----------\n","        .. [1] D. C.-L. Fong and M. A. Saunders,\n","               \"LSMR: An iterative algorithm for sparse least-squares problems\",\n","               SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.\n","               :arxiv:`1006.0758`\n","        .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import lsmr\n","        >>> A = csc_matrix([[1., 0.], [1., 1.], [0., 1.]], dtype=float)\n","        \n","        The first example has the trivial solution `[0, 0]`\n","        \n","        >>> b = np.array([0., 0., 0.], dtype=float)\n","        >>> x, istop, itn, normr = lsmr(A, b)[:4]\n","        >>> istop\n","        0\n","        >>> x\n","        array([ 0.,  0.])\n","        \n","        The stopping code `istop=0` returned indicates that a vector of zeros was\n","        found as a solution. The returned solution `x` indeed contains `[0., 0.]`.\n","        The next example has a non-trivial solution:\n","        \n","        >>> b = np.array([1., 0., -1.], dtype=float)\n","        >>> x, istop, itn, normr = lsmr(A, b)[:4]\n","        >>> istop\n","        1\n","        >>> x\n","        array([ 1., -1.])\n","        >>> itn\n","        1\n","        >>> normr\n","        4.440892098500627e-16\n","        \n","        As indicated by `istop=1`, `lsmr` found a solution obeying the tolerance\n","        limits. The given solution `[1., -1.]` obviously solves the equation. The\n","        remaining return values include information about the number of iterations\n","        (`itn=1`) and the remaining difference of left and right side of the solved\n","        equation.\n","        The final example demonstrates the behavior in the case where there is no\n","        solution for the equation:\n","        \n","        >>> b = np.array([1., 0.01, -1.], dtype=float)\n","        >>> x, istop, itn, normr = lsmr(A, b)[:4]\n","        >>> istop\n","        2\n","        >>> x\n","        array([ 1.00333333, -0.99666667])\n","        >>> A.dot(x)-b\n","        array([ 0.00333333, -0.00333333,  0.00333333])\n","        >>> normr\n","        0.005773502691896255\n","        \n","        `istop` indicates that the system is inconsistent and thus `x` is rather an\n","        approximate solution to the corresponding least-squares problem. `normr`\n","        contains the minimal distance that was found.\n","    \n","    lsqr(A, b, damp=0.0, atol=1e-08, btol=1e-08, conlim=100000000.0, iter_lim=None, show=False, calc_var=False, x0=None)\n","        Find the least-squares solution to a large, sparse, linear system\n","        of equations.\n","        \n","        The function solves ``Ax = b``  or  ``min ||Ax - b||^2`` or\n","        ``min ||Ax - b||^2 + d^2 ||x||^2``.\n","        \n","        The matrix A may be square or rectangular (over-determined or\n","        under-determined), and may have any rank.\n","        \n","        ::\n","        \n","          1. Unsymmetric equations --    solve  A*x = b\n","        \n","          2. Linear least squares  --    solve  A*x = b\n","                                         in the least-squares sense\n","        \n","          3. Damped least squares  --    solve  (   A    )*x = ( b )\n","                                                ( damp*I )     ( 0 )\n","                                         in the least-squares sense\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, ndarray, LinearOperator}\n","            Representation of an m-by-n matrix.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` and ``A^T x`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : array_like, shape (m,)\n","            Right-hand side vector ``b``.\n","        damp : float\n","            Damping coefficient.\n","        atol, btol : float, optional\n","            Stopping tolerances. If both are 1.0e-9 (say), the final\n","            residual norm should be accurate to about 9 digits.  (The\n","            final x will usually have fewer correct digits, depending on\n","            cond(A) and the size of damp.)\n","        conlim : float, optional\n","            Another stopping tolerance.  lsqr terminates if an estimate of\n","            ``cond(A)`` exceeds `conlim`.  For compatible systems ``Ax =\n","            b``, `conlim` could be as large as 1.0e+12 (say).  For\n","            least-squares problems, conlim should be less than 1.0e+8.\n","            Maximum precision can be obtained by setting ``atol = btol =\n","            conlim = zero``, but the number of iterations may then be\n","            excessive.\n","        iter_lim : int, optional\n","            Explicit limitation on number of iterations (for safety).\n","        show : bool, optional\n","            Display an iteration log.\n","        calc_var : bool, optional\n","            Whether to estimate diagonals of ``(A'A + damp^2*I)^{-1}``.\n","        x0 : array_like, shape (n,), optional\n","            Initial guess of x, if None zeros are used.\n","        \n","            .. versionadded:: 1.0.0\n","        \n","        Returns\n","        -------\n","        x : ndarray of float\n","            The final solution.\n","        istop : int\n","            Gives the reason for termination.\n","            1 means x is an approximate solution to Ax = b.\n","            2 means x approximately solves the least-squares problem.\n","        itn : int\n","            Iteration number upon termination.\n","        r1norm : float\n","            ``norm(r)``, where ``r = b - Ax``.\n","        r2norm : float\n","            ``sqrt( norm(r)^2  +  damp^2 * norm(x)^2 )``.  Equal to `r1norm` if\n","            ``damp == 0``.\n","        anorm : float\n","            Estimate of Frobenius norm of ``Abar = [[A]; [damp*I]]``.\n","        acond : float\n","            Estimate of ``cond(Abar)``.\n","        arnorm : float\n","            Estimate of ``norm(A'*r - damp^2*x)``.\n","        xnorm : float\n","            ``norm(x)``\n","        var : ndarray of float\n","            If ``calc_var`` is True, estimates all diagonals of\n","            ``(A'A)^{-1}`` (if ``damp == 0``) or more generally ``(A'A +\n","            damp^2*I)^{-1}``.  This is well defined if A has full column\n","            rank or ``damp > 0``.  (Not sure what var means if ``rank(A)\n","            < n`` and ``damp = 0.``)\n","        \n","        Notes\n","        -----\n","        LSQR uses an iterative method to approximate the solution.  The\n","        number of iterations required to reach a certain accuracy depends\n","        strongly on the scaling of the problem.  Poor scaling of the rows\n","        or columns of A should therefore be avoided where possible.\n","        \n","        For example, in problem 1 the solution is unaltered by\n","        row-scaling.  If a row of A is very small or large compared to\n","        the other rows of A, the corresponding row of ( A  b ) should be\n","        scaled up or down.\n","        \n","        In problems 1 and 2, the solution x is easily recovered\n","        following column-scaling.  Unless better information is known,\n","        the nonzero columns of A should be scaled so that they all have\n","        the same Euclidean norm (e.g., 1.0).\n","        \n","        In problem 3, there is no freedom to re-scale if damp is\n","        nonzero.  However, the value of damp should be assigned only\n","        after attention has been paid to the scaling of A.\n","        \n","        The parameter damp is intended to help regularize\n","        ill-conditioned systems, by preventing the true solution from\n","        being very large.  Another aid to regularization is provided by\n","        the parameter acond, which may be used to terminate iterations\n","        before the computed solution becomes very large.\n","        \n","        If some initial estimate ``x0`` is known and if ``damp == 0``,\n","        one could proceed as follows:\n","        \n","          1. Compute a residual vector ``r0 = b - A*x0``.\n","          2. Use LSQR to solve the system  ``A*dx = r0``.\n","          3. Add the correction dx to obtain a final solution ``x = x0 + dx``.\n","        \n","        This requires that ``x0`` be available before and after the call\n","        to LSQR.  To judge the benefits, suppose LSQR takes k1 iterations\n","        to solve A*x = b and k2 iterations to solve A*dx = r0.\n","        If x0 is \"good\", norm(r0) will be smaller than norm(b).\n","        If the same stopping tolerances atol and btol are used for each\n","        system, k1 and k2 will be similar, but the final solution x0 + dx\n","        should be more accurate.  The only way to reduce the total work\n","        is to use a larger stopping tolerance for the second system.\n","        If some value btol is suitable for A*x = b, the larger value\n","        btol*norm(b)/norm(r0)  should be suitable for A*dx = r0.\n","        \n","        Preconditioning is another way to reduce the number of iterations.\n","        If it is possible to solve a related system ``M*x = b``\n","        efficiently, where M approximates A in some helpful way (e.g. M -\n","        A has low rank or its elements are small relative to those of A),\n","        LSQR may converge more rapidly on the system ``A*M(inverse)*z =\n","        b``, after which x can be recovered by solving M*x = z.\n","        \n","        If A is symmetric, LSQR should not be used!\n","        \n","        Alternatives are the symmetric conjugate-gradient method (cg)\n","        and/or SYMMLQ.  SYMMLQ is an implementation of symmetric cg that\n","        applies to any symmetric A and will converge more rapidly than\n","        LSQR.  If A is positive definite, there are other implementations\n","        of symmetric cg that require slightly less work per iteration than\n","        SYMMLQ (but will take the same number of iterations).\n","        \n","        References\n","        ----------\n","        .. [1] C. C. Paige and M. A. Saunders (1982a).\n","               \"LSQR: An algorithm for sparse linear equations and\n","               sparse least squares\", ACM TOMS 8(1), 43-71.\n","        .. [2] C. C. Paige and M. A. Saunders (1982b).\n","               \"Algorithm 583.  LSQR: Sparse linear equations and least\n","               squares problems\", ACM TOMS 8(2), 195-209.\n","        .. [3] M. A. Saunders (1995).  \"Solution of sparse rectangular\n","               systems using LSQR and CRAIG\", BIT 35, 588-604.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import lsqr\n","        >>> A = csc_matrix([[1., 0.], [1., 1.], [0., 1.]], dtype=float)\n","        \n","        The first example has the trivial solution `[0, 0]`\n","        \n","        >>> b = np.array([0., 0., 0.], dtype=float)\n","        >>> x, istop, itn, normr = lsqr(A, b)[:4]\n","        >>> istop\n","        0\n","        >>> x\n","        array([ 0.,  0.])\n","        \n","        The stopping code `istop=0` returned indicates that a vector of zeros was\n","        found as a solution. The returned solution `x` indeed contains `[0., 0.]`.\n","        The next example has a non-trivial solution:\n","        \n","        >>> b = np.array([1., 0., -1.], dtype=float)\n","        >>> x, istop, itn, r1norm = lsqr(A, b)[:4]\n","        >>> istop\n","        1\n","        >>> x\n","        array([ 1., -1.])\n","        >>> itn\n","        1\n","        >>> r1norm\n","        4.440892098500627e-16\n","        \n","        As indicated by `istop=1`, `lsqr` found a solution obeying the tolerance\n","        limits. The given solution `[1., -1.]` obviously solves the equation. The\n","        remaining return values include information about the number of iterations\n","        (`itn=1`) and the remaining difference of left and right side of the solved\n","        equation.\n","        The final example demonstrates the behavior in the case where there is no\n","        solution for the equation:\n","        \n","        >>> b = np.array([1., 0.01, -1.], dtype=float)\n","        >>> x, istop, itn, r1norm = lsqr(A, b)[:4]\n","        >>> istop\n","        2\n","        >>> x\n","        array([ 1.00333333, -0.99666667])\n","        >>> A.dot(x)-b\n","        array([ 0.00333333, -0.00333333,  0.00333333])\n","        >>> r1norm\n","        0.005773502691896255\n","        \n","        `istop` indicates that the system is inconsistent and thus `x` is rather an\n","        approximate solution to the corresponding least-squares problem. `r1norm`\n","        contains the norm of the minimal residual that was found.\n","    \n","    minres(A, b, x0=None, shift=0.0, tol=1e-05, maxiter=None, M=None, callback=None, show=False, check=False)\n","        Use MINimum RESidual iteration to solve Ax=b\n","        \n","        MINRES minimizes norm(A*x - b) for a real symmetric matrix A.  Unlike\n","        the Conjugate Gradient method, A can be indefinite or singular.\n","        \n","        If shift != 0 then the method solves (A - shift*I)x = b\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real symmetric N-by-N matrix of the linear system\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : integer\n","            Provides convergence information:\n","                0  : successful exit\n","                >0 : convergence to tolerance not achieved, number of iterations\n","                <0 : illegal input or breakdown\n","        \n","        Other Parameters\n","        ----------------\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol : float\n","            Tolerance to achieve. The algorithm terminates when the relative\n","            residual is below `tol`.\n","        maxiter : integer\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M : {sparse matrix, dense matrix, LinearOperator}\n","            Preconditioner for A.  The preconditioner should approximate the\n","            inverse of A.  Effective preconditioning dramatically improves the\n","            rate of convergence, which implies that fewer iterations are needed\n","            to reach a given error tolerance.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","        \n","        Examples\n","        --------\n","        >>> import numpy as np\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import minres\n","        >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\n","        >>> A = A + A.T\n","        >>> b = np.array([2, 4, -1], dtype=float)\n","        >>> x, exitCode = minres(A, b)\n","        >>> print(exitCode)            # 0 indicates successful convergence\n","        0\n","        >>> np.allclose(A.dot(x), b)\n","        True\n","        \n","        References\n","        ----------\n","        Solution of sparse indefinite systems of linear equations,\n","            C. C. Paige and M. A. Saunders (1975),\n","            SIAM J. Numer. Anal. 12(4), pp. 617-629.\n","            https://web.stanford.edu/group/SOL/software/minres/\n","        \n","        This file is a translation of the following MATLAB implementation:\n","            https://web.stanford.edu/group/SOL/software/minres/minres-matlab.zip\n","    \n","    norm(x, ord=None, axis=None)\n","        Norm of a sparse matrix\n","        \n","        This function is able to return one of seven different matrix norms,\n","        depending on the value of the ``ord`` parameter.\n","        \n","        Parameters\n","        ----------\n","        x : a sparse matrix\n","            Input sparse matrix.\n","        ord : {non-zero int, inf, -inf, 'fro'}, optional\n","            Order of the norm (see table under ``Notes``). inf means numpy's\n","            `inf` object.\n","        axis : {int, 2-tuple of ints, None}, optional\n","            If `axis` is an integer, it specifies the axis of `x` along which to\n","            compute the vector norms.  If `axis` is a 2-tuple, it specifies the\n","            axes that hold 2-D matrices, and the matrix norms of these matrices\n","            are computed.  If `axis` is None then either a vector norm (when `x`\n","            is 1-D) or a matrix norm (when `x` is 2-D) is returned.\n","        \n","        Returns\n","        -------\n","        n : float or ndarray\n","        \n","        Notes\n","        -----\n","        Some of the ord are not implemented because some associated functions like,\n","        _multi_svd_norm, are not yet available for sparse matrix.\n","        \n","        This docstring is modified based on numpy.linalg.norm.\n","        https://github.com/numpy/numpy/blob/master/numpy/linalg/linalg.py\n","        \n","        The following norms can be calculated:\n","        \n","        =====  ============================\n","        ord    norm for sparse matrices\n","        =====  ============================\n","        None   Frobenius norm\n","        'fro'  Frobenius norm\n","        inf    max(sum(abs(x), axis=1))\n","        -inf   min(sum(abs(x), axis=1))\n","        0      abs(x).sum(axis=axis)\n","        1      max(sum(abs(x), axis=0))\n","        -1     min(sum(abs(x), axis=0))\n","        2      Not implemented\n","        -2     Not implemented\n","        other  Not implemented\n","        =====  ============================\n","        \n","        The Frobenius norm is given by [1]_:\n","        \n","            :math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`\n","        \n","        References\n","        ----------\n","        .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,\n","            Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import *\n","        >>> import numpy as np\n","        >>> from scipy.sparse.linalg import norm\n","        >>> a = np.arange(9) - 4\n","        >>> a\n","        array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n","        >>> b = a.reshape((3, 3))\n","        >>> b\n","        array([[-4, -3, -2],\n","               [-1, 0, 1],\n","               [ 2, 3, 4]])\n","        \n","        >>> b = csr_matrix(b)\n","        >>> norm(b)\n","        7.745966692414834\n","        >>> norm(b, 'fro')\n","        7.745966692414834\n","        >>> norm(b, np.inf)\n","        9\n","        >>> norm(b, -np.inf)\n","        2\n","        >>> norm(b, 1)\n","        7\n","        >>> norm(b, -1)\n","        6\n","    \n","    onenormest(A, t=2, itmax=5, compute_v=False, compute_w=False)\n","        Compute a lower bound of the 1-norm of a sparse matrix.\n","        \n","        Parameters\n","        ----------\n","        A : ndarray or other linear operator\n","            A linear operator that can be transposed and that can\n","            produce matrix products.\n","        t : int, optional\n","            A positive parameter controlling the tradeoff between\n","            accuracy versus time and memory usage.\n","            Larger values take longer and use more memory\n","            but give more accurate output.\n","        itmax : int, optional\n","            Use at most this many iterations.\n","        compute_v : bool, optional\n","            Request a norm-maximizing linear operator input vector if True.\n","        compute_w : bool, optional\n","            Request a norm-maximizing linear operator output vector if True.\n","        \n","        Returns\n","        -------\n","        est : float\n","            An underestimate of the 1-norm of the sparse matrix.\n","        v : ndarray, optional\n","            The vector such that ||Av||_1 == est*||v||_1.\n","            It can be thought of as an input to the linear operator\n","            that gives an output with particularly large norm.\n","        w : ndarray, optional\n","            The vector Av which has relatively large 1-norm.\n","            It can be thought of as an output of the linear operator\n","            that is relatively large in norm compared to the input.\n","        \n","        Notes\n","        -----\n","        This is algorithm 2.4 of [1].\n","        \n","        In [2] it is described as follows.\n","        \"This algorithm typically requires the evaluation of\n","        about 4t matrix-vector products and almost invariably\n","        produces a norm estimate (which is, in fact, a lower\n","        bound on the norm) correct to within a factor 3.\"\n","        \n","        .. versionadded:: 0.13.0\n","        \n","        References\n","        ----------\n","        .. [1] Nicholas J. Higham and Francoise Tisseur (2000),\n","               \"A Block Algorithm for Matrix 1-Norm Estimation,\n","               with an Application to 1-Norm Pseudospectra.\"\n","               SIAM J. Matrix Anal. Appl. Vol. 21, No. 4, pp. 1185-1201.\n","        \n","        .. [2] Awad H. Al-Mohy and Nicholas J. Higham (2009),\n","               \"A new scaling and squaring algorithm for the matrix exponential.\"\n","               SIAM J. Matrix Anal. Appl. Vol. 31, No. 3, pp. 970-989.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import onenormest\n","        >>> A = csc_matrix([[1., 0., 0.], [5., 8., 2.], [0., -1., 0.]], dtype=float)\n","        >>> A.todense()\n","        matrix([[ 1.,  0.,  0.],\n","                [ 5.,  8.,  2.],\n","                [ 0., -1.,  0.]])\n","        >>> onenormest(A)\n","        9.0\n","        >>> np.linalg.norm(A.todense(), ord=1)\n","        9.0\n","    \n","    qmr(A, b, x0=None, tol=1e-05, maxiter=None, M1=None, M2=None, callback=None, atol=None)\n","        Use Quasi-Minimal Residual iteration to solve ``Ax = b``.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, dense matrix, LinearOperator}\n","            The real-valued N-by-N matrix of the linear system.\n","            Alternatively, ``A`` can be a linear operator which can\n","            produce ``Ax`` and ``A^T x`` using, e.g.,\n","            ``scipy.sparse.linalg.LinearOperator``.\n","        b : {array, matrix}\n","            Right hand side of the linear system. Has shape (N,) or (N,1).\n","        \n","        Returns\n","        -------\n","        x : {array, matrix}\n","            The converged solution.\n","        info : integer\n","            Provides convergence information:\n","                0  : successful exit\n","                >0 : convergence to tolerance not achieved, number of iterations\n","                <0 : illegal input or breakdown\n","        \n","        Other Parameters\n","        ----------------\n","        x0  : {array, matrix}\n","            Starting guess for the solution.\n","        tol, atol : float, optional\n","            Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","            The default for ``atol`` is ``'legacy'``, which emulates\n","            a different legacy behavior.\n","        \n","            .. warning::\n","        \n","               The default value for `atol` will be changed in a future release.\n","               For future compatibility, specify `atol` explicitly.\n","        maxiter : integer\n","            Maximum number of iterations.  Iteration will stop after maxiter\n","            steps even if the specified tolerance has not been achieved.\n","        M1 : {sparse matrix, dense matrix, LinearOperator}\n","            Left preconditioner for A.\n","        M2 : {sparse matrix, dense matrix, LinearOperator}\n","            Right preconditioner for A. Used together with the left\n","            preconditioner M1.  The matrix M1*A*M2 should have better\n","            conditioned than A alone.\n","        callback : function\n","            User-supplied function to call after each iteration.  It is called\n","            as callback(xk), where xk is the current solution vector.\n","        \n","        See Also\n","        --------\n","        LinearOperator\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import qmr\n","        >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\n","        >>> b = np.array([2, 4, -1], dtype=float)\n","        >>> x, exitCode = qmr(A, b)\n","        >>> print(exitCode)            # 0 indicates successful convergence\n","        0\n","        >>> np.allclose(A.dot(x), b)\n","        True\n","    \n","    spilu(A, drop_tol=None, fill_factor=None, drop_rule=None, permc_spec=None, diag_pivot_thresh=None, relax=None, panel_size=None, options=None)\n","        Compute an incomplete LU decomposition for a sparse, square matrix.\n","        \n","        The resulting object is an approximation to the inverse of `A`.\n","        \n","        Parameters\n","        ----------\n","        A : (N, N) array_like\n","            Sparse matrix to factorize\n","        drop_tol : float, optional\n","            Drop tolerance (0 <= tol <= 1) for an incomplete LU decomposition.\n","            (default: 1e-4)\n","        fill_factor : float, optional\n","            Specifies the fill ratio upper bound (>= 1.0) for ILU. (default: 10)\n","        drop_rule : str, optional\n","            Comma-separated string of drop rules to use.\n","            Available rules: ``basic``, ``prows``, ``column``, ``area``,\n","            ``secondary``, ``dynamic``, ``interp``. (Default: ``basic,area``)\n","        \n","            See SuperLU documentation for details.\n","        \n","        Remaining other options\n","            Same as for `splu`\n","        \n","        Returns\n","        -------\n","        invA_approx : scipy.sparse.linalg.SuperLU\n","            Object, which has a ``solve`` method.\n","        \n","        See also\n","        --------\n","        splu : complete LU decomposition\n","        \n","        Notes\n","        -----\n","        To improve the better approximation to the inverse, you may need to\n","        increase `fill_factor` AND decrease `drop_tol`.\n","        \n","        This function uses the SuperLU library.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import spilu\n","        >>> A = csc_matrix([[1., 0., 0.], [5., 0., 2.], [0., -1., 0.]], dtype=float)\n","        >>> B = spilu(A)\n","        >>> x = np.array([1., 2., 3.], dtype=float)\n","        >>> B.solve(x)\n","        array([ 1. , -3. , -1.5])\n","        >>> A.dot(B.solve(x))\n","        array([ 1.,  2.,  3.])\n","        >>> B.solve(A.dot(x))\n","        array([ 1.,  2.,  3.])\n","    \n","    splu(A, permc_spec=None, diag_pivot_thresh=None, relax=None, panel_size=None, options={})\n","        Compute the LU decomposition of a sparse, square matrix.\n","        \n","        Parameters\n","        ----------\n","        A : sparse matrix\n","            Sparse matrix to factorize. Should be in CSR or CSC format.\n","        permc_spec : str, optional\n","            How to permute the columns of the matrix for sparsity preservation.\n","            (default: 'COLAMD')\n","        \n","            - ``NATURAL``: natural ordering.\n","            - ``MMD_ATA``: minimum degree ordering on the structure of A^T A.\n","            - ``MMD_AT_PLUS_A``: minimum degree ordering on the structure of A^T+A.\n","            - ``COLAMD``: approximate minimum degree column ordering\n","        \n","        diag_pivot_thresh : float, optional\n","            Threshold used for a diagonal entry to be an acceptable pivot.\n","            See SuperLU user's guide for details [1]_\n","        relax : int, optional\n","            Expert option for customizing the degree of relaxing supernodes.\n","            See SuperLU user's guide for details [1]_\n","        panel_size : int, optional\n","            Expert option for customizing the panel size.\n","            See SuperLU user's guide for details [1]_\n","        options : dict, optional\n","            Dictionary containing additional expert options to SuperLU.\n","            See SuperLU user guide [1]_ (section 2.4 on the 'Options' argument)\n","            for more details. For example, you can specify\n","            ``options=dict(Equil=False, IterRefine='SINGLE'))``\n","            to turn equilibration off and perform a single iterative refinement.\n","        \n","        Returns\n","        -------\n","        invA : scipy.sparse.linalg.SuperLU\n","            Object, which has a ``solve`` method.\n","        \n","        See also\n","        --------\n","        spilu : incomplete LU decomposition\n","        \n","        Notes\n","        -----\n","        This function uses the SuperLU library.\n","        \n","        References\n","        ----------\n","        .. [1] SuperLU http://crd.lbl.gov/~xiaoye/SuperLU/\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import splu\n","        >>> A = csc_matrix([[1., 0., 0.], [5., 0., 2.], [0., -1., 0.]], dtype=float)\n","        >>> B = splu(A)\n","        >>> x = np.array([1., 2., 3.], dtype=float)\n","        >>> B.solve(x)\n","        array([ 1. , -3. , -1.5])\n","        >>> A.dot(B.solve(x))\n","        array([ 1.,  2.,  3.])\n","        >>> B.solve(A.dot(x))\n","        array([ 1.,  2.,  3.])\n","    \n","    spsolve(A, b, permc_spec=None, use_umfpack=True)\n","        Solve the sparse linear system Ax=b, where b may be a vector or a matrix.\n","        \n","        Parameters\n","        ----------\n","        A : ndarray or sparse matrix\n","            The square matrix A will be converted into CSC or CSR form\n","        b : ndarray or sparse matrix\n","            The matrix or vector representing the right hand side of the equation.\n","            If a vector, b.shape must be (n,) or (n, 1).\n","        permc_spec : str, optional\n","            How to permute the columns of the matrix for sparsity preservation.\n","            (default: 'COLAMD')\n","        \n","            - ``NATURAL``: natural ordering.\n","            - ``MMD_ATA``: minimum degree ordering on the structure of A^T A.\n","            - ``MMD_AT_PLUS_A``: minimum degree ordering on the structure of A^T+A.\n","            - ``COLAMD``: approximate minimum degree column ordering\n","        use_umfpack : bool, optional\n","            if True (default) then use umfpack for the solution.  This is\n","            only referenced if b is a vector and ``scikit-umfpack`` is installed.\n","        \n","        Returns\n","        -------\n","        x : ndarray or sparse matrix\n","            the solution of the sparse linear equation.\n","            If b is a vector, then x is a vector of size A.shape[1]\n","            If b is a matrix, then x is a matrix of size (A.shape[1], b.shape[1])\n","        \n","        Notes\n","        -----\n","        For solving the matrix expression AX = B, this solver assumes the resulting\n","        matrix X is sparse, as is often the case for very sparse inputs.  If the\n","        resulting X is dense, the construction of this sparse result will be\n","        relatively expensive.  In that case, consider converting A to a dense\n","        matrix and using scipy.linalg.solve or its variants.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import spsolve\n","        >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\n","        >>> B = csc_matrix([[2, 0], [-1, 0], [2, 0]], dtype=float)\n","        >>> x = spsolve(A, B)\n","        >>> np.allclose(A.dot(x).todense(), B.todense())\n","        True\n","    \n","    spsolve_triangular(A, b, lower=True, overwrite_A=False, overwrite_b=False, unit_diagonal=False)\n","        Solve the equation ``A x = b`` for `x`, assuming A is a triangular matrix.\n","        \n","        Parameters\n","        ----------\n","        A : (M, M) sparse matrix\n","            A sparse square triangular matrix. Should be in CSR format.\n","        b : (M,) or (M, N) array_like\n","            Right-hand side matrix in ``A x = b``\n","        lower : bool, optional\n","            Whether `A` is a lower or upper triangular matrix.\n","            Default is lower triangular matrix.\n","        overwrite_A : bool, optional\n","            Allow changing `A`. The indices of `A` are going to be sorted and zero\n","            entries are going to be removed.\n","            Enabling gives a performance gain. Default is False.\n","        overwrite_b : bool, optional\n","            Allow overwriting data in `b`.\n","            Enabling gives a performance gain. Default is False.\n","            If `overwrite_b` is True, it should be ensured that\n","            `b` has an appropriate dtype to be able to store the result.\n","        unit_diagonal : bool, optional\n","            If True, diagonal elements of `a` are assumed to be 1 and will not be\n","            referenced.\n","        \n","            .. versionadded:: 1.4.0\n","        \n","        Returns\n","        -------\n","        x : (M,) or (M, N) ndarray\n","            Solution to the system ``A x = b``. Shape of return matches shape\n","            of `b`.\n","        \n","        Raises\n","        ------\n","        LinAlgError\n","            If `A` is singular or not triangular.\n","        ValueError\n","            If shape of `A` or shape of `b` do not match the requirements.\n","        \n","        Notes\n","        -----\n","        .. versionadded:: 0.19.0\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csr_matrix\n","        >>> from scipy.sparse.linalg import spsolve_triangular\n","        >>> A = csr_matrix([[3, 0, 0], [1, -1, 0], [2, 0, 1]], dtype=float)\n","        >>> B = np.array([[2, 0], [-1, 0], [2, 0]], dtype=float)\n","        >>> x = spsolve_triangular(A, B)\n","        >>> np.allclose(A.dot(x), B)\n","        True\n","    \n","    svds(A, k=6, ncv=None, tol=0, which='LM', v0=None, maxiter=None, return_singular_vectors=True, solver='arpack')\n","        Compute the largest or smallest k singular values/vectors for a sparse matrix. The order of the singular values is not guaranteed.\n","        \n","        Parameters\n","        ----------\n","        A : {sparse matrix, LinearOperator}\n","            Array to compute the SVD on, of shape (M, N)\n","        k : int, optional\n","            Number of singular values and vectors to compute.\n","            Must be 1 <= k < min(A.shape).\n","        ncv : int, optional\n","            The number of Lanczos vectors generated\n","            ncv must be greater than k+1 and smaller than n;\n","            it is recommended that ncv > 2*k\n","            Default: ``min(n, max(2*k + 1, 20))``\n","        tol : float, optional\n","            Tolerance for singular values. Zero (default) means machine precision.\n","        which : str, ['LM' | 'SM'], optional\n","            Which `k` singular values to find:\n","        \n","                - 'LM' : largest singular values\n","                - 'SM' : smallest singular values\n","        \n","            .. versionadded:: 0.12.0\n","        v0 : ndarray, optional\n","            Starting vector for iteration, of length min(A.shape). Should be an\n","            (approximate) left singular vector if N > M and a right singular\n","            vector otherwise.\n","            Default: random\n","        \n","            .. versionadded:: 0.12.0\n","        maxiter : int, optional\n","            Maximum number of iterations.\n","        \n","            .. versionadded:: 0.12.0\n","        return_singular_vectors : bool or str, optional\n","            - True: return singular vectors (True) in addition to singular values.\n","        \n","            .. versionadded:: 0.12.0\n","        \n","            - \"u\": only return the u matrix, without computing vh (if N > M).\n","            - \"vh\": only return the vh matrix, without computing u (if N <= M).\n","        \n","            .. versionadded:: 0.16.0\n","        solver : str, optional\n","                Eigenvalue solver to use. Should be 'arpack' or 'lobpcg'.\n","                Default: 'arpack'\n","        \n","        Returns\n","        -------\n","        u : ndarray, shape=(M, k)\n","            Unitary matrix having left singular vectors as columns.\n","            If `return_singular_vectors` is \"vh\", this variable is not computed,\n","            and None is returned instead.\n","        s : ndarray, shape=(k,)\n","            The singular values.\n","        vt : ndarray, shape=(k, N)\n","            Unitary matrix having right singular vectors as rows.\n","            If `return_singular_vectors` is \"u\", this variable is not computed,\n","            and None is returned instead.\n","        \n","        \n","        Notes\n","        -----\n","        This is a naive implementation using ARPACK or LOBPCG as an eigensolver\n","        on A.H * A or A * A.H, depending on which one is more efficient.\n","        \n","        Examples\n","        --------\n","        >>> from scipy.sparse import csc_matrix\n","        >>> from scipy.sparse.linalg import svds, eigs\n","        >>> A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, -1, 0], [0, 0, 3]], dtype=float)\n","        >>> u, s, vt = svds(A, k=2)\n","        >>> s\n","        array([ 2.75193379,  5.6059665 ])\n","        >>> np.sqrt(eigs(A.dot(A.T), k=2)[0]).real\n","        array([ 5.6059665 ,  2.75193379])\n","    \n","    use_solver(**kwargs)\n","        Select default sparse direct solver to be used.\n","        \n","        Parameters\n","        ----------\n","        useUmfpack : bool, optional\n","            Use UMFPACK over SuperLU. Has effect only if scikits.umfpack is\n","            installed. Default: True\n","        assumeSortedIndices : bool, optional\n","            Allow UMFPACK to skip the step of sorting indices for a CSR/CSC matrix.\n","            Has effect only if useUmfpack is True and scikits.umfpack is installed.\n","            Default: False\n","        \n","        Notes\n","        -----\n","        The default sparse solver is umfpack when available\n","        (scikits.umfpack is installed). This can be changed by passing\n","        useUmfpack = False, which then causes the always present SuperLU\n","        based solver to be used.\n","        \n","        Umfpack requires a CSR/CSC matrix to have sorted column/row indices. If\n","        sure that the matrix fulfills this, pass ``assumeSortedIndices=True``\n","        to gain some speed.\n","\n","DATA\n","    __all__ = ['ArpackError', 'ArpackNoConvergence', 'LinearOperator', 'Ma...\n","\n","FILE\n","    /usr/local/lib/python3.7/dist-packages/scipy/sparse/linalg/__init__.py\n","\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5N3dUEoHCA7","outputId":"eefd4ebf-41fa-496d-d995-b29169536848"},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on function bicgstab in module scipy.sparse.linalg.isolve.iterative:\n","\n","bicgstab(A, b, x0=None, tol=1e-05, maxiter=None, M=None, callback=None, atol=None)\n","    Use BIConjugate Gradient STABilized iteration to solve ``Ax = b``.\n","    \n","    Parameters\n","    ----------\n","    A : {sparse matrix, dense matrix, LinearOperator}\n","        The real or complex N-by-N matrix of the linear system.\n","        Alternatively, ``A`` can be a linear operator which can\n","        produce ``Ax`` using, e.g.,\n","        ``scipy.sparse.linalg.LinearOperator``.\n","    b : {array, matrix}\n","        Right hand side of the linear system. Has shape (N,) or (N,1).\n","    \n","    Returns\n","    -------\n","    x : {array, matrix}\n","        The converged solution.\n","    info : integer\n","        Provides convergence information:\n","            0  : successful exit\n","            >0 : convergence to tolerance not achieved, number of iterations\n","            <0 : illegal input or breakdown\n","    \n","    Other Parameters\n","    ----------------\n","    x0  : {array, matrix}\n","        Starting guess for the solution.\n","    tol, atol : float, optional\n","        Tolerances for convergence, ``norm(residual) <= max(tol*norm(b), atol)``.\n","        The default for ``atol`` is ``'legacy'``, which emulates\n","        a different legacy behavior.\n","    \n","        .. warning::\n","    \n","           The default value for `atol` will be changed in a future release.\n","           For future compatibility, specify `atol` explicitly.\n","    maxiter : integer\n","        Maximum number of iterations.  Iteration will stop after maxiter\n","        steps even if the specified tolerance has not been achieved.\n","    M : {sparse matrix, dense matrix, LinearOperator}\n","        Preconditioner for A.  The preconditioner should approximate the\n","        inverse of A.  Effective preconditioning dramatically improves the\n","        rate of convergence, which implies that fewer iterations are needed\n","        to reach a given error tolerance.\n","    callback : function\n","        User-supplied function to call after each iteration.  It is called\n","        as callback(xk), where xk is the current solution vector.\n","\n"]}],"source":["help(sparse.linalg.bicgstab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psrBG4cxHCA7","outputId":"db70c8ef-6879-44a8-93a0-08342910fc2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 1  5  0  0  0]\n"," [ 0  2  8  0  0]\n"," [ 0  0  3  9  0]\n"," [ 0  0  0  4 10]\n"," [ 0  0  0  0  5]]\n"]}],"source":["print(mtx.todense())"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6wtScbCHCA7","executionInfo":{"status":"ok","timestamp":1663601864095,"user_tz":-120,"elapsed":14,"user":{"displayName":"Edie Miglio","userId":"11723019705501296075"}},"outputId":"0daf1897-ea31-46fb-d033-e0149bd5056b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([106.00000002, -21.        ,   5.5       ,  -1.5       ,\n","         1.        ])"]},"metadata":{},"execution_count":8}],"source":["x,info =sparse.linalg.bicgstab(mtx,rhs,tol=1e-10,maxiter=10)\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C36inCpgHCA8","outputId":"b8ca9e59-b682-49b9-8d52-c9474e31afbf"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["info"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4gVpyak2HCA8"},"source":["### Read Matrix Market format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhHmyieMHCA8","outputId":"ac20e883-11df-47c1-c166-6c3092a6b02a"},"outputs":[{"data":{"text/plain":["scipy.sparse.coo.coo_matrix"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["import scipy.io as scio\n","import matplotlib.pyplot as plt\n","A=scio.mmread('mcfe.mtx')\n","type(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIX61hMrHCA8","outputId":"0f2c65d3-adc1-4184-c7be-24ae358b140f"},"outputs":[{"ename":"TypeError","evalue":"spy() missing 1 required positional argument: 'Z'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-22ff7494a158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: spy() missing 1 required positional argument: 'Z'"]}],"source":["plt.show(plt.spy())"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4kSJXFwqHCA8"},"source":["### Laplacian 2D\n","\n","See a separate [notebook.](2DLaplaceFD.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNpzp_anHCA8"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}