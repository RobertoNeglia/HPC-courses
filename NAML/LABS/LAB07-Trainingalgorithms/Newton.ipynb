{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCDNUgw0nI3"
      },
      "source": [
        "# Newton method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IJcJCwFhc8cs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "# We enable double precision in JAX\n",
        "from jax.config import config\n",
        "\n",
        "config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7qkdJdGg1AP1"
      },
      "source": [
        "We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n",
        "We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c0h8ihCddDPf"
      },
      "outputs": [],
      "source": [
        "n = 100\n",
        "\n",
        "np.random.seed(0)\n",
        "A = np.random.randn(n, n)\n",
        "x_ex = np.random.randn(n)\n",
        "b = A @ x_ex"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UanVhF4xAVoX"
      },
      "source": [
        "Define the loss function\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(x):\n",
        "    return jnp.sum(jnp.square(A @ x - b))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZ9XGaiAs3X"
      },
      "source": [
        "By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function (_Hint_: use the `jacrev` or the `jacfwd`) function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KflmuLXld2T4"
      },
      "outputs": [],
      "source": [
        "grad = jax.grad(loss)\n",
        "hess = jax.jacfwd(jax.jacrev(loss))\n",
        "\n",
        "loss_jit = jax.jit(loss)\n",
        "grad_jit = jax.jit(grad)\n",
        "hess_jit = jax.jit(hess)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMg8ocDBndO"
      },
      "source": [
        "Check that the results are correct (up to machine precision).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "xZulGRQ1efFP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.35683433196323e-12\n",
            "9.195400840588531e-13\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "x_guess = np.random.randn(n)\n",
        "\n",
        "G_ad = grad_jit(x_guess)\n",
        "G_ex = 2 * A.T @ (A @ x_guess - b)\n",
        "print(np.linalg.norm(G_ad - G_ex))\n",
        "\n",
        "H_ad = hess_jit(x_guess)\n",
        "H_ex = 2 * A.T @ A\n",
        "print(np.linalg.norm(H_ad - H_ex))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gA_kKPB2SV"
      },
      "source": [
        "Exploit the formula\n",
        "\n",
        "$$\n",
        "\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n",
        "$$\n",
        "\n",
        "to write an optimized function returning the hessian-vector-product\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n",
        "$$\n",
        "\n",
        "Compare the computational performance w.r.t. the full hessian computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9969dU4kc6f",
        "outputId": "368b2173-e971-474c-e3b1-f649d9bb15d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0242789406024479e-12\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "v = np.random.randn(n)\n",
        "\n",
        "hvp_basic = lambda x, v: hess(x) @ v\n",
        "phi = lambda x, v: grad(x) @ v\n",
        "hvp = jax.grad(phi, argnums=0)\n",
        "\n",
        "hvp_basic_jit = jax.jit(hvp_basic)\n",
        "hvp_jit = jax.jit(hvp)\n",
        "\n",
        "Hv_ad = hvp_jit(x_guess, v)\n",
        "Hv_ex = H_ex @ v\n",
        "print(np.linalg.norm(Hv_ad - Hv_ex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jsA4eUnuj3ju"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "190 µs ± 5.29 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
            "9.23 µs ± 278 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit hvp_basic_jit(x_guess, v)\n",
        "%timeit hvp_jit(x_guess, v)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TagmrdjG4Ww4"
      },
      "source": [
        "Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- epoch 0\n",
            "loss: 5.122e+13\n",
            "incr: 1.000e+19\n",
            "--- epoch 1\n",
            "loss: 9.527e-08\n",
            "incr: 7.504e+08\n",
            "--- epoch 2\n",
            "loss: 7.992e-28\n",
            "incr: 3.147e-02\n",
            "--- epoch 3\n",
            "loss: 5.813e-28\n",
            "incr: 1.689e-12\n"
          ]
        }
      ],
      "source": [
        "max_iter = 100\n",
        "tol = 1.0e-8\n",
        "x = np.ones(x_guess.shape[0]) + 1000000000000000000\n",
        "for k in range(max_iter):\n",
        "    G = grad_jit(x)\n",
        "    H = hess_jit(x)\n",
        "    incr = np.linalg.solve(H, -G)\n",
        "    x += incr\n",
        "    l = loss_jit(x)\n",
        "    incr_norm = np.linalg.norm(incr)\n",
        "    print(\"--- epoch %d\" % k)\n",
        "    print(\"loss: %1.3e\" % l)\n",
        "    print(\"incr: %1.3e\" % incr_norm)\n",
        "    if incr_norm < tol:\n",
        "        break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uNL7303C4oTL"
      },
      "source": [
        "Repeat the optimization loop for the loss function\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_quad(x):\n",
        "    return jnp.sum(jnp.square(jnp.square(b - A @ x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "grad_quad = jax.grad(loss_quad)\n",
        "hess_quad = jax.jacfwd(jax.jacrev(loss_quad))\n",
        "\n",
        "loss_quad_jit = jax.jit(loss_quad)\n",
        "grad_quad_jit = jax.jit(grad_quad)\n",
        "hess_quad_jit = jax.jit(hess_quad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- epoch 0\n",
            "loss: 4.806e+07\n",
            "incr: 5.158e+00\n",
            "--- epoch 1\n",
            "loss: 9.493e+06\n",
            "incr: 3.439e+00\n",
            "--- epoch 2\n",
            "loss: 1.875e+06\n",
            "incr: 2.293e+00\n",
            "--- epoch 3\n",
            "loss: 3.704e+05\n",
            "incr: 1.528e+00\n",
            "--- epoch 4\n",
            "loss: 7.317e+04\n",
            "incr: 1.019e+00\n",
            "--- epoch 5\n",
            "loss: 1.445e+04\n",
            "incr: 6.793e-01\n",
            "--- epoch 6\n",
            "loss: 2.855e+03\n",
            "incr: 4.529e-01\n",
            "--- epoch 7\n",
            "loss: 5.639e+02\n",
            "incr: 3.019e-01\n",
            "--- epoch 8\n",
            "loss: 1.114e+02\n",
            "incr: 2.013e-01\n",
            "--- epoch 9\n",
            "loss: 2.200e+01\n",
            "incr: 1.342e-01\n",
            "--- epoch 10\n",
            "loss: 4.346e+00\n",
            "incr: 8.945e-02\n",
            "--- epoch 11\n",
            "loss: 8.585e-01\n",
            "incr: 5.964e-02\n",
            "--- epoch 12\n",
            "loss: 1.696e-01\n",
            "incr: 3.976e-02\n",
            "--- epoch 13\n",
            "loss: 3.350e-02\n",
            "incr: 2.651e-02\n",
            "--- epoch 14\n",
            "loss: 6.617e-03\n",
            "incr: 1.767e-02\n",
            "--- epoch 15\n",
            "loss: 1.307e-03\n",
            "incr: 1.178e-02\n",
            "--- epoch 16\n",
            "loss: 2.582e-04\n",
            "incr: 7.853e-03\n",
            "--- epoch 17\n",
            "loss: 5.100e-05\n",
            "incr: 5.236e-03\n",
            "--- epoch 18\n",
            "loss: 1.007e-05\n",
            "incr: 3.490e-03\n",
            "--- epoch 19\n",
            "loss: 1.990e-06\n",
            "incr: 2.327e-03\n",
            "--- epoch 20\n",
            "loss: 3.931e-07\n",
            "incr: 1.551e-03\n",
            "--- epoch 21\n",
            "loss: 7.764e-08\n",
            "incr: 1.034e-03\n",
            "--- epoch 22\n",
            "loss: 1.534e-08\n",
            "incr: 6.895e-04\n",
            "--- epoch 23\n",
            "loss: 3.029e-09\n",
            "incr: 4.596e-04\n",
            "--- epoch 24\n",
            "loss: 5.984e-10\n",
            "incr: 3.064e-04\n",
            "--- epoch 25\n",
            "loss: 1.182e-10\n",
            "incr: 2.043e-04\n",
            "--- epoch 26\n",
            "loss: 2.335e-11\n",
            "incr: 1.362e-04\n",
            "--- epoch 27\n",
            "loss: 4.612e-12\n",
            "incr: 9.079e-05\n",
            "--- epoch 28\n",
            "loss: 9.111e-13\n",
            "incr: 6.053e-05\n",
            "--- epoch 29\n",
            "loss: 1.800e-13\n",
            "incr: 4.035e-05\n",
            "--- epoch 30\n",
            "loss: 3.555e-14\n",
            "incr: 2.690e-05\n",
            "--- epoch 31\n",
            "loss: 7.022e-15\n",
            "incr: 1.793e-05\n",
            "--- epoch 32\n",
            "loss: 1.387e-15\n",
            "incr: 1.196e-05\n",
            "--- epoch 33\n",
            "loss: 2.740e-16\n",
            "incr: 7.971e-06\n",
            "--- epoch 34\n",
            "loss: 5.412e-17\n",
            "incr: 5.314e-06\n",
            "--- epoch 35\n",
            "loss: 1.069e-17\n",
            "incr: 3.543e-06\n",
            "--- epoch 36\n",
            "loss: 2.112e-18\n",
            "incr: 2.362e-06\n",
            "--- epoch 37\n",
            "loss: 4.171e-19\n",
            "incr: 1.574e-06\n",
            "--- epoch 38\n",
            "loss: 8.239e-20\n",
            "incr: 1.050e-06\n",
            "--- epoch 39\n",
            "loss: 1.628e-20\n",
            "incr: 6.998e-07\n",
            "--- epoch 40\n",
            "loss: 3.215e-21\n",
            "incr: 4.665e-07\n",
            "--- epoch 41\n",
            "loss: 6.350e-22\n",
            "incr: 3.110e-07\n",
            "--- epoch 42\n",
            "loss: 1.254e-22\n",
            "incr: 2.073e-07\n",
            "--- epoch 43\n",
            "loss: 2.478e-23\n",
            "incr: 1.382e-07\n",
            "--- epoch 44\n",
            "loss: 4.894e-24\n",
            "incr: 9.215e-08\n",
            "--- epoch 45\n",
            "loss: 9.668e-25\n",
            "incr: 6.143e-08\n",
            "--- epoch 46\n",
            "loss: 1.910e-25\n",
            "incr: 4.096e-08\n",
            "--- epoch 47\n",
            "loss: 3.772e-26\n",
            "incr: 2.730e-08\n",
            "--- epoch 48\n",
            "loss: 7.451e-27\n",
            "incr: 1.820e-08\n",
            "--- epoch 49\n",
            "loss: 1.472e-27\n",
            "incr: 1.214e-08\n",
            "--- epoch 50\n",
            "loss: 2.907e-28\n",
            "incr: 8.090e-09\n"
          ]
        }
      ],
      "source": [
        "max_iter = 100\n",
        "tol = 1.0e-8\n",
        "\n",
        "x = x_guess.copy()\n",
        "for k in range(max_iter):\n",
        "    G = grad_quad_jit(x)\n",
        "    H = hess_quad_jit(x)\n",
        "    incr = np.linalg.solve(H, -G)\n",
        "    x += incr\n",
        "    incr_norm = jnp.linalg.norm(incr)\n",
        "    l = loss_quad_jit(x)\n",
        "    print(\"--- epoch %d\" % k)\n",
        "    print(\"loss: %1.3e\" % l)\n",
        "    print(\"incr: %1.3e\" % incr_norm)\n",
        "    if incr_norm < tol:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
