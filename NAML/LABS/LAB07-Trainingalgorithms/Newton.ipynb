{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCDNUgw0nI3"
      },
      "source": [
        "# Newton method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcJCwFhc8cs"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "# We enable double precision in JAX\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qkdJdGg1AP1"
      },
      "source": [
        "We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n",
        "We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0h8ihCddDPf"
      },
      "source": [
        "n = 100\n",
        "\n",
        "np.random.seed(0)\n",
        "A = np.random.randn(n,n)\n",
        "x_ex = np.random.randn(n)\n",
        "b = A @ x_ex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UanVhF4xAVoX"
      },
      "source": [
        "Define the loss function\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZ9XGaiAs3X"
      },
      "source": [
        "By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function (*Hint*: use the `jacrev` or the `jacfwd`) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KflmuLXld2T4"
      },
      "source": [
        "grad = ...\n",
        "hess = ...\n",
        "\n",
        "loss_jit = ...\n",
        "grad_jit = ...\n",
        "hess_jit = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMg8ocDBndO"
      },
      "source": [
        "Check that the results are correct (up to machine precision)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZulGRQ1efFP"
      },
      "source": [
        "np.random.seed(0)\n",
        "x_guess = np.random.randn(n)\n",
        "\n",
        "G_ad = grad_jit(x_guess)\n",
        "G_ex = 2 * A.T @ (A @ x_guess - b)\n",
        "print(np.linalg.norm(G_ad - G_ex))\n",
        "\n",
        "H_ad = hess_jit(x_guess)\n",
        "H_ex = 2 * A.T @ A\n",
        "print(np.linalg.norm(H_ad - H_ex))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gA_kKPB2SV"
      },
      "source": [
        "Exploit the formula\n",
        "$$\n",
        "\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n",
        "$$\n",
        "where \n",
        "$$\n",
        "\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n",
        "$$\n",
        "to write an optimized function returning the hessian-vector-product\n",
        "$$\n",
        "(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n",
        "$$\n",
        "Compare the computational performance w.r.t. the full hessian computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9969dU4kc6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368b2173-e971-474c-e3b1-f649d9bb15d1"
      },
      "source": [
        "np.random.seed(1)\n",
        "v = np.random.randn(n)\n",
        "\n",
        "hvp_basic = lambda x, v: hess(x) @ v\n",
        "hvp = ...\n",
        "\n",
        "hvp_basic_jit = ...\n",
        "hvp_jit = ...\n",
        "\n",
        "Hv_ad = hvp_jit(x_guess, v)\n",
        "Hv_ex = H_ex @ v\n",
        "print(np.linalg.norm(Hv_ad - Hv_ex))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2715738311164232e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsA4eUnuj3ju"
      },
      "source": [
        "%timeit hvp_basic_jit(x_guess, v)\n",
        "%timeit hvp_jit(x_guess, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TagmrdjG4Ww4"
      },
      "source": [
        "Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNL7303C4oTL"
      },
      "source": [
        "Repeat the optimization loop for the loss function\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n",
        "$$"
      ]
    }
  ]
}