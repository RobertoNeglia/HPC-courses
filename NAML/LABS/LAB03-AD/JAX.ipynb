{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1aDiop5c-V90dbJAUkJjgfpwZHa4vvdOa","authorship_tag":"ABX9TyMO/dG6kzcvxKD6k1drTHdL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e7q1DhWpV-Dw"},"source":["# Auto-diff with JAX\n","\n","https://github.com/google/jax"]},{"cell_type":"markdown","metadata":{"id":"Snjf4u7vf0sa"},"source":["JAX is a Google research project, developed by the former developers of [Autograd](https://github.com/hips/autograd), bringing together the potentialities of Autograd and the linear algebra accelerator [XLA](https://www.tensorflow.org/xla). It is based on three pillars:\n","- `grad`: Automatic Differentiation\n","- `jit`: Just-in-time compilation\n","- `vmap`: Automatic vectorization.\n","\n","## Automatic differentiation in JAX\n","\n","JAX augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs. JAX's augmented numpy lives at `jax.numpy`. With a few exceptions, you can think of `jax.numpy` as directly interchangeable with `numpy`. As a general rule, you should use `jax.numpy` whenever you plan to use any of JAX's transformations.\n","\n","The function `df = jax.grad(f, argnums = 0)` takes the callable object `f` and returns another callable object, `df`, evaluating the gradient of `f` w.r.t. the argument(s) of index(es) `argnums`. For more information, check out the [documentation](https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.grad)."]},{"cell_type":"markdown","metadata":{"id":"yBVAFA6LiZhv"},"source":["**Example**\n","\n","We consider the function:\n","$$\n","f(x) = x \\sin(x^2)\n","$$\n","\n","and we compute $f'(x_0)$ for $x_0 = 0.13$"]},{"cell_type":"code","metadata":{"id":"pJsIHEuC0BwB"},"source":["import numpy as np\n","import jax.numpy as jnp\n","import jax\n","\n","func = ...\n","x0 = 0.13\n","dfunc_AD = ...\n","df_AD = ...\n","\n","# analytical derivative\n","dfunc = lambda x : np.sin(x**2)+2 * x**2 * np.cos(x**2)\n","df_ex = dfunc(x0)\n","\n","print('df (ex): %f' % df_ex)\n","print('df (AD): %f' % df_AD)\n","\n","print('err (AD): %e' % (abs(df_AD - df_ex)/abs(df_ex)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate the execution times of the functions `func` and `dfunc_AD`."],"metadata":{"id":"Jt667yP0fy73"}},{"cell_type":"markdown","metadata":{"id":"v9ziWf_lcjLn"},"source":["### Speed it up with JIT!\n","\n","Compile the functions `func` and `dfunc_AD` using the [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation) utility `jax.jit`. \n","\n","With `f_jit = jax.jit(f)` a callable `f` is compiled into `f_jit`.\n","\n","Then, check that the compiled functions return the same results as the original ones. Finally, evaluate the execution times and compare it with the previous results."]}]}