{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"e7q1DhWpV-Dw"},"source":["# Auto-diff with JAX\n","\n","https://github.com/google/jax\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Snjf4u7vf0sa"},"source":["JAX is a Google research project, developed by the former developers of [Autograd](https://github.com/hips/autograd), bringing together the potentialities of Autograd and the linear algebra accelerator [XLA](https://www.tensorflow.org/xla). It is based on three pillars:\n","\n","- `grad`: Automatic Differentiation\n","- `jit`: Just-in-time compilation\n","- `vmap`: Automatic vectorization.\n","\n","## Automatic differentiation in JAX\n","\n","JAX augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs. JAX's augmented numpy lives at `jax.numpy`. With a few exceptions, you can think of `jax.numpy` as directly interchangeable with `numpy`. As a general rule, you should use `jax.numpy` whenever you plan to use any of JAX's transformations.\n","\n","The function `df = jax.grad(f, argnums = 0)` takes the callable object `f` and returns another callable object, `df`, evaluating the gradient of `f` w.r.t. the argument(s) of index(es) `argnums`. For more information, check out the [documentation](https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.grad).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yBVAFA6LiZhv"},"source":["**Example**\n","\n","We consider the function:\n","\n","$$\n","f(x) = x \\sin(x^2)\n","$$\n","\n","and we compute $f'(x_0)$ for $x_0 = 0.13$\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pJsIHEuC0BwB"},"outputs":[{"name":"stdout","output_type":"stream","text":["df (ex): 0.050694\n","df (AD): 0.050694\n","err (AD): 7.348529e-08\n"]}],"source":["import numpy as np\n","import jax.numpy as jnp\n","import jax\n","\n","func = lambda x: x * jnp.sin(x**2)\n","x0 = 0.13\n","dfunc_AD = jax.grad(func)\n","df_AD = dfunc_AD((x0))\n","\n","# analytical derivative\n","dfunc = lambda x: np.sin(x**2) + 2 * x**2 * np.cos(x**2)\n","df_ex = dfunc(x0)\n","\n","print(\"df (ex): %f\" % df_ex)\n","print(\"df (AD): %f\" % df_AD)\n","\n","print(\"err (AD): %e\" % (abs(df_AD - df_ex) / abs(df_ex)))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Jt667yP0fy73"},"source":["Evaluate the execution times of the functions `func` and `dfunc_AD`.\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["11.3 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"]}],"source":["%timeit func(x0)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3.25 ms ± 93.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["%timeit dfunc_AD(x0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"v9ziWf_lcjLn"},"source":["### Speed it up with JIT!\n","\n","Compile the functions `func` and `dfunc_AD` using the [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation) utility `jax.jit`.\n","\n","With `f_jit = jax.jit(f)` a callable `f` is compiled into `f_jit`.\n","\n","Then, check that the compiled functions return the same results as the original ones. Finally, evaluate the execution times and compare it with the previous results.\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["df (JIT): 0.050694\n","df (JIT-JIT): 0.050694\n","df (AD-JIT): 0.050694\n","err (JIT): 7.348529e-08\n","err (JIT-JIT): 7.348529e-08\n","err (AD-JIT): 7.348529e-08\n"]}],"source":["# JIT of the original function\n","func_jit = jax.jit(func)\n","\n","# AD of the JIT'd function\n","dfunc_jit = jax.grad(func_jit)\n","df_jit = dfunc_jit(x0)\n","\n","# JIT ot the AD of the JIT'd function\n","dfunc_jit_jit = jax.jit(dfunc_jit)\n","df_jit_jit = dfunc_jit_jit(x0)\n","\n","# JIT of the AD of the original function (NOT JIT'd previously)\n","dfunc_AD_jit = jax.jit(dfunc_AD)\n","df_AD_jit = dfunc_AD_jit(x0)\n","\n","\n","print(\"df (JIT): %f\" % df_jit)\n","print(\"df (JIT-JIT): %f\" % df_jit_jit)\n","print(\"df (AD-JIT): %f\" % df_AD_jit)\n","print(\"err (JIT): %e\" % (abs(df_jit - df_ex) / abs(df_ex)))\n","print(\"err (JIT-JIT): %e\" % (abs(df_jit_jit - df_ex) / abs(df_ex)))\n","print(\"err (AD-JIT): %e\" % (abs(df_AD_jit - df_ex) / abs(df_ex)))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4.55 µs ± 50.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"]}],"source":["%timeit func_jit(x0)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.57 ms ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"]}],"source":["%timeit dfunc_jit(x0)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4.66 µs ± 43.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"]}],"source":["%timeit dfunc_jit_jit(x0)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4.58 µs ± 51.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"]}],"source":["%timeit dfunc_AD_jit(x0)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMO/dG6kzcvxKD6k1drTHdL","collapsed_sections":[],"mount_file_id":"1aDiop5c-V90dbJAUkJjgfpwZHa4vvdOa","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
