{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jPUgULFGwoy5C8YoJgAzpAJgjvsLC5RD","authorship_tag":"ABX9TyN9uuTdPVzEMsp+U7fjP7Zf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VKvkeUvRrBtr"},"source":["# Regularization of ANN weights\n"]},{"cell_type":"code","metadata":{"id":"Si_DpGyVWZ5I"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import time\n","import jax.numpy as jnp\n","import jax"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tORMQT4qrH9C"},"source":["Load the [Auto MPG dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg)."]},{"cell_type":"code","source":["url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n","column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n","data = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\n","data"],"metadata":{"id":"iKmCEoGjLpkK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ceh3Vv7bUXvN"},"source":["Check is there are missing entries in the dataset."]},{"cell_type":"code","metadata":{"id":"DT0cq8gGUS-N"},"source":["print(data.isna().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hVa1Yw4CUBk"},"source":["Remove records with missing entries."]},{"cell_type":"code","metadata":{"id":"mUziyL4kUVOj"},"source":["data = data.dropna()\n","print(data.isna().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8exyJVturUOy"},"source":["## Data inspection\n","\n","Display some basic information."]},{"cell_type":"code","metadata":{"id":"apJCc1IPrTyp"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66g2s9v9nFLh"},"source":["data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXDJ4ARqnPfY"},"source":["data.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GN6DWxerbXy"},"source":["We are interested in predicting the field `MPG`, measuring [fuel efficiency](https://en.wikipedia.org/wiki/Fuel_efficiency#:~:text=Fuel%20economy%20is%20the%20distance,a%20certain%20volume%20of%20fuel)), expressed in miles per gallon (MPG), where 1 MPG = 0.354006 km/L. Plot its distribution."]},{"cell_type":"code","metadata":{"id":"wPG0IsEWnWlN"},"source":["sns.displot(data['MPG'], kde = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqF8zt4pCxTO"},"source":["Look for linear correlations among data."]},{"cell_type":"code","metadata":{"id":"tBNLx91znnpu"},"source":["data.corr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYkJHwf_np_j"},"source":["sns.heatmap(data.corr(), annot = True, cmap = 'vlag_r', vmin = -1, vmax = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Hk4oyCmCc15"},"source":["sns.pairplot(data, diag_kind='kde')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJc_xoA3vLiv"},"source":["## Data normalization\n","\n","Apply an affine transformation to the data, so that each feature has zero mean and unitary standard deviation."]},{"cell_type":"code","metadata":{"id":"9eVFn6wkpDk7"},"source":["data_mean = data.mean()\n","data_std = data.std()\n","data_normalized = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfyO7bropWw7"},"source":["_, ax = plt.subplots(figsize=(16,6))\n","sns.violinplot(data = data_normalized, ax = ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-Hzcd6RvrHK"},"source":["## Train-validation split\n","\n","Shuffle the data using the [np.random.shuffle](https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html) function and split the data as follows:\n","- put 80% in the train dataset\n","- put 20% in the validation dataset"]},{"cell_type":"code","metadata":{"id":"dT4j1Jb1AWWi"},"source":["data_normalized_np = data_normalized.to_numpy()\n","np.random.seed(0)\n","np.random.shuffle(data_normalized_np)\n","\n","fraction_validation = 0.2\n","num_train = int(data_normalized_np.shape[0] * (1 - fraction_validation))\n","x_train = data_normalized_np[:num_train,1:]\n","y_train = data_normalized_np[:num_train,:1]\n","x_valid = data_normalized_np[num_train:,1:]\n","y_valid = data_normalized_np[num_train:,:1]\n","\n","print('train set size     : %d' % x_train.shape[0])\n","print('validation set size: %d' % x_valid.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lz8v0XDnwLqw"},"source":["## ANN setup\n","\n","Write a function `params = initialize_params(layers_size)` that initializes the parameters, given the ANN architecture.\n","Initialize biases with zero values, and weights with a [Glorot Normal](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) initialization, i.e. sampling from a Gaussian distribution with zero mean and with standard deviation \n","$$\n","\\sqrt{\\frac{2}{n + m}},\n","$$\n","where $n$ and $m$ are the number of input and output neurons of the corresponding weights matrix.\n"]},{"cell_type":"code","metadata":{"id":"Sz4fSpfmzxnp"},"source":["def initialize_params(layers_size):\n","  np.random.seed(0) # for reproducibility\n","  params = list()\n","  ...\n","  return params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EF8tDoECD9z1"},"source":["Implement a generic feedforward ANN with a function `y = ANN(x, params)`, using $ReLU$ as activation function."]},{"cell_type":"code","metadata":{"id":"TtwLtoDzqaM2"},"source":["activation = lambda x: jnp.maximum(0.0, x)\n","\n","def ANN(x, params):\n","  ...\n","\n","\n","params = initialize_params([7, 10, 1])\n","ANN(x_train[:10,:], params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmvIfrrQ0Fl_"},"source":["Implement the quadratic loss (MSE) function `L = MSE(x, y, params)`."]},{"cell_type":"code","metadata":{"id":"svtW7wfW0Da_"},"source":["def MSE(x, y, params):\n","  ...\n","\n","params = initialize_params([7, 10, 1])\n","print(MSE(x_train, y_train, params))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naytNmfsHV_B"},"source":["Implement an $l^2$ regularization term for the ANN weights:\n","$$\n","\\mathrm{MSW} = \\frac{1}{n_{weights}} \\sum_{i=1}^{n_{weights}} w_i^2\n","$$\n","and define the loss function as\n","$$\n","\\mathcal{L} = \\mathrm{MSE} + \\beta \\, \\mathrm{MSW}\n","$$\n","where $\\beta$ is a suitable penalization parameter."]},{"cell_type":"code","metadata":{"id":"wVE1y4lLERez"},"source":["def MSW(params):\n","  ...\n","\n","def loss(x, y, params, penalization):\n","  ...\n","\n","print(MSW(params))\n","print(loss(x_train, y_train, params, 1.0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afi3qB3P0UFe"},"source":["Run this cell: we will this callback to monitor training."]},{"cell_type":"code","metadata":{"id":"iFgDkl7dqkVy"},"source":["from IPython import display\n","\n","class Callback:\n","  def __init__(self, refresh_rate = 250):\n","    self.refresh_rate = refresh_rate\n","    self.fig, self.axs = plt.subplots(1, figsize=(16,8))\n","    self.epoch = 0\n","    self.__call__(-1)\n","\n","  def __call__(self, epoch):\n","    self.epoch = epoch\n","    if (epoch + 1) % self.refresh_rate == 0:\n","      self.draw()\n","      display.clear_output(wait=True)\n","      display.display(plt.gcf())\n","      time.sleep(1e-16)\n","\n","  def draw(self):\n","    if self.epoch > 0:\n","      self.axs.clear()\n","      self.axs.loglog(history_loss_train, 'b-', label = 'loss train')\n","      self.axs.loglog(history_loss_valid, 'r-', label = 'loss validation')\n","      self.axs.loglog(history_MSE_train, 'b--', label = 'RMSE train')\n","      self.axs.loglog(history_MSE_valid, 'r--', label = 'RMSE validation')\n","      self.axs.legend()\n","      self.axs.set_title('epoch %d' % (self.epoch + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iVlrCC00eKd"},"source":["## Training\n","\n","Train an ANN with two hidden layers with 20 neurons each, using 5000 epochs of the SGD method (with minibatch size 100) with momentum ($\\alpha = 0.9$).\n","Employ a linear decay of the learning rate:\n","$$\n","\\lambda_k = \\max\\left(\\lambda_\\min, \\lambda_\\max \\left(1 - \\frac{k}{K}\\right)\\right)\n","$$\n","with $\\lambda_\\min = 5e-3$, $\\lambda_\\max = 1e-1$ and decay length $K= 1000$.\n","\n","During training, store both the MSE error and the loss function obtained on the train and validation sets in 4 lists, respectively called:\n","- `history_loss_train`\n","- `history_loss_valid`\n","- `history_MSE_train`\n","- `history_MSE_valid`\n","\n","Test different choices of the penalization parameter $\\beta$.\n","\n"]},{"cell_type":"code","metadata":{"id":"GYcMlwxltMt6"},"source":["# Hyperparameters\n","layers_size = [7, 20, 20, 1]\n","penalization = 0.0\n","# Training options\n","num_epochs = 5000\n","learning_rate_max = 1e-1\n","learning_rate_min = 5e-3\n","learning_rate_decay = 1000\n","batch_size = 100\n","alpha = 0.9\n","########################################\n","\n","...\n","\n","history_loss_train = list()\n","history_loss_valid = list()\n","history_MSE_train = list()\n","history_MSE_valid = list()\n","\n","...\n","\n","print('loss (train     ): %1.3e' % history_loss_train[-1])\n","print('loss (validation): %1.3e' % history_loss_valid[-1])\n","print('MSE  (train     ): %1.3e' % history_MSE_train[-1])\n","print('MSE  (validation): %1.3e' % history_MSE_valid[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtM6IQsxJomr"},"source":["We now want to to investigate more in depth the effect of the penalization parameter $\\beta$.\n","Write a function that, given the penalization parameter, trains the ANN (with the same setting used above) and returns a dictionary containing the final values of:\n","- train loss\n","- validation loss\n","- train MSE\n","- validation MSE\n","- MSW"]},{"cell_type":"code","metadata":{"id":"QTGrkHNOQZEC"},"source":["# Hyperparameters\n","layers_size = [7, 20, 20, 1]\n","# Training options\n","num_epochs = 5000\n","learning_rate_max = 1e-1\n","learning_rate_min = 5e-3\n","learning_rate_decay = 1000\n","batch_size = 100\n","alpha = 0.9\n","\n","def train(penalization):\n","\n","  ...\n","\n","  return {\n","      'MSE_train'  : ... ,\n","      'MSE_valid'  : ... ,\n","      'MSW'        : ... ,\n","      }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8rOapXpKT_Y"},"source":["Using the above defined function, store the obtained results for $\\beta = 0, 0.25, 0.5, 0.75, \\dots, 2$."]},{"cell_type":"markdown","metadata":{"id":"3O29cV3oKim4"},"source":["Plot the trend of the five quantities as functions of $\\beta$."]},{"cell_type":"markdown","metadata":{"id":"0HtJD2jsKq0Y"},"source":["Plot the *Tikhonov L-curve*, which is - in this context - the curve \"train MSE\" versus \"MSW\"."]}]}