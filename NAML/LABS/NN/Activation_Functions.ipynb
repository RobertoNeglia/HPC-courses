{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "FrsmqVPa_l2L",
        "outputId": "63f8c2b8-be9d-48e5-a1f6-f5113142b9a1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import integrate\n",
        "\n",
        "# Data for plotting\n",
        "t = np.arange(-5.0, 5.0, 1)\n",
        "#s = t>0                              # Step\n",
        "#s = t                                # Linear\n",
        "#s = 1/(1+np.exp(-t))                 # Sigmoid\n",
        "#ds = s*(1-s)                         # Derivative of the sigmoid\n",
        "#s = np.tanh(t)                       # Tanh\n",
        "#ds = 1-s**2                          # Derivative of tanh\n",
        "#s = np.maximum(0,t)                  # ReLU\n",
        "#ds = s>0                             # Derivative of the ReLU\n",
        "#alpha = 0.1                          # Parameter for the Leaky ReLU\n",
        "#s = np.maximum(alpha*t,t)            # Leaky ReLU\n",
        "#ds = (s<0)*alpha+(s>=0)              # Derivative of the Leaky ReLU\n",
        "#a = 1                                # Parameter for the ELU\n",
        "#s = (t<0)*a*(np.exp(t)-1)+(t>=0)*t   # ELU\n",
        "#ds = (t<0)*(s+a)+(t>=0)              # Derivative of the ELU\n",
        "#s = t/(1+np.exp(-t))                 # Swish\n",
        "s = np.exp(t)/np.sum(np.exp(t))      # Softmax\n",
        "print(sum(s))                        # Softmax probabilities sum to 1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(t, s)\n",
        "#ax.plot(t,ds,'r')\n",
        "\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAco0lEQVR4nO3deXRcZ5nn8e+jkmR5kbxHdmwnXkNiHOIExc5ig4DQ49CQNMPmMMyQM9DunoOZpNkmwJwcSHcP3dCHhiGeYQwDp4fpxIQAg2kMbjpQieQmiR3bWWzHVtlxbCt2ovIuW7JUqmf+qJJUUqSoZN+qW8vvc07Fd3l136feI/9889a9dc3dERGR4lcRdgEiIhIMBbqISIlQoIuIlAgFuohIiVCgi4iUiMqwOp42bZrPnTs3rO4Dce7cOcaPHx92GQVD49FPYzGQxmOgSxmPZ555Ju7u04faF1qgz507l23btoXVfSCi0SiNjY1hl1EwNB79NBYDaTwGupTxMLOXh9unKRcRkRKhQBcRKREKdBGREqFAFxEpEQp0EZESoUAXESkRCnQRkRIR2nXoIiLlJpl0vvbrPcxK9OTk+DpDFxHJk91Hz/C9ppdobU/m5PgKdBGRPGlqiQPw5qmRnBxfgS4ikifNsTbeVF/LpJrcRK8CXUQkDzq7e9h68CQrFk3LWR8KdBGRPHj6pRN0JZKsVKCLiBS3ppY2qiMVLJ83NWd9KNBFRPKgqSXOW6+czNjq3HwgClkGupmtMrO9ZhYzs/uGafNhM9ttZrvM7KFgyxQRKV6vne3kxWNnWXlV7qZbIIsbi8wsAqwD3g0cAbaa2UZ3353RZhHwReBWdz9pZpflqmARkWKzJZa6XHHlwiEfNBSYbM7QlwExdz/g7l3ABuDOQW3+FFjn7icB3P21YMsUESleTS1xJo+r4s2X1+W0n2xu/Z8FHM5YPwIsH9TmKgAz2wJEgK+4+28GH8jM1gBrAOrr64lGoxdRcuFob28v+vcQJI1HP43FQOU8Hu7O73Z1cNXkCp544nEgd+MR1He5VAKLgEZgNvCEmV3r7qcyG7n7emA9QENDgxf7Mwb1nMSBNB79NBYDlfN47Hv1LKc2P8EHbl1M441XALkbj2ymXFqBORnrs9PbMh0BNrp7t7u/BOwjFfAiImXtiX1tAKxYlNv5c8gu0LcCi8xsnplVA6uBjYPa/D9SZ+eY2TRSUzAHAqxTRKQoNcfizJ82nlmTxua8rxED3d0TwFpgM7AHeMTdd5nZA2Z2R7rZZuC4me0Gfg983t2P56poEZFicCHRw1MHTuT0dv9MWc2hu/smYNOgbfdnLDvwmfRLRESA7S+foqO7h5V5mG4B3SkqIpIzTS1tRCqMm+ZPyUt/CnQRkRxpjsW5fs4kamuq8tKfAl1EJAdOnuvi+dbTeZtuAQW6iEhObNkfx528fSAKCnQRkZxobolTW1PJdbMn5q1PBbqISMDcnaaWODfPn0plJH8xq0AXEQnYwePnaT3Vwcqr8jd/Dgp0EZHANbWkbvdfuTB/8+egQBcRCVxTS5zZk8dy5dRxee1XgS4iEqDuniRP7j/OykXTMbO89q1AFxEJ0LOHT3H2QoKVebxcsZcCXUQkQE0tcczglgVT8963Al1EJEDNsThvmTWRSeOq8963Al1EJCBnOrvZefhUXm/3z6RAFxEJyB/2H6cn6Xm93T+TAl1EJCDNLXHGVUe44YrJofSvQBcRCUhzLM5N86dSXRlOtCrQRUQCcPjEeV6Kn2NFnu8OzaRAFxEJQHMsDhDK9ee9FOgiIgFobolTXzeGhZdNCK0GBbqIyCXqSTpb9sdDud0/kwJdROQSvdB6mlPnu0OdboEsA93MVpnZXjOLmdl9Q+y/28zazGxn+vXJ4EsVESlMvfPnt4b4gShA5UgNzCwCrAPeDRwBtprZRnffPajpj919bQ5qFBEpaE0tbVwzs45pE8aEWkc2Z+jLgJi7H3D3LmADcGduyxIRKQ7nuxI88/JJ3hbydAtkF+izgMMZ60fS2wb7gJk9Z2aPmtmcQKoTESlwTx04QXdPeLf7ZxpxyiVLvwQedvcLZvZnwD8A7xzcyMzWAGsA6uvriUajAXUfjvb29qJ/D0HSePTTWAxUyuPx8J4LVFZAx6EXiLZmd4VLrsYjm0BvBTLPuGent/Vx9+MZq98Hvj7Ugdx9PbAeoKGhwRsbG0dTa8GJRqMU+3sIksajn8ZioFIej/+243FuXlDDH71redY/k6vxyGbKZSuwyMzmmVk1sBrYmNnAzGZmrN4B7AmuRBGRwvTqmU72vdoe6u3+mUY8Q3f3hJmtBTYDEeAH7r7LzB4Atrn7RuA/m9kdQAI4Adydw5pFRApCU0vqcsVCmD+HLOfQ3X0TsGnQtvszlr8IfDHY0kRECltzSxtTx1dzzYy6sEsBdKeoiMhFSSad5thxViyaRkVFeLf7Z1Kgi4hchBePnSXefqFg5s9BgS4iclGaY20AoT0/dCgKdBGRi9DUEmfRZROYMbEm7FL6KNBFREaps7uHp186UTBXt/RSoIuIjNK2gye5kEiG/nW5gynQRURGqSnWRlXEWD5vatilDKBAFxEZpeaWODdcMZnxY4L6OqxgKNBFREYh3n6BXa+cKbjpFlCgi4iMypZY7+3+hXO5Yi8FuojIKDS3xJk4toprZ00Mu5TXUaCLiGTJ3WmOxbl14VQiBXK7fyYFuohIlva3tXP0dCcrFhbedAso0EVEstb7dbmF+IEoKNBFRLLW3BJn7tRxzJkyLuxShqRAFxHJQlciyZMHjhfc7f6ZFOgiIlnYcegk57p6Cnb+HBToIiJZaY7FiVQYNy8orNv9MynQRUSy0NQS57rZE5k4tirsUoalQBcRGcHp8908d+RUQd4dmkmBLiIygn/dHyfphXu5Yi8FuojICJpicSaMqWTpnElhl/KGFOgiIiNobolz0/ypVEUKOzKzqs7MVpnZXjOLmdl9b9DuA2bmZtYQXIkiIuF5+fg5Dp04X/DTLZBFoJtZBFgH3A4sBu4ys8VDtKsF7gGeCrpIEZGw9N7uX8g3FPXK5gx9GRBz9wPu3gVsAO4cot1fAn8LdAZYn4hIqJpb4syaNJb508aHXcqIsnl+0izgcMb6EWB5ZgMzuwGY4+6/MrPPD3cgM1sDrAGor68nGo2OuuBC0t7eXvTvIUgaj34ai4GKdTx6ks7je89z44xKHn/88cCOm6vxuOQH4plZBfBN4O6R2rr7emA9QENDgzc2Nl5q96GKRqMU+3sIksajn8ZioGIdj+2HTtLxz//Kh1ZeS+N1lwd23FyNRzZTLq3AnIz12eltvWqBJUDUzA4CNwEb9cGoiBS75pY4ZnDrwsKfP4fsAn0rsMjM5plZNbAa2Ni7091Pu/s0d5/r7nOBJ4E73H1bTioWEcmT5pY4Sy6fyJTx1WGXkpURA93dE8BaYDOwB3jE3XeZ2QNmdkeuCxQRCUP7hQTbD50siqtbemU1h+7um4BNg7bdP0zbxksvS0QkXE/uP04i6awskukW0J2iIiJDao7Fqamq4K1zJ4ddStYU6CIiQ2hqaWP5vKmMqYyEXUrWFOgiIoO8cqqD/W3niuJ2/0wKdBGRQZrTt/uvLPDvPx9MgS4iMkhTLM5ltWO4qn5C2KWMigJdRCRDMulsicVZsXAaZhZ2OaOiQBcRybD76BlOnOsqquvPeynQRUQy9H1dbhFdf95LgS4ikqE51sbVM2q5rK4m7FJGTYEuIpLW0dXD1pdOFuXZOSjQRUT6PH3wBF09SVZeVVyXK/ZSoIuIpDW3tFEdqWDZ3Clhl3JRFOgiImlNLXEa5k5mbHXx3O6fSYEuIgK8draTF4+dLcrLFXsp0EVEgC2x1OWKbyuy2/0zKdBFREhNt0wZX83imXVhl3LRFOgiUvbcneaWOLcsmEpFRXHd7p9JgS4iZW/fq+28dvZC0X1d7mAKdBEpe00tbQCsKOL5c1Cgi4jQHIszf/p4Zk0aG3Ypl0SBLiJl7UKihycPHC+qh0EPR4EuImXtmZdP0tmdLLqnEw0lq0A3s1VmttfMYmZ23xD7/9zMnjeznWbWbGaLgy9VRCR4zS1xKiuMmxZMDbuUSzZioJtZBFgH3A4sBu4aIrAfcvdr3X0p8HXgm4FXKiKSA00tca6/YhITxlSGXcoly+YMfRkQc/cD7t4FbADuzGzg7mcyVscDHlyJIiK5cfJcFy+8cpoVC4t/ugUgm3+SZgGHM9aPAMsHNzKzTwGfAaqBdw51IDNbA6wBqK+vJxqNjrLcwtLe3l707yFIGo9+GouBCnU8nj6awB3Gtx8iGm3NW7+5Go/A/h/D3dcB68zso8B/BT4+RJv1wHqAhoYGb2xsDKr7UESjUYr9PQRJ49FPYzFQoY7Hb376HLU1R7n7fe+gMpK/a0RyNR7ZvINWYE7G+uz0tuFsAP7kUooSEck1d6cpfbt/PsM8l7J5F1uBRWY2z8yqgdXAxswGZrYoY/WPgZbgShQRCd5L8XO0nuooicsVe4045eLuCTNbC2wGIsAP3H2XmT0AbHP3jcBaM7sN6AZOMsR0i4hIIWlOf11usX9/S6as5tDdfROwadC2+zOW7wm4LhGRnGpqiTNnyliunDo+7FICUxoTRyIio9Ddk+QP+4+XzOWKvRToIlJ2nj18ivYLCd5WQtMtoEAXkTLU1BKnwuCWBQp0EZGi1tTSxrWzJzFxXFXYpQRKgS4iZeVMZzfPHjldctMtoEAXkTLzh/3H6Uk6K0rg+88HU6CLSFlpbokzrjrC9VdMDruUwCnQRaSsNLW0cdP8qVRXll78ld47EhEZxuET5zl4/HxJ3R2aSYEuImWjFG/3z6RAF5Gy0dTSxoy6GhZMnxB2KTmhQBeRstCTdLbEjrNy0TTMLOxyckKBLiJl4YXW05zu6GZFiU63gAJdRMpE7/z5rSV4/XkvBbqIlIXH97WxeGYd0yaMCbuUnFGgi0jJ27zrGE+/dILbl8wIu5ScUqCLSElrPdXBFx59jmtnTWTN2+eHXU5OKdBFpGQlepLc8/AOepLOd+66njGVkbBLyqmsHkEnIlKMvvUvLWx7+STfXr2UudNK51Fzw9EZuoiUpC2xOOuiMT7cMJs7l84Ku5y8UKCLSMmJt1/g3h/vZMH0CXzljjeHXU7eaMpFREpKMul85pFnOd3RzY8+sYxx1eUTc1mdoZvZKjPba2YxM7tviP2fMbPdZvacmT1mZlcGX6qIyMi+13SAJ/a1cf97F3P1jLqwy8mrEQPdzCLAOuB2YDFwl5ktHtRsB9Dg7m8BHgW+HnShIiIj2XHoJN/YvJfbl8zg3y2/Iuxy8i6bM/RlQMzdD7h7F7ABuDOzgbv/3t3Pp1efBGYHW6aIyBs73dHNpx/eQX1dDX/zgbeU7BdwvZFsJpdmAYcz1o8Ay9+g/SeAXw+1w8zWAGsA6uvriUaj2VVZoNrb24v+PQRJ49FPYzFQrsfD3fkfz17glVM9fGl5DTue2pKzvoKQq/EI9NMCM/sY0AC8faj97r4eWA/Q0NDgjY2NQXafd9FolGJ/D0HSePTTWAyU6/F46KlDbD32PP9l1dV8snFBzvoJSq7GI5tAbwXmZKzPTm8bwMxuA74MvN3dLwRTnojIG9t77Cxf/eUuVi6axp+9rbRv7R9JNnPoW4FFZjbPzKqB1cDGzAZmdj3wv4A73P214MsUEXm9jq4e1j60ndqaKr754aVUVJTfvHmmEQPd3RPAWmAzsAd4xN13mdkDZnZHutk3gAnAT8xsp5ltHOZwIiKB+eovdxFra+dbH1nK9NrS/VrcbGU1h+7um4BNg7bdn7F8W8B1iYi8oY3PvsKGrYf51DsWlPRTiEZDt/6LSNF5+fg5vvSz53nrlZO597arwi6nYCjQRaSodCWSfPrhHVQYfHv1UqoiirFe5fMlByJSEr6x+UWeO3Ka737sBmZPHhd2OQVF/7SJSNH43Yuv8r2ml/gPN1/JqiUzwy6n4CjQRaQoHDvdyed+8hxXz6jlS++5JuxyCpICXUQKXk/SuffHO+jo6uHBj95ATVVpP0ruYmkOXUQK3oO/i/HkgRP83YeuY+FlE8Iup2DpDF1ECtpTB47z7cf28f7rZ/GBG8rjUXIXS4EuIgXr5Lku7tmwkyunjucv/2RJWX4l7mhoykVECpK787mfPMuJc1387OO3MGGM4mokOkMXkYL0wy0HeezF1/jie65myayJYZdTFBToIlJwnj9ymq/9eg+3XVPP3bfMDbucoqFAF5GC0n4hwacf3s60CWP4xgfL81FyF0uTUiJSMNydL//8eQ6dOM+GNTczeXx12CUVFZ2hi0jBePSZI/xi5yvce9tVLJs3Jexyio4CXUQKQuy1s9z/i13cPH8qn3rHwrDLKUoKdBEJXWd3D2sf2sHY6gjfWr2USJk/Su5iaQ5dREL317/aw4vHzvLDu2+kvq4m7HKKls7QRSRUv3nhKD968mX+dOU83nH1ZWGXU9QU6CISmsMnzvOFR5/jutkT+fy/uTrscoqeAl1EQtHdk+SeDTtwh+/cdQPVlYqjS6U5dBEJxd//dh/bD53iO3ddzxVT9Si5IOifRBHJu6aWNv7n4/u5a9kc3nfd5WGXUzKyCnQzW2Vme80sZmb3DbH/bWa23cwSZvbB4MsUkVLx2tlO/uLHO1k4fQL3v/fNYZdTUkYMdDOLAOuA24HFwF1mtnhQs0PA3cBDQRcoIqUjmXQ++8iznO1M8OBHb2BstR4lF6Rs5tCXATF3PwBgZhuAO4HdvQ3c/WB6XzIHNYpIifjuE/tpaonztX97LW+aURt2OSUnm0CfBRzOWD8CLL+YzsxsDbAGoL6+nmg0ejGHKRjt7e1F/x6CpPHop7EYqL29ne///DH+7ulOls2IMOPcfqLRA2GXFZpc/X7k9SoXd18PrAdoaGjwxsbGfHYfuGg0SrG/hyBpPPppLAb61W9/zw9fcC6fNJbv//lK6mqqwi4pVLn6/cgm0FuBORnrs9PbRERG5O784IULvHomyaP/6ZayD/NcyuYql63AIjObZ2bVwGpgY27LEpFS0JVI8t8fi/HMqz18YdWbWDpnUtgllbQRz9DdPWFma4HNQAT4gbvvMrMHgG3uvtHMbgR+DkwG3mdmX3V3XY8kUqa6Ekl+uv0ID/4uRuupDt5aH+GTK+aHXVbJy2oO3d03AZsGbbs/Y3krqakYESljg4N86ZxJ/PX7l+Cv7KJCX4mbc7r1X0Qu2XBB/varpmNmRI/uHvkgcskU6CJy0UYKcskvBbqIjJqCvDAp0EUkawrywqZAF5ERDQ7y6xTkBUmBLiLDGirI/+r9S2hUkBckBbqIvI6CvDgp0EWkj4K8uCnQRURBXiIU6CJlTEFeWhToImVIQV6aFOgiZURBXtoU6CJlQEFeHhToIiVMQV5eFOgiJaS7J8mBtnPsOXqGPUfP8E/PHVWQlxEFukiROnW+i91Hz7Dn6Nm+AG95tZ2uniQA1ZEKlirIy4oCXaTAJZPOwePnBgT3nqNneOV0Z1+baROquWZmHXffOpdrZtayeOZE5k8fT1Ukm6dMSqlQoIsUkPYLCV5MB/budIDvPXaWju4eACIVxoLp47lx3hSumVmXftVyWW1NyJVLIVCgi4TA3TlysiN9tp0+8z52hpePn+9rU1dTyTUz6/jIjXNYnA7vRfUTqKmKhFi5FDIFukiOdXb3sO/Vs+x+5Ux/gB87w9nOBABmcOWUcbz58jo+eMPs1Fn35XVcPrFG894yKgp0kYvk7nR2Jznd0c3pjm5One/qW35qfxc/PbqDPUfPcKCtnaSnfmZcdYSrZ9Ryx3WX902ZXD2jlvFj9FdRLp1+i6TsdSX6Q/l0R1dGQHf3b89YPtXRv9yVSA573FmTTnLNzFres2RGX3hfMWUcFRU665bcyCrQzWwV8G0gAnzf3f9m0P4xwP8B3gocBz7i7geDLVWkn7uTSDrdPUm6E86Fnh66e5wL3T2c6UxkBPEwAZ3xOt/V84Z91Y6ppG5sFZPGVTFxbBWLLpvApHFV1I1NrU8aW83E3uV0mxe2P8Xtt70jT6MhkjJioJtZBFgHvBs4Amw1s43uvjuj2SeAk+6+0MxWA38LfCQXBcvQ3J2epJN0SLqnX9CTdHyoZXeSyf52yb711HKqbXrZPX38jGNnLqfXdxxLcHpnK909TlcimQrbniQXMpZT232IbUm6epyuRCqYe7d3ZezvPW5X+ufcsx+fsVWRvtCdOK6KOVPGsWRsFZMytvXtH1vFpHGpkK6rqaTyIi7921+ps3DJv2zO0JcBMXc/AGBmG4A7gcxAvxP4Snr5UeBBMzP30fyVy84jWw+zvukAkAqxXgM6GtRr5upwP5NZqWfsGbB90HE7OzsZ84fH8PQ+x/vaeF97T+/r77uvvad78v5aerdlHq+v20HbMtsWjJ07h91VWWFURSqorqxI/RmxvuXe7dWRCmqqKqirqUxtr6xgTMb+1Dbr21aV/pnUn6nj1dVUDQjviWOrGFOpK0Ok9GUT6LOAwxnrR4Dlw7Vx94SZnQamAvHMRma2BlgDUF9fTzQaHXXBra8lmFyR6D/mMO0Gbx/uYgEbZsUyVobrI1GdpKoq0be/t4++9fR/bNBy5nEzf8bob5RarsBG+BkMKtLbDKiw1HJqm1GRuX+I5VR769teYf19VGQcM7Vsfcd/fV/Q1dlB7fhxVFVAZUXqmumqCohYar1i2Cs2HOhJv0bJgUT61es0tJN6tY7+iIFob2+/qN/vUqXxGChX45HXD0XdfT2wHqChocEbGxtHfYxG4C8CreriRaNRLuY9lCqNRz+NxUAaj4FyNR7ZTA62AnMy1mfz+hOfvjZmVglMJPXhqIiI5Ek2gb4VWGRm88ysGlgNbBzUZiPw8fTyB4Hf5WL+XEREhjfilEt6TnwtsJnUZYs/cPddZvYAsM3dNwL/G/iRmcWAE6RCX0RE8iirOXR33wRsGrTt/ozlTuBDwZYmIiKjoe/WFBEpEQp0EZESoUAXESkRCnQRkRJhYV1daGZtwMuhdB6caQy6G7bMaTz6aSwG0ngMdCnjcaW7Tx9qR2iBXgrMbJu7N4RdR6HQePTTWAyk8RgoV+OhKRcRkRKhQBcRKREK9EuzPuwCCozGo5/GYiCNx0A5GQ/NoYuIlAidoYuIlAgFuohIiVCgB8TMPmtmbmbTwq4lLGb2DTN70cyeM7Ofm9mksGsKg5mtMrO9ZhYzs/vCridMZjbHzH5vZrvNbJeZ3RN2TWEzs4iZ7TCzfwr62Ar0AJjZHOCPgENh1xKy3wJL3P0twD7giyHXk3cZD1W/HVgM3GVmi8OtKlQJ4LPuvhi4CfhUmY8HwD3AnlwcWIEejL8HvsDrHk9dXtz9n9299+meT5J6ulW56Xuourt3Ab0PVS9L7n7U3benl8+SCrJZ4VYVHjObDfwx8P1cHF+BfonM7E6g1d2fDbuWAvMfgV+HXUQIhnqoetkGWCYzmwtcDzwVbiWh+hapk79kLg6e14dEFysz+xdgxhC7vgx8idR0S1l4o7Fw91+k23yZ1P9q/2M+a5PCZWYTgJ8C97r7mbDrCYOZvRd4zd2fMbPGXPShQM+Cu9821HYzuxaYBzxrZpCaYthuZsvc/VgeS8yb4cail5ndDbwXeFeZPlc2m4eqlxUzqyIV5v/o7j8Lu54Q3QrcYWbvAWqAOjP7v+7+saA60I1FATKzg0CDu5flt8qZ2Srgm8Db3b0t7HrCYGaVpD4QfhepIN8KfNTdd4VaWEgsdabzD8AJd7837HoKRfoM/XPu/t4gj6s5dAnSg0At8Fsz22lm3w27oHxLfyjc+1D1PcAj5RrmabcC/x54Z/p3Ymf6DFVyQGfoIiIlQmfoIiIlQoEuIlIiFOgiIiVCgS4iUiIU6CIiJUKBLiJSIhToIiIl4v8DZXjPggaqXnAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "id": "OPy8u7tshkFU",
        "outputId": "6235c031-4d70-455b-a279-e0bea6c9cbe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.80134161e-05, 2.12062451e-04, 5.76445508e-04, 1.56694135e-03,\n",
              "       4.25938820e-03, 1.15782175e-02, 3.14728583e-02, 8.55520989e-02,\n",
              "       2.32554716e-01, 6.32149258e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step function** \n",
        "\n",
        "*Cons*\n",
        "\n",
        "* It cannot provide multi-value outputs—for example, it cannot be used for multiclass\n",
        "classification problems.\n",
        "* The gradient of the step function is zero, which causes a hindrance in the\n",
        "backpropagation process.\n",
        "\n",
        "**Linear**\n",
        "\n",
        "*Cons*\n",
        "\n",
        "* It’s not possible to use backpropagation as the derivative of the function is a\n",
        "constant and has no relation to the input x.\n",
        "* All layers of the neural network will collapse into one if a linear activation\n",
        "function is used. No matter the number of layers in the neural network, the last\n",
        "layer will still be a linear function of the first layer. So, essentially, a linear\n",
        "activation function turns the neural network into just one layer.\n",
        "\n",
        "**Sigmoid**\n",
        "\n",
        "*Pros*\n",
        "\n",
        "* It is commonly used for models where we have to predict the probability as an\n",
        "output. Since probability of anything exists only between the range of 0 and 1,\n",
        "sigmoid is the right choice because of its range.\n",
        "* The function is differentiable and provides a smooth gradient, i.e., preventing\n",
        "jumps in output values. This is represented by an S-shape of the sigmoid activation function.\n",
        "\n",
        "*Cons*\n",
        "\n",
        "* As we can see from the Figure, the gradient values are only significant for\n",
        "range -3 to 3, and the graph gets much flatter in other regions.\n",
        "It implies that for values greater than 3 or less than -3, the function will have very\n",
        "small gradients. As the gradient value approaches zero, the network ceases to learn\n",
        "and suffers from the *Vanishing gradient problem*.\n",
        "* The output of the logistic function is not symmetric around zero. This makes the training of the\n",
        "neural network more difficult and unstable.\n",
        "\n",
        "**Tanh**\n",
        "\n",
        "*Pros*\n",
        "\n",
        "* The output of the tanh activation function is Zero centered; hence we can\n",
        "easily map the output values as strongly negative, neutral, or strongly positive.\n",
        "* Usually used in hidden layers of a neural network as its values lie between -1 and 1; therefore, the mean for the hidden layer comes out to be 0 or very close to\n",
        "it. It helps in centering the data and makes learning for the next layer much\n",
        "easier.\n",
        "\n",
        "*Cons*\n",
        "\n",
        "* Also tanh faces the problem of *vanishing gradients* similar to the\n",
        "sigmoid activation function. Plus the gradient of the tanh function is much steeper as\n",
        "compared to the sigmoid function.\n",
        "Although both sigmoid and tanh face vanishing gradient issue, tanh is\n",
        "zero centered.\n",
        "Therefore, in practice, tanh nonlinearity is always preferred to sigmoid\n",
        "nonlinearity.\n",
        "\n",
        "**ReLU**\n",
        "\n",
        "*Pros*\n",
        "\n",
        "* Since only a certain number of neurons are activated, the ReLU function is far\n",
        "more computationally efficient when compared to the sigmoid and tanh\n",
        "functions.\n",
        "* ReLU accelerates the convergence of gradient descent towards the global\n",
        "minimum of the loss function due to its linear, non-saturating property.\n",
        "\n",
        "*Cons*\n",
        "\n",
        "* The negative side of the graph makes the gradient value zero. Due to this reason,\n",
        "during the backpropagation process, the weights and biases for some neurons are\n",
        "not updated. This can create dead neurons which never get activated.\n",
        "All the negative input values become zero immediately, which decreases the\n",
        "model’s ability to fit or train from the data properly.\n",
        "\n",
        "**Leaky ReLU**\n",
        "\n",
        "*Pros*\n",
        "\n",
        "* The advantages of Leaky ReLU are same as that of ReLU, in addition to the fact that\n",
        "it does enable backpropagation, even for negative input values.\n",
        "By making this minor modification for negative input values, the gradient of the left\n",
        "side of the graph comes out to be a non-zero value. Therefore, we would no longer\n",
        "encounter dead neurons in that region.\n",
        "\n",
        "*Cons*\n",
        "\n",
        "* The predictions may not be consistent for negative input values.\n",
        "* The gradient for negative values is a small value that makes the learning of\n",
        "model parameters time-consuming.\n",
        "\n",
        "**ELU (Exponential Linear Unit)**\n",
        "\n",
        "*Pros*\n",
        "\n",
        "* ELU becomes smooth slowly until its output equal to $a$ whereas RELU sharply smoothes.\n",
        "* Avoids dead ReLU problem by introducing log curve for negative values of\n",
        "input. It helps the network nudge weights and biases in the right direction.\n",
        "\n",
        "*Cons*\n",
        "\n",
        "* It increases the computational time because of the exponential operation\n",
        "included\n",
        "* No learning of the $a$ value takes place\n",
        "* Exploding gradient problem\n",
        "\n",
        "**Swish**\n",
        "\n",
        "*Pros*\n",
        "* Swish is a smooth function that means that it does not abruptly change\n",
        "direction like ReLU does near x = 0. Rather, it smoothly bends from 0 towards\n",
        "values $< 0$ and then upwards again.\n",
        "* Small negative values were zeroed out in ReLU activation function. However,\n",
        "those negative values may still be relevant for capturing patterns underlying\n",
        "the data. Large negative values are zeroed out for reasons of sparsity making it a win-win situation.\n",
        "\n",
        "**Softmax**\n",
        "\n",
        "*Pros*\n",
        "\n",
        "* It calculates the relative probabilities. Similar to the sigmoid/logistic activation\n",
        "function, the SoftMax function returns the probability of each class.\n",
        "It is most commonly used as an activation function for the last layer of the neural\n",
        "network in the case of multi-class classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "_tUhOzO7Zse4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aIVqtLEfHtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}